,author,body,number,created_at,empty
0,carllerche,I debugged this some and it seems like the scheduler is stuck checking the queue for max ticks so doesn’t see that the runtime should shut down.  ,4290,2021-12-01T00:43:36Z,0
1,Darksonn,"I do not think that the behavior is incorrect, actually. We don't guarantee anything about when we poll which tasks, so if we want to poll a bunch of other tasks before we poll main again, then that is allowed under our guarantees. The runtime shuts down when it's supposed to, i.e. immediately when `main` returns.

The other task in the `select!` is not cancelled until `main` returns because dropping the `JoinHandle` doesn't cancel the task.",4290,2021-12-01T11:17:14Z,0
2,austenadler,"It could definitely not be a bug; I'm not too familiar with the guarantees of the library, but if ""runtime shutdown *immediately* when main returns"" is a promise, isn't this being broken? The task is being polled 57 times after (the async) main returns here because the poller doesn't know the runtime needs to be shut down. 

Edit: I see that after ""done"" in the example above, there is no code running. I will modify the above example without wrapping it in a tokio::spawn",4290,2021-12-01T17:41:01Z,0
3,Darksonn,"It really should shut down properly once main returns, even if you remove the `tokio::spawn` wrapping.",4290,2021-12-01T21:47:40Z,0
4,MOZGIII,"Oh, so this is why my tests are failing. Is there a workaround we could use in the meantime?",1897,2020-06-10T20:43:21Z,0
5,Darksonn,@carllerche So much has changed since this issue was filed. Is this still the case?,1897,2020-06-10T20:57:20Z,0
6,MOZGIII,"In my code, with time paused, `delay_for` set for one hour completes almost immediately. I expect this to happen only if I advance the time, but not without it. Maybe I'm wrong, but I assume this is what's causing that behavior.
Thus, something like this must still be present.

This is with `tokio` `0.2.21`.

UPD: this occasionally happens in the multi-threaded scheduler with `spawn` instead of `block_on` too. But the fact that time is paused seems to be relevant.",1897,2020-06-10T21:00:03Z,0
7,Darksonn,That sounds like a different issue. @MOZGIII can you open a new issue about paused time advancing unexpectedly?,1897,2020-06-11T20:36:46Z,0
8,MOZGIII,"https://github.com/tokio-rs/tokio/issues/2090 is already opened. Honestly, I found that one first and then came here, as this is referred as a root cause for than one. :smile: ",1897,2020-06-11T21:05:37Z,0
9,seunghunee,"#3582
I think we can close this issue.",1897,2021-12-02T12:29:33Z,0
10,taiki-e,"FYI, links to some distributions' rustc package and firefox's rust update policy:
- https://packages.debian.org/en/stable/rustc
- https://packages.ubuntu.com/search?lang=en&suite=all&keywords=rustc
- https://wiki.mozilla.org/Rust_Update_Policy_for_Firefox (it doesn't seem to be updated...)
",3412,2021-01-20T07:11:46Z,0
11,jaskij,"As someone using embedded Linux, I'll throw some extra info about Yocto/OpenEmbedded - one of the popular  frameworks for building embedded Linux images and distributions.

- Current LTS can be made to support 1.49 and 1.51 through an external layer
- Latest release supports 1.54 in core
- Releases are on a half year cadence April and October and will probably use latest stable, to within one or two minors
- There's an upcoming LTS in April
- LTS support is two years minimum",3412,2021-12-05T16:30:50Z,0
12,Darksonn,"This is a breaking change, but we also have two other pending changes that are breaking, so it might be time to do a breaking tokio-util release soon.",4241,2021-11-18T17:18:02Z,0
13,Darksonn,Thanks.,4303,2021-12-07T10:07:02Z,0
14,Darksonn,Thanks.,4303,2021-12-07T10:07:04Z,0
15,Darksonn,"Although this could be useful, it wont help with your use-case. You have to return a reference from the closure, and `RwLockReadGuard<U>` is not a reference.",4305,2021-12-07T15:58:30Z,0
16,Darksonn,"What you implemented here doesn't actually integrate with Tokio's coop system. You can see an example of how to do that [here][1] (the `poll_proceed` and `made_progress` calls). There's an overview of the coop strategy used by Tokio [here][2].

[1]: https://github.com/tokio-rs/tokio/blob/ee4b2ede83c661715c054d3cda170994a499c39f/tokio/src/runtime/task/join.rs#L203-L240
[2]: https://tokio.rs/blog/2020-04-preemption",4300,2021-12-05T14:43:26Z,0
17,Darksonn,"Also, seems like you have a warning in the code:
```text
error: unused import: `empty`
 --> tokio/tests/io_util_empty.rs:2:17
  |
2 | use tokio::io::{empty, AsyncBufReadExt, AsyncReadExt};
  |                 ^^^^^
  |
  = note: `-D unused-imports` implied by `-D warnings`

error: could not compile `tokio` due to previous error
```",4300,2021-12-05T14:45:21Z,0
18,Darksonn,"And also, welcome! ",4300,2021-12-05T14:52:20Z,0
19,BraulioVM,"Thanks for all the feedback and all the pointers! 

The coop system makes a lot of sense. I implemented the changes but I believe they won't fix the original issue! That's because of how `timeout` and `Sleep` are implemented.

See the future implementation for `Timeout`
https://github.com/tokio-rs/tokio/blob/ee4b2ede83c661715c054d3cda170994a499c39f/tokio/src/time/timeout.rs#L163-L183

and `poll_elapsed` for `Sleep`
https://github.com/tokio-rs/tokio/blob/ee4b2ede83c661715c054d3cda170994a499c39f/tokio/src/time/driver/sleep.rs#L379-L388

`empty::read` may return `Polling` when its budget decreases to 0, but then `me.delay.poll` will also always return `Polling` in that case, even if the timeout has been hit, because `Sleep` will also observe that it has no budget. When the executor takes another stab at completing the task, with a fresh new budget, it will go back to `empty::read`, until the budget is exhausted again.

I've observed this behaviour locally after applying the changes. Does this sound reasonable to you?

As for next steps:

1. I could re-implement the tests to use an async block and `tokio::task::yield_now`, and we could get that merged, but being aware that this change doesn't fix the original issue.
2. I could try fixing the original issue, and I believe this would involve changing the implementation of the `Timeout` future. Maybe poll the `Sleep` with an unconstrained budget? I don't know, I'm sure you can come up with better approaches, and that's why I'd prefer to hear from you before working on any specific fix. 

What are your thoughts?",4300,2021-12-06T13:20:01Z,0
20,Darksonn,"Thank you for pointing that out, that explains why some other case I was thinking of also doesn't work. We do want to make `tokio::io::empty` use coop, but we should also fix the timer.

Can you update this PR with the changes I requested for `tokio::io::empty`, and then open a new issue or PR for the issue with timeout? It seems like we can fix the timeout by having it check the budget before and after polling the task, and having it unconditionally check the timer if the budget was exhausted somewhere inside the task, but still abort if the budget was exhausted before the timeout got polled.

If you want to discuss the changes to timeout further before opening a PR, then please open an issue so we can do it there instead of here.",4300,2021-12-06T14:23:26Z,0
21,BraulioVM,Sounds like a plan!,4300,2021-12-06T15:08:00Z,0
22,BraulioVM,I will fix the failing build later today,4300,2021-12-06T16:04:04Z,0
23,BraulioVM,Is there anything I should do before this gets merged? ,4300,2021-12-10T10:03:24Z,0
24,Darksonn,"Let's wait for std to stabilize `try_exists`. I'd rather not stabilize this, then see std change the signature of theirs so we don't match.",4299,2021-12-10T11:42:03Z,0
25,Darksonn,"I'm generally not a fan of these methods and try to avoid recommending them. I'd prefer to recommend other alternatives such as spawning multiple tasks, collecting into a `FuturesUnordered`, or using the `StreamExt` combinators.",4287,2021-12-10T11:53:30Z,0
26,Darksonn,Your PR doesn't actually expose `Elapsed`.,4286,2021-12-10T11:54:06Z,0
27,mathstuf,Hmm. So how does `Timeout` get exposed then? But looking at the docs…it isn't either. I guess all of the impls and related structures need exposed then?,4286,2021-12-10T12:38:11Z,0
28,Darksonn,"I suppose, yes. ",4286,2021-12-10T12:42:43Z,0
29,Darksonn,This is being discussed at https://users.rust-lang.org/t/my-benchmark-done-elixir-is-faster-than-rust-tokio-how-do-i-improve-rust-code/68577/7,4311,2021-12-10T12:38:03Z,0
30,Darksonn,Thoughts @ipetkov ?,4312,2021-12-10T12:43:21Z,0
31,ipetkov,"Unfortunately, I think this is the only way to accomplish this. We pretty much wrap the stdlib types, and there aren't any OS extensions that I could find which do the `setpgid` calls for us",4312,2021-12-10T17:57:23Z,0
32,kpcyrd,Would it help if I propose this extension in the stdlib?,4312,2021-12-10T18:10:59Z,0
33,ipetkov,"It's worth a shot! We don't *need* to wait on the stdlib to potentially add such a wrapper, but having the feature parity would be nice",4312,2021-12-10T18:13:41Z,0
34,Darksonn,You need to update `tokio-stream` too.,4313,2021-12-10T14:23:21Z,0
35,Darksonn,Build failure is [mio#1539](https://github.com/tokio-rs/mio/issues/1539).,4313,2021-12-10T16:40:31Z,0
36,tobz,"Per @Darksonn, created a `tokio-util-0.6.x` branch for any future backports, etc.",4313,2021-12-10T18:25:56Z,0
37,hawkw,"Reading from `io::empty` always returns immediately, so the loop will never yield. The timeout cannot fire unless the main task yields, because it's blocking the thread.

If you spawn the `async` block inside the timeout as a separate task, you should see it time out as expected: https://play.rust-lang.org/?version=stable&mode=debug&edition=2021&gist=8c1de2e3ce490e8d57ff848f32ae95db",4291,2021-12-01T23:33:46Z,0
38,hawkw,It occurs to me that `io::empty` should maybe participate in [cooperative yielding](https://tokio.rs/blog/2020-04-preemption) to prevent busy-loops like this from hanging the entire runtime...,4291,2021-12-01T23:34:28Z,0
39,Darksonn,Thanks for reporting this. We should make sure that `io::empty` and friends participate in coop.,4291,2021-12-02T08:36:40Z,0
40,francis-starlab,"Possibly related/same thing:

```rust
use std::process::Stdio;
use std::time::Duration;
use tokio::io::AsyncReadExt;
use tokio::process::Command;
use tokio::time::timeout;

#[tokio::main]
async fn main() {
    let mut command = Command::new(""yes"");
     command.stdout(Stdio::piped());
    let mut spawned = command.spawn().expect(""error spawning"");
    let mut stdout = spawned.stdout.take().expect(""no stdout?"");
    if timeout(Duration::from_millis(1000), async {
        loop {
            let mut buf = [0u8; 1];
            let _ = stdout.read(&mut buf).await;
        }
    }).await.is_err() {
        println!(""timedout"");
    };
}

```


Assuming you have a fast enough yes on your system, then the above also doesn't timeout when it should.",4291,2021-12-02T18:42:09Z,0
41,BraulioVM,"Actually the original issue isn't fixed yet because of https://github.com/tokio-rs/tokio/pull/4300#issuecomment-986769404 . I should have updated the PR body. Anyway, I'm working on a PR that finally fixes this problem.

`tokio::io::empty` does participate in coop now, which is what the currrent title of the issue refers to, so I guess it could make sense to keep it closed.",4291,2021-12-11T13:14:34Z,0
42,Darksonn,"> The gRPC server is started by calling `grpcio::Server::start()` in `main()` (not using `#[tokio::main]`), which then blocks to run the event loop by passing a `futures::channel::oneshot` to `Runtime::block_on()` that is only ever completed after ctrlc signal is caught.

Can you elaborate on where exactly this `block_on` call occurs?",4309,2021-12-09T22:44:03Z,0
43,Darksonn,@ipetkov This appears to be related to `tokio::process`. Thoughts on the above?,4309,2021-12-09T22:44:53Z,0
44,iitalics,"> Can you elaborate on where exactly this block_on call occurs?

It looks effectively like this:

```rust
fn entrypoint() -> Result<(), AppError> {
   let tokio_runtime = tokio::runtime::Runtime::new()?;
   ...
   grpcio_server.start();
   let shutdown_fut = ctrlc_rx.map(|_| Ok(()));
   tokio_runtime.block_on(shutdown_fut)
}

fn main() {
  std::process::exit(match entrypoint() {
    Ok(_) => 0,
    Err(_) => ...
  })
}
```",4309,2021-12-09T22:49:48Z,0
45,Darksonn,"Right, so `grpcio_server` is started in the background and the `start` call does not actually block? If so, and if you have no other uses of `block_on`, then coop is not related.",4309,2021-12-09T22:54:22Z,0
46,Darksonn,How are you reimplementing `wait_with_output`?,4309,2021-12-09T22:58:09Z,0
47,iitalics,"> How are you reimplementing wait_with_output?

Here is what was working (exact code): https://gist.github.com/iitalics/710401de72725fa4b6db27742124ae24",4309,2021-12-09T23:01:32Z,0
48,Darksonn,"The broken version using `wait_with_output` appears incomplete, e.g. I am surprised that the borrow-checker is letting you return the `write_all` future from the `and_then` call when `write_all` only borrows `stdin` and `stdin` is in a local variable in the closure. 

Anyway, it seems like one difference between the two snippets is that your original snippet using `wait_with_output` does not appear to be closing `stdin` before calling `wait_with_output`.",4309,2021-12-09T23:08:02Z,0
49,iitalics,"Right, that code is paraphrased, I forgot to make it borrow check compliant. I fix the borrow checking issue by using an async move block so its actually like this:
```rust
let fut = Box::pin(async move { stdin.write_all(input.as_bytes()).await });
fut.and_then(move |_| child.wait_with_output())
```",4309,2021-12-09T23:10:51Z,0
50,iitalics,A version of `communicate()` which drops the stdout/stderr handles inside `read_*_fut` block also causes the crash to happen (at at that point it is nearly identical to `wait_with_output()`),4309,2021-12-09T23:11:54Z,0
51,ipetkov,"First thing that comes to mind: have you tried splitting out the stdin.write and stdout.read into two separate futures? I suppose this is what you're essentially doing with the `communicate()` reimplementation, but I think the culprit could be that the call to `child.wait_with_output` is not started _until `write_all` completes_.

My guidance for piping input/output of a child process is always do them independently, otherwise if the child hangs because its stdout buffer is full (parent isn't reading from it yet!) it may stop reading its input, in which case the parent could get stuck writing to it. And if the parent is buffering data to send to the child with no back-pressure it could be holding that data for too long and running out of memory.

Could you try changing the code as follows and see if the OOM reproduces still?

```rust
let cmd = tokio::process::Command::new(...)...;
futures::future::lazy(|_| cmd.spawn())
  .and_then(|mut child| {
    let mut stdin = child.stdin.take().unwrap();
    tokio::spawn(async move { stdin.write_all(...).unwrap() });

    child.wait_with_output()
  })
  .then(|result| ...);
```
",4309,2021-12-10T01:28:02Z,0
52,iitalics,"I'm testing this right now. But the input buffer is completely in memory beforehand (see how my `communicate()` takes String argument) --- so I would expect this to deadlock, not crash.",4309,2021-12-10T16:24:17Z,0
53,iitalics,"I've just observed the crash again when using the following:

```rust
async move {
    tokio::spawn(async move {
        stdin.write_all(input.as_bytes()).await
    });
    child.wait_with_output().await
}
```",4309,2021-12-10T16:53:40Z,0
54,ipetkov,"Okay, thanks for testing that. Next thing that comes to mind: do you have an idea of how much data the child is writing back on stdout/stderr? Do you need all of that data present within the parent process or could that be routed elsewhere?

`wait_with_output` will basically buffer _everything_ for as long as the child process continues to write data.

My first recommendation is to avoid having the parent read any streams it doesn't care about (e.g. redirect stderr to /dev/null or to a file directly). My second recommendation would be to change the application so that it can process the data in a more streaming fashion instead of capturing all the output up front, so that it can discard data as quickly as possible",4309,2021-12-10T18:04:00Z,0
55,iitalics,"I mentioned before that this crash only appeared after having upgraded the tokio infrastructure of the application. Before, there is no problem with collecting all subprocess output -- behavior I'm not interested in changing.",4309,2021-12-12T02:39:24Z,0
56,Darksonn,"We aren't saying that your application is doing something incorrect here — it certainly sounds like a bug in Tokio, but it would be helpful to narrow down the issue if you can test that the issue still happens even if you stop reading from the pipe once the vector becomes super large. Something like this:
```Rust
let read_stderr_fut = async {
    let mut buf = Vec::new();
    while buf.len() < 50_000_000 {
        stderr.read_buf(&mut buf).await?;
    }
    std::io::Result::<_>::Ok(buf)
};
```",4309,2021-12-12T09:43:16Z,0
57,Darksonn,"I have opened a PR that changes `wait_with_output` to the version that doesn't crash for you, but I would still really want to understand what is going on here. ",4309,2021-12-12T10:13:39Z,0
58,iitalics,Running some tests today but its taking forever because the problem tends to go away when I add more logs. FWIW the largest stderr we ever see is <256 bytes.,4309,2021-12-13T21:33:21Z,0
59,iitalics,"Update: I was running a new test which was doing `read_buf` in a loop. It would crash until I turned logging on, then it would stop crashing. Then I realized something: I had a logging statement in between receiving EOF and closing the pipe:
```rust
loop {
  let n = stderr.read_buf(&mut buf).await?;
  warn!(...);
  if n == 0 {
    drop(stderr); // (implicit)
    return Ok(buf);
  }
}
```
The logging probably added a delay that made it so `close()` would occur after the process had exited, mimicking the working version where we explicitly drop the pipes at the end. Sure enough, when I rearrange the log to happen *after* the `if` (thus nothing happens between EOF and `close()`) then I can reproduce the crash.",4309,2021-12-14T15:32:13Z,0
60,ipetkov,"Thanks for the analysis @iitalics! Now I'm really curious about the epoll and kernel interactions around closing descriptors and other processes, but at least we can merge #4315 with the confidence that it should mitigate this issue",4309,2021-12-14T18:04:29Z,0
61,BraulioVM,The solution was inspired by @Darksonn 's comment in https://github.com/tokio-rs/tokio/pull/4300#issuecomment-986822888,4314,2021-12-11T13:29:49Z,0
62,BraulioVM,Anything else I can do here?,4314,2021-12-15T10:50:09Z,0
63,Darksonn,"No, thanks.",4314,2021-12-15T10:59:10Z,0
64,carllerche,I'm not sure what is causing the CI failure. @taiki-e ?,4320,2021-12-14T22:38:39Z,0
65,carllerche,CI is breaking because this depends on a tokio-macros release as well,4320,2021-12-14T22:46:19Z,0
66,carllerche,greeeeeeen :green_apple: ,4320,2021-12-15T04:12:28Z,0
67,Darksonn,"I just merged #4314, so you should probably add it to the changelog.",4320,2021-12-15T10:59:47Z,0
68,ipetkov,"Thanks for the report! I'll try to take a look at this when I'm back from vacation next week. 

If anyone wants take a shot at implementing it, feel free to ping me on the PR!",4285,2021-11-28T00:27:39Z,0
69,Yogaflre,"It looks like it's unstable yet.
https://github.com/rust-lang/rust/issues/44434",4285,2021-11-30T14:33:20Z,0
70,alecmocatta,"> It looks like it's unstable yet. [rust-lang/rust#44434](https://github.com/rust-lang/rust/issues/44434)

Good point, though it's scheduled for stabilisation in a couple days' time: https://github.com/rust-lang/rust/blob/master/RELEASES.md#version-1570-2021-12-02",4285,2021-11-30T14:55:38Z,0
71,ipetkov,"Unfortunately, [our MSRV policy](https://github.com/tokio-rs/tokio/blob/master/CONTRIBUTING.md#mininum-supported-rust-version-msrv) requires that we support compilers from at least 6 months back, meaning we won't be able to rely on these methods from the stdlib for 6 months... :(

I've opened #4295 to add a `cmd.as_std()` method which will allow the caller to access the underlying `std::process::Command`, and therefore the new methods (if their stdlib version is new enough)",4285,2021-12-02T19:08:47Z,0
72,ipetkov,@alecmocatta #4295 has landed in 1.15.0 so I'm going to resolve this for now. Feel free to reopen if you have other issues or questions!,4285,2021-12-15T21:20:59Z,0
73,Darksonn,We don't enable unstable features in `full`.,4325,2021-12-18T08:33:47Z,0
74,bIgBV,@Darksonn I can take a look at this one this week.,4184,2021-10-26T16:27:14Z,0
75,hi-rustin,"The results of my tests look as if they are correct.

My code: 
```rs
use bytes::Bytes;
use futures::SinkExt;

use tokio::fs::File;
use tokio::io::{AsyncRead, AsyncWrite};
use tokio_util::codec::{Framed, LengthDelimitedCodec};

async fn write_frame<T>(io: T) -> Result<(), Box<dyn std::error::Error>>
where
    T: AsyncRead + AsyncWrite + Unpin,
{
    let codec = LengthDelimitedCodec::builder()
        .length_field_offset(0) // default value
        .length_field_length(2)
        .length_adjustment(-2) // size of head
        .num_skip(0) // Do not strip frame header
        .new_codec();
    let mut transport = Framed::new(io, codec);
    let frame = Bytes::from(""hello world"");

    print!(""{:?}"", frame);
    transport.send(frame).await?;
    Ok(())
}

#[tokio::main]
async fn main() {
    let file = File::create(""foo.txt"").await.unwrap();
    write_frame(file).await.unwrap();
}

```",4184,2021-12-18T15:26:26Z,0
76,bjorn3,Can you double check that the code still passes miri? I'm not completely certain stacked borrows allows this. I may be wrong though.,4307,2021-12-08T21:09:17Z,0
77,Darksonn,I'm quite certain that stacked borrows is fine with this.,4307,2021-12-08T21:39:52Z,0
78,Cyborus04,"Well, the miri run failed, but it wasn't because of this

Mostly unsupported functions, but also a couple borrowing issues:

```
error: Undefined Behavior: no item granting read access to tag <5958705> at alloc2139059+0x28 found in borrow stack.
   --> tokio\src\fs\file.rs:747:47
    |
747 |         if let Err(e) = poll_fn(|cx| Pin::new(&mut *self).poll_flush(cx)).await {
    |                                               ^^^^^^^^^^ no item granting read access to tag <5958705> at alloc2139059+0x28 found in borrow stack.
    |
    = help: this indicates a potential bug in the program: it performed an invalid operation, but the rules it violated are still experimental
    = help: see https://github.com/rust-lang/unsafe-code-guidelines/blob/master/wip/stacked-borrows.md for further information
```

```
error: Undefined Behavior: trying to reborrow for SharedReadOnly at alloc80792+0x10, but parent tag <206238> does not have an appropriate item in the borrow stack
   --> tokio\src\sync\watch.rs:365:28
    |
365 |             let notified = self.shared.notify_rx.notified();
    |                            ^^^^^^^^^^^ trying to reborrow for SharedReadOnly at alloc80792+0x10, but parent tag <206238> does not have an appropriate item in the borrow stack
    |
    = help: this indicates a potential bug in the program: it performed an invalid operation, but the rules it violated are still experimental
    = help: see https://github.com/rust-lang/unsafe-code-guidelines/blob/master/wip/stacked-borrows.md for further information
```",4307,2021-12-08T22:13:37Z,0
79,Darksonn,It doesn't really make sense to run miri on async code these days due to [this issue](https://github.com/rust-lang/rust/issues/63818).,4307,2021-12-08T22:15:57Z,0
80,bjorn3,"None of the `ReadBuf` methods are async, so testing just `ReadBuf` should be fine. Unfortunately it seems like `ReadBuf` itself doesn't have any tests to run in miri.",4307,2021-12-08T23:15:00Z,0
81,jebrosen,Looks like this also closes #2269.,2391,2020-04-09T19:03:18Z,0
82,jonhoo,This is _fantastic_! So many hacks to set the thread pool size when I want to run with fewer cores can go away :tada: ,2391,2020-04-09T20:57:45Z,0
83,Vagelis-Prokopiou,"@seanmonstar @jonhoo I had to look into this matter too, in the context of actix-web.

In my specific case, the configuration with `workers = number of physical cpus` outperformed the configuration with `workers = number of logical cpus`. You can check this issue for specifics: https://github.com/actix/actix-web/issues/957

I believe that some amount of benchmarks should be the judge in this matter, because there is a possibility that the performance is degraded out of the box with this configuration.",2391,2021-12-18T20:56:45Z,0
84,jonhoo,"@Vagelis-Prokopiou I suspect that's because hyperthreading is generally bad for performance (at least in my experience). It's a tricky trade-off. What I'd really like for us to do is to respect cgroups/affinity, but _ignore_ hyperthreading, but I don't know that that's feasible to implement :sweat_smile: ",2391,2021-12-18T23:59:16Z,0
85,Vagelis-Prokopiou,"@jonhoo I am not that aware of the internals in order to provide a good proposal.

What I am proposing is:

a) Some benchmarks by various people, in order to decide the best default option from the currently available options.

b) If option `a` is not possible, at least better documentation for the current setup (I created a PR for this #4330).

",2391,2021-12-19T09:01:44Z,0
86,Darksonn,Did you find an answer?,4329,2021-12-19T09:42:23Z,0
87,gipsyh,"> 

I should use spawn_blocking to execute sync code.",4329,2021-12-19T10:12:43Z,0
88,carllerche,"I agree, this should probably be fixed!",186,2018-03-06T17:50:19Z,0
89,BijanVan,"I think BufRead requires something like:
fn read_line(&mut self, buf: &mut String, max_length: usize) -> Result<usize>
in addition to:
fn read_line(&mut self, buf: &mut String) -> Result<usize>

https://doc.rust-lang.org/stable/std/io/trait.BufRead.html#method.read_line",186,2018-03-06T18:19:22Z,0
90,carllerche,I would probably not use the impl in std and write our own.,186,2018-03-06T18:26:29Z,0
91,kpp,"This issue should be created in `futures` repo, because they copied `Lines` combinator: https://github.com/rust-lang-nursery/futures-rs/blob/master/futures-util/src/io/lines.rs#L48",186,2018-03-08T22:01:26Z,0
92,carllerche,"My *guess* is that the futures team is not interested in this change as they are focused on mirroring `std` and the issue exists in `std` as well.

/cc @cramertj @aturon 

If it isn't resolved there, Tokio can provide an alternate, more robust, implementation.",186,2018-03-13T16:39:06Z,0
93,cramertj,IMO both `std` and `futures` should probably also offer a `read_line_max` or similar that allows specifying a maximum line size. I think it makes sense to offer this (or similar) functionality on the tokio side for now.,186,2018-03-13T19:04:29Z,0
94,jasondavies,"It sounds like the best way forward might be to add an optional `max_line_length` parameter to the lines combinator in `futures-util`?  It seems that `std` will not add `read_line_max` (from the above linked issue) so I think [Read::take](https://doc.rust-lang.org/nightly/std/io/trait.Read.html#method.take) could be used instead.

I'm not clear on why the same lines utility combinator exists in tokio-io and futures-util -- is the former going to be dropped?",186,2018-05-12T18:45:30Z,0
95,BijanVan,"This could be a possible solution:

    
    use bytes::BytesMut;
    use futures::{Async, Poll, Stream};
    use std::io::{self, BufRead};

    use AsyncRead;

    /// Combinator created by the top-level `lines` method which is a stream over
    /// the lines of text on an I/O object.
    #[derive(Debug)]
    pub struct Lines<A> {
        io: A,
        line: BytesMut,
        max_length: usize,
    }

    /// Creates a new stream from the I/O object given representing the lines of
    /// input that are found on `A`.
    ///
    /// This method takes an asynchronous I/O object, `a`, and returns a `Stream` of
    /// lines that the object contains. The returned stream will reach its end once
    /// `a` reaches EOF.
    pub fn lines_with_max_length<A>(a: A, max_length: usize) -> Lines<A>
    where
        A: AsyncRead + BufRead,
    {
        Lines {
            io: a,
            line: BytesMut::with_capacity(max_length),
            max_length: max_length,
        }
    }

    impl<A> Lines<A> {
        /// Returns the underlying I/O object.
        ///
        /// Note that this may lose data already read into internal buffers. It's
        /// recommended to only call this once the stream has reached its end.
        pub fn into_inner(self) -> A {
            self.io
        }
    }

    impl<A> Stream for Lines<A>
    where
        A: AsyncRead + BufRead,
    {
        type Item = String;
        type Error = io::Error;

        fn poll(&mut self) -> Poll<Option<String>, io::Error> {
            loop {
                if self.line.len() > self.max_length {
                    return Err(io::Error::new(
                        io::ErrorKind::Other,
                        ""Maximum length error."",
                    ));
                }

                let n = try_ready!(self.io.read_buf(&mut self.line));
                if n == 0 && self.line.len() == 0 {
                    return Ok(None.into());
                }

                let pos = self
                    .line
                    .windows(2)
                    .enumerate()
                    .find(|&(_, bytes)| bytes == b""\r\n"")
                    .map(|(i, _)| i);

                if let Some(pos) = pos {
                    let mut line = self.line.split_to(pos + 2);
                    line.split_off(pos);
                    return Ok(Async::Ready(Some(
                        String::from_utf8_lossy(&line).to_string(),
                    )));
                }
            }
        }
    }


",186,2018-05-29T15:19:51Z,0
96,carllerche,This should be possible to add to `LinesCodec` once #360 lands.,186,2018-05-29T23:37:56Z,0
97,spion,"While now there is an easier way to read lines with a maximum limit, this still doesn't address the issue that the easiest to use API will result with unsafe behavior. Instead of buffered reader, you need to be aware of the LinesCodec and FramedReader. Unfortunately, there is nothing about using `read_line` from Buffered Reader that will at least point you in the right direction (such as maybe via compiler warnings) - as far as I can tell?",186,2021-12-19T14:27:01Z,0
98,Matthias247,"FWIW, I was not able to reproduce this on my Macbook (old, dual-core) so far. So it might be either a windows issue, or it only shows up if something is fast enough.

Edit: Also happens on the Windows machine in WSL/Ubuntu. So it's likely more a speed related race condition than a wrong WinApi usage.",1768,2019-11-13T06:48:32Z,0
99,swarmer,"I'm able to reproduce this on a Linux machine, ~~but so far not on my macbook~~ and just did on my macbook also, with NR_ITERATIONS=1.
It does run a few times slower on my macbook.

I've added the two missing unwraps, but they didn't panic when this deadlocked.",1768,2019-11-14T21:42:32Z,0
100,carllerche,An attempt to repro w/ loom: https://github.com/tokio-rs/tokio/tree/eliza/sticky-mutex,1768,2019-11-15T00:59:16Z,0
101,Matthias247,"I root-caused this a bit deeper to the async block inside `tokio::spawn` not getting executed in some iterations by adding and observing some atomic variables.

With the added variables I can observe that `SPAWN_PER_BLOCK_ON` is always 0: The code inside `tokio::spawn` did not get executed. Therefore `BLOCK_ON_STATE` is always 2 (it waits for the async block inside spawn to complete). `BLOCK_ON_PER_RT_COUNT` has an arbitrary value => The issue doesn not necesarily happen immediately after a `Runtime` had been started.

Whether it might have something to do with `spawn` happening shortly after `block_on` is unclear. The only time we ever do a `spawn` in this example is directly after a `block_on`.


```rust
use tokio::sync::Mutex;
use tokio::sync::oneshot::channel;
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, Ordering};

static BLOCK_ON_PER_RT_COUNT: AtomicUsize = AtomicUsize::new(0);
static BLOCK_ON_STATE: AtomicUsize = AtomicUsize::new(0);
static SPAWN_PER_BLOCK_ON: AtomicUsize = AtomicUsize::new(0);

async fn do_iteration() {
    BLOCK_ON_STATE.store(1, Ordering::Release);

    // Changing this to a higher value will prevent it from getting stuck
    const NR_ITERATIONS: usize = 5;
    let m = Arc::new(Mutex::new(()));//IntrusiveMutex::new((), true));
    let (sender, receiver) = channel();

    let m = m.clone();
    tokio::spawn(async move {
        SPAWN_PER_BLOCK_ON.fetch_add(1, Ordering::Release);
        for _ in 0..NR_ITERATIONS {
            let _ = m.lock().await;
        }
        sender.send(());
    });

    BLOCK_ON_STATE.store(2, Ordering::Release);
    receiver.await;
    BLOCK_ON_STATE.store(3, Ordering::Release);
}

fn main() {
    const NR_ITERATIONS: usize = 10000;
    const BLOCK_ON_PER_RT: usize = 3000;

    for i in 0 .. NR_ITERATIONS {
        {
            BLOCK_ON_PER_RT_COUNT.store(0, Ordering::Release);
            println!(""Starting runtime {}"", i);
            let mut builder = tokio::runtime::Builder::new();
            builder.thread_pool().num_threads(1);
            let mut rt = builder.build().unwrap();

            for _ in 0 .. BLOCK_ON_PER_RT {
                BLOCK_ON_STATE.store(0, Ordering::Release);
                SPAWN_PER_BLOCK_ON.store(0, Ordering::Release);
                BLOCK_ON_PER_RT_COUNT.fetch_add(1, Ordering::Release);

                rt.block_on(async {
                    do_iteration().await;
                });
            }

            println!(""Stopping runtime"");
        }
        println!(""Stopped runtime"");
    }
}
```",1768,2019-11-17T03:06:17Z,0
102,hawkw,"> An attempt to repro w/ loom: https://github.com/tokio-rs/tokio/tree/eliza/sticky-mutex

for future readers: it should be noted that this does _not_ actually repro the bug",1768,2019-11-17T04:35:14Z,0
103,carllerche,I have not reproduced this bug yet,1768,2019-11-17T05:20:27Z,0
104,swarmer,"I just did some more debugging and maybe I understand what's going on?
First of all, I found out that this is *much* easier to reproduce when setting up 2 threads instead of one, so this is how I was testing it.
Basically, `tokio/src/runtime/thread_pool/idle.rs:worker_to_notify` checks if it needs to notify any workers, sees that one is searching and doesn't notify anything. However *just before this* the worker that was searching tried to find something to steal at `tokio/src/runtime/thread_pool/worker.rs:process_available_work`, didn't find anything, and parked itself.
This is the first time I'm digging into any of this code, so it's quite probable that there's more to the story and there is a reason why this should work anyway, that I don't know about.",1768,2019-11-17T21:04:41Z,0
105,hawkw,@swarmer do you have a more consistent repro with two threads that you can share?,1768,2019-11-17T21:08:17Z,0
106,swarmer,"@hawkw I'm just running the first example @Matthias247 provided, but with 1 replaced by 2, and when I reproduced this on latest master, I renamed the builder method according to the latest changes:

```rust
use tokio::sync::Mutex;
use tokio::sync::oneshot::channel;
use std::sync::Arc;




async fn do_iteration() {
    // Changing this to a higher value will prevent it from getting stuck
    const NR_ITERATIONS: usize = 1;
    let m = Arc::new(Mutex::new(()));//IntrusiveMutex::new((), true));
    let (sender, receiver) = channel();

    let m = m.clone();
    tokio::spawn(async move {
        for _ in 0..NR_ITERATIONS {
            let _ = m.lock().await;
        }

        sender.send(()).unwrap();
    });

    receiver.await.unwrap();
}

fn main() {
    const NR_ITERATIONS: usize = 10000;
    const BLOCK_ON_PER_RT: usize = 3000;

    for i in 0 .. NR_ITERATIONS {
        {
            println!(""Starting runtime {}"", i);
            let mut builder = tokio::runtime::Builder::new();
            builder.threaded_scheduler().num_threads(2);
            let mut rt = builder.build().unwrap();

            for _ in 0 .. BLOCK_ON_PER_RT {
                rt.block_on(async {
                    do_iteration().await;
                });
            }

            println!(""Stopping runtime"");
        }
        println!(""Stopped runtime"");
    }
}
```

Oh yeah, and just remembered that NR_ITERATIONS is 1 here.",1768,2019-11-17T21:11:26Z,0
107,Matthias247,"One observation we had yesterday was that this is actually not related to anything inside the subtask, and that just spawning and joining the task is sufficient. Therefore the example can be reduced to:

```rust

async fn do_iteration() {
    let jh = tokio::spawn(async move {        
    });

    jh.await;
}

fn main() {
    const NR_ITERATIONS: usize = 10000;
    const BLOCK_ON_PER_RT: usize = 3000;

    for i in 0 .. NR_ITERATIONS {
        {
            println!(""Starting runtime {}"", i);
            let mut builder = tokio::runtime::Builder::new();
            builder.threaded_scheduler().num_threads(2);
            let mut rt = builder.build().unwrap();

            for _ in 0 .. BLOCK_ON_PER_RT {
                rt.block_on(async {
                    do_iteration().await;
                });
            }

            println!(""Stopping runtime"");
        }
        println!(""Stopped runtime"");
    }
}
```` 

Edit: I also tried running multiple iterations of `do_iteration` inside a `block_on` to check whether spawning always fails on the first iteration of a `block_on` - but this is not the case. It seems the `spawn` which gets not executed is arbitrary - which matches @swarmer's theory of the task not getting picked up by a worker. ",1768,2019-11-17T21:14:14Z,0
108,carllerche,"@swarmer thanks for the debugging. I am on mobile atm so have limited ability to dig in. 

You are correct regarding the searching hazard. in theory, when the last searching worker exits the searching state before parking, it checks the queues: https://github.com/tokio-rs/tokio/blob/master/tokio/src/runtime/thread_pool/worker.rs#L547 this should prevent the case you are mentioning. ",1768,2019-11-17T23:04:36Z,0
109,swarmer,"@carllerche I just found this code and put a log there; and this the end of logs I got:
```
searching: 0, unparked: 0, num_workers: 2
searching: 0, unparked: 0, num_workers: 2
searching: 0, unparked: 1, num_workers: 2
searching: 0, unparked: 1, num_workers: 2
searching work <these two logs are about stealing>
work not found
final sweep empty
searching: 1, unparked: 1, num_workers: 2 <such logs are only printed from the spawning thread, I think>
<it deadlocks here>
```

So apparently this sweep doesn't find anything. I don't have an explanation why.",1768,2019-11-17T23:15:41Z,0
110,carllerche,"So you are saying that:

### Injector

- Pushes task
- Sees searching == 1

### Worker

- Decrements searching
- Does final sweep, sees no task.",1768,2019-11-18T02:28:46Z,0
111,swarmer,"The exact order/interleaving of events is in question, but basically yes, that's how I understand what I'm seeing. I now want to try to dig into queue stuff more.",1768,2019-11-18T15:36:49Z,0
112,Matthias247,"With the `let state = State(self.state.fetch_add(0, SeqCst));` change from #1788 I can not observe the issue anymore in any of the tests that I had been trying so far.",1768,2019-11-19T04:09:16Z,0
113,hidva,"A Great Catch! Rust uses the C++ memory model, and the C++ standard stipulates:
> An implementation should ensure that the last value (in modification order) assigned by an atomic or synchronization operation will become visible to all other threads in a finite period of time. Implementations should make atomic stores visible to atomic loads within a reasonable amount of time.
> 
> Atomic read-modify-write operations shall always read the last value (in the modification order) written before the write associated with the read-modify-write operation.

This means that load is not guaranteed to see the latest value, but RMW has this guarantee, so I think it may be the root cause.",1768,2021-12-20T02:45:01Z,0
114,PureWhiteWu,Kindly ping if you missed this~ @Darksonn @Noah-Kennedy ,4250,2021-12-07T06:47:06Z,0
115,Darksonn,"I'm in the middle of exam week, so I'm busy with other stuff this week.",4250,2021-12-07T08:36:43Z,0
116,carllerche,"Thanks for submitting the PR. The code seems fine. IIRC, when this was discussed, there was a question about whether or not an env variable should change the default globally. I.e. should an env variable impact a runtime created deep within a library (for example).

I think what we could do instead is to limit the env variable to `#[tokio::main]`. So, it would only change the number of threads created for the ""main"" runtime in a bin.

This change should be discussed by @tokio-rs/maintainers though before we decide to move forward.",4250,2021-12-10T18:30:58Z,0
117,jonhoo,"My instinct here is the same as @Darksonn's initial take in https://github.com/tokio-rs/tokio/issues/4249#issuecomment-975297521, which is that this should be up to each application to do by using the builder construction. I don't know that the addition of a magic env var actually buys you that much compared to the slight increase in complexity for callers when they _do_ need this. If anything, the requirement to be explicit about the size of the thread pool being determined by an environment variable seems valuable. It also makes it easier to do things like pick up variables by other names that may already be set in the environment for this purpose.

If we _do_ decide to adopt this, I'm torn as to whether it should affect all executors or just `#[tokio::main]`. On the one hand, this is clearly intended for when you _don't_ want to use `num_cpus`, and why wouldn't that also apply to executors you did not construct directly yourself? But on the other hand, it changes the behavior of other executors in a way that's hard to opt _out_ of if you _do_ want them to continue to use `num_cpus`. I think I lean towards ""have it apply globally if set"", but this ambiguity is to me another sign that we shouldn't adopt this kind of switch.",4250,2021-12-10T21:57:02Z,0
118,davidpdrsn,I agree with @jonhoo and @Darksonn. Not sure this needs to be built in :thinking: I don't feel it gives you much outside of using the builder directly.,4250,2021-12-10T22:24:02Z,0
119,PureWhiteWu,"Firstly, I'd propose this is because in some cases, people who write code isn't people who really run and operate the application, and the author also doesn't know which environment the application will run in. And the environment sometimes have custom limitations that the num_cpus can't figure out.
For example, in our production environment, the application runs in a 192-core physical machine, but we only want it to use 4 cores, and we apply this limitation in a custom way instead of cgroup so num_cpus can't figure out(there are some complex reasons that I just skip here). In this situation, tokio creates 192 threads and we have no way to fix this in runtime(which caused great performance decrease).

Second, I think this change should be ""global"", all the libraries which create tokio multi-thread runtime should apply this change. Because if a user only creates a multi-thread runtime without specify the worker threads using the builder, then I think we can assume this should be self-adaptive in runtime, and the user gives us(tokio) the right to do this.
Now we only get the values from `num_cpus` which auto-detects the value by parsing cgroup configs. But this is limited, and sometimes may not work. I think we can also give the rights to the operator and the environment to specify this value manually. This is much more accurate and should be trusted.

Finally, sorry for my poor English😭.",4250,2021-12-11T03:41:01Z,0
120,Ralith,"> we have no way to fix this

Why can't you ask the people responsible for developing the software to read the desired number of cores from an environment variable?",4250,2021-12-11T06:51:31Z,0
121,PureWhiteWu,"> > we have no way to fix this
> 
> Why can't you ask the people responsible for developing the software to read the desired number of cores from an environment variable?

First, in our situation, there's thousands of developers in the company, and there's thousands of server-side applications, we are not able to ask everyone to write such logic in their code. It's also impossible to ask them to use a internal-version of tokio(we will never do things like fork an internal version of tokio). And this also can't solve the problem that some dependency may use the ""official"" tokio.

Second, we think this is quite a common logic, because the run-time environment (and the operator) should have ability to specify the thread count, that's what called ""cloud native"": you write the code, and you don't care (and won't know) what environment it runs in. To achieve this, we need to expose this ability. And I think that's why Go supports using ""GOMAXPROCS"" to specify the P nums.",4250,2021-12-11T07:20:32Z,0
122,tobz,"As is, I don't think this PR should be merged.

It controls a single variable of the runtime, and adds extra code that may or may not end up surprising users when the runtime doesn't behave as they expect simply because an environment variable existed when an application was started.

However, I _do_ think there could be a better pattern for allowing runtime configuration to be overridden via environment variables.

While this is a personal feeling, I'd be interested in seeing a helper type -- perhaps in `tokio-util` -- that allowed pulling in various runtime configurations via environment variable, perhaps with a configurable prefix.  Practically speaking, this type would have to be constructed directly but would read the most common runtime configuration flags/values as environment variables, and could be passed a `runtime::Builder`, where it would then apply the configuration it loaded and provide a method to actually build the `Runtime`.

While you raise a fair point that it might be difficult to get hundreds or thousands of developers to use a common pattern, such a helper type as I describe could build a runtime from environment variables in only 2-3 lines of code.  That's not many more lines than the actual Cargo.toml imports of Tokio itself.

Benefits of this approach:
- `tokio` needs no changes (better for long-term support, avoids code bloat)
- `tokio-util` is still an official library, so the helper type will be held up to the standards of the Tokio project overall
- able to configure more than just one aspect of the runtime
- has to be called directly, so there's no surprise behavior for users",4250,2021-12-11T17:02:03Z,0
123,PureWhiteWu,"Firstly, I think we need to be clear about what the user expects when he specifies to use multi-thread runtime. Has he given tokio the right to decide on the number of threads? I think the answer is ""yes"".
So the next question is: ""what's the source of truth?"" Maybe ""num_cpus"" can provide a default value, but it also has limitations, it can only detect the settings of cgroup, it doesn't know anything else. So I think we can also expose this ability to ask the runtime to specify this value, this is reasonable and useful.

Secondly, I have also considered your approach, but this still does not solve the problem. Almost all users use `#[tokio::main]` directly on their `main` func to use tokio, If we only provide something in `tokio-util`, then we also can't ask everyone to change their code to manually build the runtime by using something from `tokio-util`.",4250,2021-12-12T04:54:42Z,0
124,Darksonn,"Eh, I agree that putting the feature to do this in `tokio-util` isn't really a solution. People aren't going to be using that.",4250,2021-12-12T09:34:38Z,0
125,PureWhiteWu,"Hello, do you have any more concerns about this PR?
Thanks.",4250,2021-12-21T02:57:06Z,0
126,Darksonn,"Well, the current status is that there is no consensus about the shape we want this feature in. There isn't anything wrong with the implementation per se.",4250,2021-12-21T09:25:45Z,0
127,hawkw,"Tasks can be named using an unstable Tokio API: https://github.com/tokio-rs/tokio/issues/4114

Currently, I believe Tokio's documentation is built on docs.rs without unstable features enabled. This means they don't show up in the docs.rs documentation. This should probably be fixed; we should probably build the documentation with the unstable features included, but flagged as unstable, so that users know that `--cfg tokio_unstable` is required to use those APIs.

The console docs don't currently include `task::Builder` API because it's a `tokio` API, rather than a `console-subscriber` one. But we should probably mention that it exists and how to use it.

I'm transferring this issue to the Tokio repository, though, because I think the most important change is probably to ensure that unstable features are visible in the docs.rs documentation.",4328,2021-12-19T01:53:54Z,0
128,davidpdrsn,Removing my PR review request. I'm happy when alice is happy 😊,4331,2021-12-21T10:16:57Z,0
129,Darksonn,"We typically set Tokio to depend on the oldest version that:

 1. Is semver-compatible with the newest version.
 2. Has all of the features used by Tokio.

So in this case it is not clear why we should _require_ the new version. People will get the newest version by default even with our current dependency configuration.",4332,2021-12-21T09:29:33Z,0
130,PureWhiteWu,"> We typically set Tokio to depend on the oldest version that:
> 
> 1. Is semver-compatible with the newest version.
> 2. Has all of the features used by Tokio.
> 
> So in this case it is not clear why we should _require_ the new version. People will get the newest version by default even with our current dependency configuration.

Because the old version has some bug, and the new version fixed the bug.",4332,2021-12-21T09:53:04Z,0
131,Darksonn,"Yes, I understand, but with the current configuration, people can still use the new version - in fact, they are getting the new version by default.",4332,2021-12-21T11:15:26Z,0
132,PureWhiteWu,"> Yes, I understand, but with the current configuration, people can still use the new version - in fact, they are getting the new version by default.

If they use --locked, then they may get the old version.
If we update this crate to 1.13.1, is there any risks or cons for us?
This update doesn't affect MSRV.",4332,2021-12-21T12:24:59Z,0
133,Darksonn,The main con is that we prevent users from choosing to use an older version of `num_cpus`.,4332,2021-12-21T12:52:27Z,0
134,PureWhiteWu,"> The main con is that we prevent users from choosing to use an older version of `num_cpus`.

Just an inquiry: so we won't update a version of our dependency for bug fixes? Or in what situation we will update our dependency?",4332,2021-12-21T14:13:06Z,0
135,Darksonn,"I think the important thing to note here is that we are talking about increasing the _minimum required_ version of the dependency. We usually only bump the minimum required version in two situations:

 1. The dependency released a new breaking change. In this case, we will upgrade to the new major version.
 2. Tokio requires some feature that was released in a newer version of the crate, and Tokio will not compile if you are using the old version. In this case, we will bump the minimum required version to the smallest version that has all of the features that Tokio are using from that crate.

Nothing here prevents people from getting the bugfix, and indeed they will get it by default when starting a new project or when they call `cargo update`.",4332,2021-12-21T14:57:54Z,0
136,PureWhiteWu,Thanks,4332,2021-12-22T02:21:36Z,0
137,freesig,"Ok some more information. I tried moving the `block_in_place` to before the wasmer calls but the error shows up so perhaps this has nothing to do with wasmer.
I think it might be something to do with calling `runtime.block_on` from within a `block_in_place` but its showing up so infrequently that it's hard to tell. 
I'm going to write some more tests to try and flush it out and will report the results back here.
",3796,2021-05-17T23:31:28Z,0
138,carllerche,"This is very strange. I reviewed the code in Tokio. Because it relates to thread-local state, there should be no concurrency factor. I also reviewed the relevant code paths and am not sure how that assertion could be hit.

Does wasmer have its own thread or thread-local implementation?",3796,2021-05-18T17:55:40Z,0
139,freesig,"After some more investigation I found [another](https://github.com/holochain/holochain/blob/d16477e04b0a0266642e105d85e4bb4cf3d5070e/crates/holochain_sqlite/src/db.rs#L48) `block_in_place` in our code base that was causing this. After removing it the issue has gone away. It was being called from the wasmer side so I suspect there must be something odd with the wasmer threads and as you suggest perhaps the thread-local is handled in an odd way there.
I'm not really sure where to go with this issue now, perhaps it's an issue with wasmer and not tokio anyway so maybe it can be closed.",3796,2021-05-19T11:37:53Z,0
140,Darksonn,Closing as it doesn't appear to be a bug in Tokio.,3796,2021-06-29T11:36:13Z,0
141,Machine90,"I've found this comment in ""std::thread::local::statik::key"" (__StaticLocalKeyInner): ""On some platforms like wasm32 there's no threads, so no need to generate thread locals and we can instead just use plain statics!"". I guess this is the root cause.",3796,2021-12-22T05:50:46Z,0
142,Darksonn,"> I think in an async context, it only makes sense to `notify_one` if you know that the tasks which are waiting will not be cancelled.

This is most likely a bad assumption. For example, most web servers will cancel a request handler if the remote connection is closed, and I'm guessing this also applies to rocket. I do not think we can just ignore cancellation.",3892,2021-06-26T08:01:53Z,0
143,ijackson,"FYI I got nerdsniped by this over the weekend.  I have an implementation (which does something more useful with cancellation) and am sorting out the tests, docs, etc.  It turned out as a separate crate.

I definitely want something like <strike>#3742</strike> #3741 so I will send an MR for that.",3892,2021-06-28T16:01:33Z,0
144,ijackson,"Well, I think that yak is mostly shaved now.  The result so far is here:
  https://crates.io/crates/async-condvar-fair

Here is now I dealt with the cancellation issue:
  https://docs.rs/async-condvar-fair/0.1.0/async_condvar_fair/struct.Baton.html

I haven't yet revisited #3741 which would make the use rather more convenient.

I think Tokio could do with a native condvar which uses the same Baton technique for handling cancellation, but which is not necessarily fair.  My runtime-agnostic crate can't optimise which task to wake but Tokio's condvars could.

I think the ability to use different mutexes is important.  There are good reasons for choosing both sync and async ones.",3892,2021-07-05T15:55:08Z,0
145,Darksonn,"I agree that the cancellation problems are avoidable if you allow spurious wakeups. I do like the idea of the `wait_baton` method. 

I think we will run into trouble if we attempt to implement a `Condvar` that is able to talk to a synchronous mutex. The guard is not `Send`, and I don't see any way to have the compiler understand that the mutex is unlocked before yielding.",3892,2021-07-06T15:06:28Z,0
146,ijackson,"> I agree that the cancellation problems are avoidable if you allow spurious wakeups. I do like the idea of the `wait_baton` method.

I think spurious wakeups are part of the spec :-).  They don't cause trouble in practical uses of condvars, provided that they aren't frequent enough to be perf issue.

> I think we will run into trouble if we attempt to implement a `Condvar` that is able to talk to a synchronous mutex. The guard is not `Send`, and I don't see any way to have the compiler understand that the mutex is unlocked before yielding.

I didn't find this a problem for `async-condvar-fair`.  My test cases seem to work just fine with `std::sync::Mutex`.  When you use a sync mutex you don't need to hold the gaurd over an await point - you only suspend inside condvar `wait` with the mutex unlocked.",3892,2021-07-06T15:41:01Z,0
147,Darksonn,"Just because they are parts of the specification for `std::sync::Condvar` doesn't mean they have to be for our `Condvar`. As for your test-cases, the problem is only triggered if the problematic code is inside a spawned task — and not if its in the main method of the test.",3892,2021-07-06T15:46:04Z,0
148,ijackson,"I have poked my tests some more and it turned out that all my test cases were using single-threaded executors.  I added a `Send` bound to my tasks and now I see the problem you are describing.

```
38 |     let mut guard = lock_async(&self.mx).await;
   |         --------- has type `Guard<'_, u32>` which is not `Send`
...
44 |       guard = self.cv.wait(for_wait(guard, &self.mx)).await;
   |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ await occurs here, with `mut guard` maybe used later
45 |     }
46 |   }
   |   - `mut guard` is later dropped here
```

This is rather disappointing because actually the `guard` that goes into `wait` is a different one to the one that comes back from it, so it is not actually held.  What is held instead is the future from `Condvar::wait`, which *is* `Send`.

Anyway, it can be made to work with `parking_lot::Mutex` since `parking_lot` has a `send_guard` feature which makes its guards `Send`.",3892,2021-07-06T16:12:19Z,0
149,ijackson,"I've verified that with `parking_lot`'s `send_guard` enabled, my test task futures are all `Send`.  So it is indeed possible to use an async condvar with a sync mutex, which is nice - although it complicates the design and API of even a runtime-specific async condvar...",3892,2021-07-06T16:46:54Z,0
150,kaimast,"Hi all, 

I thought about this some more.

As far as I understand, using`Notify` in the case where you always have at most one waiter should be more or less the same as using a Condvar. There might just be more suprious wakeups as permits are stored even if no one is waiting.

It is a little more tricky with multiple waiters. The problem here is that after checking the condition and releasing the lock, the other task might call `notify_waiters` before the waiting task calls `notified`.
However, it should be sufficient to provide a method like `notified_with_lock`, that takes a MutexGuard and does not drop it until the current thread is added to the `Notify`'s waiting list.

Additionally, it might be good to have some kind of helper function/macro to build a waiting loop. I usually write something like this which seems very verbose

```rust
loop {
    // Scope MutexGuard to avoid deadlock
    {
          let lock = mutex.lock().await;
          if check_condition(&*lock) {
                 break;
          }
     }

     // Note this only is safe if the other task calls notify_one, not notify_all
     notify.notified().await;
}
```

Unrelated: I am also not super happy with the current terminology in `Notify`.  Usually methods/functions should contains verbs and type names should be nouns. 
What was the reasoning for calling it `Notify::notified` instead of, for example, `Notfiy::wait`. And calling it `Notify` instead of `Notifier`?",3892,2021-12-24T23:13:20Z,0
151,carllerche,"Sounds very interesting. Could you provide more context around what wasm platform you are targetting? As far as I know, in the browser, there are no TCP sockets. I'm not familiar w/ other wasm platforms.",1597,2019-09-25T15:41:32Z,0
152,AchalaSB,"@carllerche  We need all the network protocols like rpc, tcp, websocket and I/O operations

I felt we should first focus on `clang` compiler as `wasm32-unknown-unknown` target is closely embedded with Rust.  Its a Rust compiler backend for WebAssembly( without emscripten)

If anyone want to use any Http calls its depends on tokio, hyper, websocket libraries. so we need to make sure all the network protocols can be handled in Rust. 
To do so the core is in `tokio.rs` so I came here for the support.
",1597,2019-09-26T06:38:41Z,0
153,davidgraeff,This is about WASI (https://hacks.mozilla.org/2019/03/standardizing-wasi-a-webassembly-system-interface/) I guess?,1597,2019-09-26T11:00:11Z,0
154,DoumanAsh,"@AchalaSB Support of wasm would require someone to investigate wasm capability.
There are many possible variants when building for wasm, and stuff like tcp/udp is definitely not available on browser.

If you're familiar with wasm platforms, it would be good if you could elaborate on availability of IO primitives and event loops there",1597,2019-10-01T11:33:35Z,0
155,carllerche,Is there a wasm platform today that supports TCP?,1597,2019-10-02T17:36:06Z,0
156,sfackler,"The `wasm32-unknown-wasi` target will in the future I believe, but it doesn't look like it does currently: https://github.com/rust-lang/rust/blob/master/src/libstd/sys/wasi/net.rs",1597,2019-10-02T17:41:58Z,0
157,quininer,"Tokio has the best sync primitive implementation, it would be great if it could be used on wasm.",1597,2020-08-06T13:04:04Z,0
158,phritz,"> tokio has the best sync primitive implementation, it would be great if it could be used on wasm.

Indeed tokio's non-networking primitives would be extremely useful in wasm in the browser: channels, mutexes, (single-threaded) runtime, etc.",1597,2020-08-25T05:28:21Z,0
159,MikailBag,"tokio with feature flags `sync` and `rt` successfully compiles for `wasm32-unknown-unknown`.
I didn't test if it works though.",1597,2020-11-05T14:39:36Z,0
160,Joshhua5,This is also a dependency for Sentry.io's rust implementation ,1597,2021-07-27T23:37:23Z,0
161,Rochet2,"> tokio with feature flags `sync` and `rt` successfully compiles for `wasm32-unknown-unknown`.
> I didn't test if it works though.

I tried these feature flags with wasmtime+WASI and it did not seem to quite work.
When waiting on a pending future, the system crashes with the following:
```
thread 'main' panicked at 'condvar wait not supported', library/std/src/sys/wasi/../unsupported/condvar.rs:23:9
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
Error: failed to run main module `target/wasm32-wasi/debug/example.wasm`
```
Seems I would need to somehow work around the current pending future wait implementation.
I tried looking into if there is an API for the tokio runtime yielding control (stop execution) and resume later, but I could not see any such API. Is there one?
It would be very useful in single threaded contexts like Wasm where the control of the program may not be in Wasm. Control should periodically be yielded outside of Wasm.
",1597,2021-08-31T10:01:37Z,0
162,d0u9,Tokio heavily depends on the epoll mechanism underlayer which is not supported in wasm runtime right now as I know. The wasm runtime(not in the browser) is sandboxed and is wrapped by a WASI layer. The WASI provides simple libc abilities and has some constriains on accessing host resource.,1597,2021-12-23T12:25:53Z,0
163,teohhanhui,"Not sure if these are relevant, but there is `wasi::poll_oneoff`:

https://docs.rs/wasi/0.10.2+wasi-snapshot-preview1/wasi/fn.poll_oneoff.html

and `wasi_tokio::sched::poll_oneoff`:

https://docs.rs/wasi-tokio/0.32.0/wasi_tokio/sched/fn.poll_oneoff.html",1597,2021-12-25T14:48:21Z,0
164,Darksonn,"You can get this by combining a [`tokio::io::duplex`][1] with [`tokio_util::io::ReaderStream`][2].

[1]: https://docs.rs/tokio/latest/tokio/io/fn.duplex.html
[2]: https://docs.rs/tokio-util/latest/tokio_util/io/struct.ReaderStream.html",4345,2021-12-25T18:44:28Z,0
165,itamarst,"Ah hah! Here's the code using that suggestion:

```rust
let (mut asyncwriter, asyncreader) = tokio::io::duplex(256 * 1024);
let streamreader = tokio_util::io::ReaderStream::new(asyncreader);
```",4345,2021-12-25T20:56:16Z,0
166,itamarst,"Perhaps this is more of a documentation issue? As a beginner I could not have figured that out on my own; I know enough of this sort of thing to know what the shape should be, but not where to look.",4345,2021-12-25T20:57:20Z,0
167,Darksonn,"I don't disagree, but where should we put this? One of the main challenges with stuff like this is putting it somewhere people will find it when they need it.

I already updated my response in the thread you linked.",4345,2021-12-25T21:09:46Z,0
168,itamarst,"In the four-quadrants model of documentation (https://documentation.divio.com/) this would be a how-to, I think. ""How to connect  AsyncRead/AsyncWrite/Sink/Stream"" and then show all the combinatorical variants. So probably under https://tokio.rs/tokio/topics? As a companion to the sync topic. And maybe another on bridging `Read` and `Write`.

I also had to do ""here's the `Stream` from Actix, turn it into a `AsyncRead`"" for the upload side, and that was slightly tricky since I had to adapt error types from `PayloadError` to `std::io::Error`; eventually I figured out `Stream` has `map`:

```rust
async fn upload_report(
    payload: web::Payload,
    config: web::Data<Configuration>,
) -> Result<web::Json<DownloadKey>, ApiError> {
    let mut payload_asyncread =
        StreamReader::new(payload.map(|i| {
            i.map_err(|_| std::io::Error::new(std::io::ErrorKind::Other, ""payload error""))
        }));
    // ...
    config
        .storage_bucket
        .put_object_stream(&mut payload_asyncread, format!(""/{}"", download_key))
        .await
        .map_err(|e| {
            error!(error = %e);
            ApiError::S3
        })?;
    info!(download_key = %download_key);

    Ok(web::Json(download_key))
}
```",4345,2021-12-25T21:18:05Z,0
169,Darksonn,"Yeah, I suppose it makes sense if you combine all the conversions into a single post. I'll put it on my endless list of topics articles I should write.",4345,2021-12-25T22:00:13Z,0
170,itamarst,Thank you!,4345,2021-12-26T14:23:28Z,0
171,Darksonn,"You will need to reuse the `stdout` object. 
```Rust
use tokio::io::{self, AsyncBufReadExt, AsyncWriteExt, BufReader};

#[tokio::main]
async fn main() -> io::Result<()> {
    let mut stdout = io::stdout();

    stdout.write_all(b""What's your name?: "").await?;
    stdout.flush().await?;
    let mut buf = String::new();
    let mut stdin = BufReader::new(io::stdin());
    stdin.read_line(&mut buf).await?;

    let greeting = ""Hello, "".to_string() + &buf;
    stdout.write_all(greeting.as_bytes()).await?;
    stdout.flush().await?;
    Ok(())
}
```",4347,2021-12-27T11:16:05Z,0
172,sfackler,"`""0.2.0""` does not pin to that exact version - it supports any `0.2.x` version.",4350,2021-12-27T19:44:11Z,0
173,ReagentX,"Is the warning `cargo` emits safe to ignore, then?",4350,2021-12-27T19:45:43Z,0
174,sfackler,Run `cargo update`.,4350,2021-12-27T19:47:12Z,0
175,Darksonn,You are welcome to submit such a PR.,4340,2021-12-25T18:11:26Z,0
176,Darksonn,Thanks.,4352,2021-12-28T14:08:26Z,0
177,taiki-e,"> error: no such subcommand: `hack`
> 
> 	Did you mean `check`?

Sorry, I forgot to mention the install of `cargo-hack`...",3131,2020-11-11T21:00:19Z,0
178,taiki-e,"> error: use of deprecated constant `std::sync::ONCE_INIT`: the `new` function is now preferred
>  --> tokio/src/process/unix/mod.rs:65:1
>   |
> 65 | / lazy_static::lazy_static! {
> 66 | |     static ref ORPHAN_QUEUE: OrphanQueueImpl<StdChild> = OrphanQueueImpl::new();
> 67 | | }
>    | |_^
>    |
>    = note: `-D deprecated` implied by `-D warnings`
>    = note: this error originates in a macro (in Nightly builds, run with -Z macro-backtrace for more info)

It seems `lazy_static`'s requirement needs to be raised to 1.4.
https://github.com/rust-lang-nursery/lazy-static.rs/commit/1dbd5ae6ccb33c1f88fe521c9ab1f8f0e4cf5193",3131,2020-11-11T21:11:37Z,0
179,taiki-e,"> Currently by not enabling the new feature resolver, test features are enabled even when building Tokio in non-test mode. This means e.g. that we never build Tokio with only the `fs` feature in CI because test-util depends on the `rt` feature, enabling that too.

This is due to v1 resolver merging the features of normal dependencies and the dev-dependencies
cargo-hack's --no-dev-deps, --remove-dev-deps, cargo's unstable -Zfeatures=all, -Zfeatures=dev-deps, -Zavoid-dev-deps can handle this issue. ([cargo-hack is originally designed to handle these problems that occur with v1 resolvers.](https://github.com/tokio-rs/tokio/pull/1695))
Our CI already uses one of them, so it works correctly.

https://github.com/tokio-rs/tokio/blob/dc1894105bfacb541d219804d7678b5108f54359/.github/workflows/ci.yml#L210

https://github.com/tokio-rs/tokio/blob/dc1894105bfacb541d219804d7678b5108f54359/.github/workflows/ci.yml#L245-L247

---

> Adding the resolver = ""2"" option to our workspace Cargo.toml fixes the issue on new rustc versions, but it causes our MSRV to emit an error when building Tokio.

AFAIK cargo does not provide a stable way to change the resolver version at runtime, but I think it would be easy to add the ability to do that to cargo-hack.",4357,2021-12-29T23:05:48Z,0
180,Darksonn,"Alright, it's good that we are handling it properly. Thanks for verifying.",4357,2021-12-29T23:15:12Z,0
181,Noah-Kennedy,Would v0.8 add support for `UdpSocket::peer_addr`?,4135,2021-09-25T19:14:22Z,0
182,Thomasdezeeuw,"> Would v0.8 add support for `UdpSocket::peer_addr`?

Yes, I think v0.7 couldn't because it was only added 1.40 which was too new at the time.",4135,2021-09-25T20:48:53Z,0
183,hawkw,"It's worth noting that Mio's `TcpSocket` already exposes some additional socket options that Tokio doesn't expose:
- [`SO_REUSEPORT` (on Unix only)](https://github.com/tokio-rs/mio/blob/27fbd5f04bb5f52a4d1c358cf0c04c6074a3d46b/src/net/tcp/socket.rs#L90-L95)
- [`SO_LINGER`](https://github.com/tokio-rs/mio/blob/27fbd5f04bb5f52a4d1c358cf0c04c6074a3d46b/src/net/tcp/socket.rs#L104-L107).

We may want to expose these as well
",3082,2020-10-31T18:58:28Z,0
184,hawkw,"Opened tokio-rs/mio#1384 to expose buffer size, and tokio-rs/mio#1385 to expose the keepalive interval. We may also want to add SO_NODELAY...",3082,2020-10-31T22:24:11Z,0
185,Darksonn,I think it makes total sense to add these methods to `TcpSocket`. :+1:,3082,2020-11-01T15:31:40Z,0
186,Freaky,"I ran into a similar difficulty while updating [tarssh](https://github.com/Freaky/tarssh) to Tokio 0.3 - I was [setting send/recv buffers](https://github.com/Freaky/tarssh/blob/f6f33e2a888f9549c2549fc320224d15e684c6ea/src/main.rs#L112-L116) on the `TcpStream` from an `accept()` call, but there seems no way to ([sensibly](https://gist.github.com/Freaky/03bcfdd5fa1a2a3e1146d7018dbaecb7)) replicate this behaviour in 0.3?",3082,2020-11-26T22:55:29Z,0
187,biluohc,So how to set TCP keepalive for socket from accept()?,3082,2021-04-23T07:52:32Z,0
188,Darksonn,"If Tokio does not expose a method for doing it, you can do it with the `socket2` crate using something like [`SockRef`][1].

[1]: https://docs.rs/socket2/0.4/socket2/struct.SockRef.html",3082,2021-04-23T08:59:28Z,0
189,thedodd,"Having used these methods from tokio 0.2 — namely `set_keepalive` — it would be great to have these in tokio 1.0 as well. For now, I'm using SockRef as @Darksonn mentioned (thanks @Darksonn).",3082,2021-06-07T16:45:02Z,0
190,esavier,"Is there any reasoning why available socket options are not exposed?
This kind of scratches Tokio out from some application",3082,2021-06-08T11:36:28Z,0
191,silence-coding," is there a better way to  set keepalive? The way I'm doing it right now is inelegant.
#4028 ",3082,2021-08-05T11:56:04Z,0
192,esavier,"yeah, i have no idea why setting options is not in the tokio api, but currently this is only option that i am aware of. Basically you have to create external socket structure and hopefully cast it to the tokio's version after setting the correct option. ",3082,2021-08-05T12:25:08Z,0
193,taiki-e,"As for the socket2 issue, I don't think it should be considered a blocker, but if one of the reviewers wants to make it a blocker, I'm fine with that.",4270,2021-11-23T06:13:48Z,0
194,taiki-e,"I got two approvals and no one had raised any objections for over a month, so I'll merge this.",4270,2021-12-31T03:28:04Z,0
195,carlosb1,"I did a [proposal](https://github.com/carlosb1/tokio/tree/3312_request_new_method_peer_addr) , but it is necessary update mio to 0.8.0",3312,2021-01-08T19:46:07Z,0
196,Darksonn,"Also, there's a merge conflict preventing this from getting merged.",4351,2021-12-30T17:54:58Z,0
197,taiki-e,(CI failure will be fixed in https://github.com/tokio-rs/tokio/pull/4361),4351,2021-12-31T09:58:16Z,0
198,Darksonn,You can merge in master to your branch to fix the CI failure.,4351,2021-12-31T10:31:40Z,0
199,Darksonn,"Unfortunately this is not possible because a mutable reference has to be unique, but you could call the `with_mut` method inside itself to get two mutable references. The intention is that you use a `Cell` or `RefCell` to do this.",4375,2022-01-04T09:01:23Z,0
200,hawkw,"IMO, the main benefit for keeping these types private is to reserve the ability to replace them with `async fn`s if/when that becomes possible. I'm less concerned about cluttering the documentation with combinator types, as we could also add `#[doc(hidden)]` attributes, or export them all in a submodule so that they don't show up in the main `stream` module.

If reserving the ability to make them `async fn`s someday is the primary reason to keep them private, we should then think about whether this is likely to happen. If/when `async fn` trait methods are possible on stable, will we want to rewrite these combinators to be `async fn`s?

It's worth noting that if we want to reserve the ability to use `async fn`s for these methods eventually, that _is_ still a breaking change with the current API surface. Currently, the various `Stream` combinators will all automatically impl `Unpin` if all their type parameters do, while an `async fn`-based implementation will never be `Unpin`. Therefore, using `async fn` would be a breaking change.

If being able to replace these combinators with `async fn`s is something we care about, we should probably also add `PhantomPinned` to them in 0.3, making them `!Unpin`.  If we _don't_ make them `!Unpin`, we are essentially committing to not replacing them with `async fn` for the next three years after when Tokio 1.0 is released.  From my perspective, if we don't make the combinators `!Unpin`, then we should just go ahead and export the types publicly, especially since many users seem to want this.",2723,2020-07-30T21:52:23Z,0
201,Darksonn,"Note that this issue is specifically about combinators, and that nothing in the list of methods actually returns a `Future`. ",2723,2020-07-31T06:50:14Z,0
202,hawkw,"Whoops! In that case, the motivation for not exposing the combinators that I mentioned doesn't really apply at all.",2723,2020-07-31T17:29:47Z,0
203,alce,"As far as I understand, there are at least two things to consider:

* Stabilization of the `Stream` trait in std. It looks like the trait will include the `next` method and `Next` future and could happen relatively quickly.

* The potential availability of generators, which will produce `!Unpin` streams. At this point probably most streams will be `!Unpin`.  As @hawkw suggests, we could force combinators to not implement `Unpin` . This  means streams would need to be pinned before iteration. 

Another possibility can be not to use combinators at all and return `Pin<Box<dyn Stream<...>>>` in lieu of `impl Stream<..>` although it seems a little heavy handed.

Edit: ...on second thought the `Pin<Box<...>>` thing doesn't make sense, does it?",2723,2020-09-10T18:33:01Z,0
204,carllerche,This is not critical to solve for 0.3. I am going to punt.,2723,2020-10-08T20:06:17Z,0
205,osa1,"In the meantime, is there anything users can do to e.g. pass return value of `StreamExt::fuse` to a function? Because the type is not public, we currently can't implement a function that takes return value of `StreamExt::fuse` as argument. Do we have any workarounds for this?",2723,2020-11-12T14:26:27Z,0
206,Darksonn,"Sure, you can use generics to accept any stream.
```rust
fn takes_stream<S: Stream<Item = ...>>(s: S) {
    ...
}
```
In the specific case of `fuse`, you could also use the analogous method from the futures crate.",2723,2020-11-12T14:32:00Z,0
207,chapa,"Hi!

Until combinators of streams are public, is it possible to write a function that returns a `Throttle`?
```rust
pub fn foo() -> /* what do I put here ? */ {
    stream::iter(vec![1, 2, 3]).throttle(Duration::from_secs(1))
}
```
If I try to use a generic type:
```rust
pub fn foo<S: Stream<Item = i32>>() -> S {
    stream::iter(vec![1, 2, 3]).throttle(Duration::from_secs(1))
}
```
I get the error:
```
pub fn foo<S: Stream<Item = i32>>() -> S {
           - this type parameter       - expected `S` because of return type
    stream::iter(vec![1, 2, 3]).throttle(Duration::from_secs(1))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ expected type parameter `S`, found struct `stream::throttle::Throttle`
```
I don't understand, why isn't `Throttle` a `S`?",2723,2020-11-30T00:02:54Z,0
208,nytopop,"@chapa 

> I don't understand, why isn't Throttle a S?

Because `S` is decided by whoever calls `foo`, not `foo`. For example, someone might attempt to call it in such a way that `S` is _not_ the return type of `stream::iter`, as in `foo::<SomethingElse>()`.

Returning an [`impl Trait`](https://doc.rust-lang.org/edition-guide/rust-2018/trait-system/impl-trait-for-returning-complex-types-with-ease.html#return-position) should work in this case:

```rust
pub fn foo() -> impl Stream<Item = i32> {
    stream::iter(vec![1, 2, 3]).throttle(Duration::from_secs(1))
}
```",2723,2020-11-30T02:24:18Z,0
209,Darksonn,"Yeah, the throttle stream may not be an `S`, since the caller might have chosen `S` to be some other stream type than `Throttle`.",2723,2020-11-30T07:54:26Z,0
210,chapa,"Ok thank you, `impl Trait` solves the problem in that case indeed. Actually I should have explain my real problem instead of trying to find a simpler equivalent that is not so equivalent :sweat_smile:

I wanted a trait with a method that returns a `Stream` but I couldn't have `impl Stream` as return type because ""`impl Trait` not allowed outside of function and inherent method return types"".
And since I can't use generics on the method (as we saw here), I ended up with an associated type :
```rust
pub trait MyTrait {
    type Stream: Stream<Item = i32>;

    fn foo(&self) -> Self::Stream;
}
```
But now I'm stuck on trait implementations, I would have liked to do something like this :
```rust
pub struct MyStruct {}

impl MyTrait for MyStruct {
    type Stream = impl Stream<Item = i32>;

    fn foo(&self) -> Self::Stream {
        stream::iter(vec![1, 2, 3]).throttle(Duration::from_secs(1))
    }
}
```
But `impl Trait` in type aliases is unstable and needs the `type_alias_impl_trait` feature.

Is there another way to have a trait's method returning a `Stream` ? (without enabling unstable features)",2723,2020-11-30T10:19:01Z,0
211,asonix,Rather than returning an `impl Stream` you could try returning a `Box<dyn Stream>` or a `Pin<Box<dyn Stream>>` and suffer the allocation until it's possible to return `impl Foo` in traits,2723,2020-11-30T14:16:48Z,0
212,chapa,"It works well with a `Pin<Box<dyn Stream>>`, thank you! I'm not sure if I prefer this or using the unstable feature, but now I know it's possible without it.",2723,2020-11-30T16:26:16Z,0
213,carllerche,Exposing combinators is not a breaking change. I am going to remove the 1.0 tag.,2723,2020-12-09T18:52:40Z,0
214,talchas,"Is it a good idea in the first place to have these combinators which cause conflicts with `futures::stream::StreamExt` but don't replace `futures` entirely? `timeout` and `throttle` obviously are tokio-specific, and `all`, `any`, `merge` at least don't exist at the moment in `futures`. The rest however cause name conflicts the moment you need any of the combinators in `futures` that don't exist in `tokio`.",2723,2020-12-12T17:51:54Z,0
215,carllerche,Tokio aims to provide everything commonly needed to implement an async app with Rust.,2723,2020-12-12T21:29:15Z,0
216,NeoLegends,"This this a decision you intend to stick to or is that something you might want to reconsider in the future? Reason being I presume most projects will rely on both `futures` and `tokio` at the same time and it might just make the adoption story easier for users if those conflicts didn't arise.

That said, that's a decision that can be made when time comes to decide whether to expose the combinators, or not.",2723,2020-12-16T16:06:02Z,0
217,Darksonn,"Note that with the introduction of the `async-stream` crate, exposing the various combinator types in that crate is something we can experiment with, without having to commit to that for Tokio 1.0.",2723,2020-12-17T14:27:22Z,0
218,ahornby,"Hi, I'd like to be able to call Chain::into_inner() to get back to the two component parts,  what's the recommended route to be able to do that?",2723,2021-07-11T18:37:56Z,0
219,alcjzk,"Why not outsource this decision to the users of the crate, by having combinators re-exported in a module(s) behind a feature flag? That way those who need access to these types can choose said access with the cost of some extra namespace clutter.

I don't really see why this would need to be an API level restriction.",2723,2022-01-04T09:13:22Z,0
220,Darksonn,"I am open to exposing the combinators in tokio-stream, but I don't think it makes sense to hide them behind a feature flag. ",2723,2022-01-04T09:46:39Z,0
221,Darksonn,Thoughts? @Thomasdezeeuw ,4349,2021-12-27T13:36:28Z,0
222,Thomasdezeeuw,"@Darksonn my first instinct would be that Tokio isn't checking [`Event::is_error`](https://docs.rs/mio/latest/mio/event/struct.Event.html#method.is_error) when handling read events, but I don't really know where the related code is so it's hard for me to say. If you can point to the related code I can look into it further.",4349,2021-12-27T14:10:46Z,0
223,Darksonn,"Well, it is getting the events [here][1]. The `set_readiness` call goes [here][2].

[1]: https://github.com/tokio-rs/tokio/blob/78e0f0b42a4f7a50f3986f576703e5a3cb473b79/tokio/src/io/driver/mod.rs#L170
[2]: https://github.com/tokio-rs/tokio/blob/78e0f0b42a4f7a50f3986f576703e5a3cb473b79/tokio/src/io/driver/scheduled_io.rs#L152",4349,2021-12-27T15:40:05Z,0
224,Thomasdezeeuw,Looking at [`Ready::from_mio`](https://github.com/tokio-rs/tokio/blob/78e0f0b42a4f7a50f3986f576703e5a3cb473b79/tokio/src/io/driver/ready.rs#L38-L69) it doesn't seem to consider `mio::Error::is_error`. So it already goes wrong in the call here: https://github.com/tokio-rs/tokio/blob/78e0f0b42a4f7a50f3986f576703e5a3cb473b79/tokio/src/io/driver/mod.rs#L174,4349,2021-12-27T16:52:15Z,0
225,BraulioVM,"What should the fix do? Should `async_fd.readable().await` return an error whenever the `EPOLLERR` happens? Or should it return success (and the user will get the error when attempting a read)? 

The first one makes more sense to me, but I want to hear what you think before working on an implementation.",4349,2022-01-04T14:01:19Z,0
226,Thomasdezeeuw,"> What should the fix do? Should `async_fd.readable().await` return an error whenever the `EPOLLERR` happens? Or should it return success (and the user will get the error when attempting a read)?
> 
> The first one makes more sense to me, but I want to hear what you think before working on an implementation.

If `EPOLLERR` is returned went something went wrong, so the most logical path would be to return the error to the user. Getting the error can be a bit tricky though. For sockets we have `SO_ERROR`, but I'm not sure a (fd) generic version of that exists.

If the error can't be retrieved we *could* pretend the fd is readable and let the user perform a read operation, that *should* also return the error. However I'm not sure it's guaranteed, for example I remember `fsync` in the past loosing errors (on Linux) but I'm a bit fussy on it.",4349,2022-01-04T14:08:16Z,0
227,BraulioVM,"> If EPOLLERR is returned went something went wrong, so the most logical path would be to return the error to the user. Getting the error can be a bit tricky though. For sockets we have SO_ERROR, but I'm not sure a (fd) generic version of that exists.

Could we not return an error to the user that basically just says ""EPOLLERR happened"", and let them retrieve the actual underlying error in the way the find appropriate? So, for instance, they could decide to read `SO_ERROR` if it's sockets they are dealing with. Or they would attempt a `read` if they don't have better options",4349,2022-01-04T14:11:09Z,0
228,3442,"> What should the fix do? Should `async_fd.readable().await` return an error whenever the `EPOLLERR` happens? Or should it return success (and the user will get the error when attempting a read)?
> 
> The first one makes more sense to me, but I want to hear what you think before working on an implementation.

The current behavior of `async_fd.writable().await` is to succeed (try the last test in my original post, it `unwrap()`s without panic). So I think it would be quite inconsistent for `readable().await` to not do exactly the same. On the other hand, this behavior does not seem to be documented at all, so the more correct way of returning a ""EPOLLERR happened"" error is feasible as long as the current `writable()` behavior is treated as another bug.",4349,2022-01-04T14:20:34Z,0
229,Thomasdezeeuw,"> Could we not return an error to the user that basically just says ""EPOLLERR happened"", and let them retrieve the actual underlying error in the way the find appropriate? So, for instance, they could decide to read `SO_ERROR` if it's sockets they are dealing with. Or they would attempt a `read` if they don't have better options

Possibly, but then it would have to be mapped to `io::Error` and the documentation spell out that itself is not really an error (or at least not the source of the error).",4349,2022-01-04T14:21:32Z,0
230,BraulioVM,"> The current behavior of async_fd.writable().await is to succeed (try the last test in my original post, it unwrap()s without panic). 

This can be traced down to 
https://github.com/tokio-rs/mio/blob/dca2134ef355b3c0d00e8e338e44e7d9ed63edac/src/sys/unix/selector/epoll.rs#L180-L187
https://github.com/tokio-rs/mio/blob/05009e4f60335fa00e9ea6a118595548afee0607/src/event/event.rs#L116-L123

An event with `EPOLLERR` is considered `is_write_closed`, which wakes up awaiters with a write interest. However, `EPOLLERR` does not affect `is_read_closed`. 

Maybe, given this issue, it should? Or would that mask errors that users would want to handle explicitly? If so, which sounds reasonable, why does that same logic not apply to `is_write_closed`? 

If `EPOLLER` affected `is_read_closed` in mio, that would lead (I think, haven't checked) to @3442's `readable().await` being woken up and then the next `read` would fail.",4349,2022-01-04T15:18:46Z,0
231,Thomasdezeeuw,"> This can be traced down to https://github.com/tokio-rs/mio/blob/dca2134ef355b3c0d00e8e338e44e7d9ed63edac/src/sys/unix/selector/epoll.rs#L180-L187 https://github.com/tokio-rs/mio/blob/05009e4f60335fa00e9ea6a118595548afee0607/src/event/event.rs#L116-L123
> 
> An event with `EPOLLERR` is considered `is_write_closed`, which wakes up awaiters with a write interest. However, `EPOLLERR` does not affect `is_read_closed`.

`EPOLLERR` also triggers `is_error`. It trigger `is_write_closed` because of the `epoll` manual, which states:

> Error condition happened on the associated file descriptor. This
event is also reported for the write end of a pipe when the read end
has been closed.

See https://github.com/tokio-rs/mio/pull/1350.

> Maybe, given this issue, it should? Or would that mask errors that users would want to handle explicitly? If so, which sounds reasonable, why does that same logic not apply to `is_write_closed`?

No `is_read_closed` and `is_error` are two different things. Tokio should simply check `is_error`, Mio is not to blame here.

> If `EPOLLER` affected `is_read_closed` in mio, that would lead (I think, haven't checked) to @3442's `readable().await` being woken up and then the next `read` would fail.

",4349,2022-01-04T15:39:51Z,0
232,BraulioVM,"> EPOLLERR also triggers is_error. It trigger is_write_closed because of the epoll manual, which states:
>> Error condition happened on the associated file descriptor. This
event is also reported for the write end of a pipe when the read end
has been closed.

Got it",4349,2022-01-04T15:50:55Z,0
233,hawkw,"> The oneshot code is honestly pretty bad now that I look at it - there are very few safety justifications in the code.

Yeah, I was also thinking about trying to clean up more of the existing implementation (maybe in a follow-up...). But, I'm pretty confident that this change fixes the specific crash in question.",4226,2021-11-12T19:05:02Z,0
234,hawkw,"As a note, I can run my repro from https://github.com/tokio-rs/tokio/issues/4225#issuecomment-967434847 with this branch for over 40 minutes without seeing any segfaults or mangled strings.",4226,2021-11-12T22:00:20Z,0
235,hawkw,"> > The oneshot code is honestly pretty bad now that I look at it - there are very few safety justifications in the code.
> 
> Yeah, I was also thinking about trying to clean up more of the existing implementation (maybe in a follow-up...). But, I'm pretty confident that this change fixes the specific crash in question.

#4229 should help make some of the oneshot implementation's invariants a bit clearer, I hope!",4226,2021-11-13T00:23:31Z,0
236,Arnavion,Is there any plan to backport this to 0.1.x ?,4226,2022-01-04T19:40:10Z,0
237,Darksonn,"We don't have any plans, no.",4226,2022-01-04T20:18:47Z,0
238,Arnavion,"Okay, thanks.",4226,2022-01-04T20:27:54Z,0
239,cecton,@Darksonn you suggested such a trait in tokio-util. Is this what you had in mind?,4244,2021-11-18T09:35:44Z,0
240,Darksonn,I'm closing this because the implementation is going in the wrong direction and there hasn't been any activity for a while. Feel free to reopen or open a new issue if you want to continue working on this.,4244,2021-12-10T11:47:33Z,0
241,cecton,I'm back on this! :grin: Unfortunately I can't reopen the ticket on my side. I already pushed one new commit that replaces the box with the associated type but it's not visible here for some reason. Should I open a new PR? cc @Darksonn ,4244,2022-01-07T12:46:08Z,0
242,Darksonn,"Yeah, just open a new PR.",4244,2022-01-07T13:39:06Z,0
243,hawkw,"Hmm, looks like `cargo audit` doesn't like me after removing the config, but I'm not sure why. It claims `chrono` is still being pulled as a dependency of `tracing-subscriber` 0.2, but when i run `cargo tree`, I only see v0.3...
```
:# eliza at noctis in tokio on  eliza/update-examples-deps [?] via ⚙️ v1.56.1 in ❄️ nix-shell [impure]
:; cargo tree -p tracing-subscriber -i
tracing-subscriber v0.3.1
[dev-dependencies]
└── examples v0.0.0 (/home/eliza/Code/tokio/examples)
```",4227,2021-11-12T21:15:10Z,0
244,Darksonn,"Yeah, I don't know either.",4227,2021-11-12T21:17:10Z,0
245,Darksonn,This will be unblocked by https://github.com/tokio-rs/loom/pull/241.,4227,2021-11-23T12:37:39Z,0
246,Darksonn,Do you want to add it to the TcpListener docs too?,4386,2022-01-07T15:31:02Z,0
247,tr3ysmith,@Darksonn Yeah i can do that,4386,2022-01-07T15:52:08Z,0
248,tr3ysmith,"@Darksonn Actually the TCP Listener already had it, so I just matched up the wording to keep it the same. All good now",4386,2022-01-07T15:55:05Z,0
249,Darksonn,I'm fine with adding these impls. You can safely use `AssertUnwindSafe` to circumvent it.,4336,2021-12-22T16:25:12Z,0
250,Darksonn,"Instead of adding it to just `UdpSocket`, I think we should add it to `PollEvented`, which should make all `tokio::net` types unwind safe.",4384,2022-01-07T10:51:14Z,0
251,Hodkinson,"Makes sense, added it there instead. I also added a unit test, but can remove/refactor this if it's too much.",4384,2022-01-07T16:13:31Z,0
252,Darksonn,"I don't mind adding some tests, but I think we should include a test that verifies that the trait is implemented on the actual `tokio::net` types rather than on `PollEvented`.",4384,2022-01-07T20:18:26Z,0
253,Darksonn,"This seems like a good idea. One question is how to handle the [`Slab`][1] we have inside it, since its [`shrink_to_fit`][2] method may not be able to shrink it if large keys are in use.

[1]: https://docs.rs/slab/0.4/slab/struct.Slab.html
[2]: https://docs.rs/slab/0.4/slab/struct.Slab.html#method.shrink_to_fit",3590,2021-03-08T07:46:55Z,0
254,aym-v,@Darksonn The soon to be released `compact` method added by https://github.com/tokio-rs/slab/pull/60 will shrink the slab past its length. This might be a solution but the keys will be changed so a way must be found to match the old keys to the new keys. Another solution might be to completely re-allocate the slab but it will be the same problem with the keys.,3590,2021-03-15T14:39:06Z,0
255,Darksonn,"Yeah, some solution to not break keys is going to be necessary.",3590,2021-03-15T16:05:55Z,0
256,b-naber,I'd be interested in working at his. I think we would need a bijective map for the `Key` <-> usize key values that were changed by the `compact` call of the slab. We could store a pointer to that map in `Data` and the closure passed to `compact` allows us to mutate that map. Is there a problem with using a crate for the bimap (because tokio try to minimize external crate dependency)? Or should we implement this ourselves?,3590,2021-03-19T16:55:07Z,0
257,Darksonn,"I don't want to use a bimap because at that point you might as well just use a `HashMap` instead of a `slab`, at which point you can just call [`HashMap::shrink_to_fit`][1] and be done with it.

[1]: https://doc.rust-lang.org/stable/std/collections/hash_map/struct.HashMap.html#method.shrink_to_fit",3590,2021-03-19T17:29:41Z,0
258,b-naber,"Well but we'd only need to keep the re-mapped keys in the bimap. Unless we somehow modify the `Key` instances we give out or use these as key values in the wheel implementation, there is no alternative to using a bimap as far as I can tell. ",3590,2021-03-19T17:37:07Z,0
259,Darksonn,Using a `HashMap` seems like a much better alternative than the bimap to me. I still don't love it though.,3590,2021-03-19T17:44:48Z,0
260,halfzebra,"Hi @Darksonn!

I'd like to take a stab at this if you agree with replacing `Slab` with a `HashMap`.
Maybe there's a better data structure on your mind that would fin this use case?

Let me know what you think!
",3590,2021-09-25T18:53:44Z,0
261,Darksonn,There was some discussion on this on the Tokio discord a few weeks ago. I would encourage you to read it. @b-naber maybe you can summarize?,3590,2021-09-25T19:15:21Z,0
262,halfzebra,"Thanks for your reply, I think I might have found the discussion you are referring to, it looks like there is progress on this issue.

I'll look into something else 🙂",3590,2021-09-25T19:20:48Z,0
263,b-naber,"Yes, Ive already implemented this, but haven't had time to fully test it yet. Sorry for not updating the progress here, hope you haven't invested too much time into this yet, @halfzebra.",3590,2021-09-25T20:02:49Z,0
264,halfzebra,"@b-naber It looks like you've done a pretty good job so far!

Thanks for working on this 👍 
",3590,2021-09-25T21:21:42Z,0
265,b-naber,Why does the security check fail now? I didn't change anything cargo related in the latest push.,4170,2021-10-20T20:34:24Z,0
266,Darksonn,Someone published a security issue for the chrono crate. It will be fixed in #4186.,4170,2021-10-21T07:18:35Z,0
267,b-naber,"@Darksonn Thanks for the review, addressed your comments.",4170,2021-10-23T21:17:57Z,0
268,b-naber,@Darksonn Addressed your review.,4170,2021-10-25T15:21:11Z,0
269,b-naber,@Darksonn Isolated the key routing logic to the slab and fixed the bug revolving around `remove`.,4170,2021-10-27T14:50:39Z,0
270,Darksonn,"Do you need further review on this from me at this time, or are you able to continue with the PR?",4170,2021-11-09T14:33:26Z,0
271,b-naber,"@Darksonn Updated the PR. Thanks for simplifying `compact`, it's looks a lot nicer now.",4170,2021-11-12T23:17:04Z,0
272,b-naber,"@Darksonn Do you want anything else to be changed here?

The lint error here is misplaced imo, I think it perfectly fine to have the if statement in this case. Using `get_or_else` or `entry` just leads to borrow checker problems and makes it less straightforward to read. Can we ignore this lint?",4170,2021-12-06T09:42:27Z,0
273,Darksonn,"You could write it like this, which doesn't seem too bad?
```Rust
if let Entry::Occupied(entry) = self.key_map.entry(key.into()) {
    entry.insert(key);
}
```
I'm not going to review the PR in detail today — this is exam week. Can you remind me again later?",4170,2021-12-06T09:53:10Z,0
274,b-naber,"> You could write it like this, which doesn't seem too bad?
> 
> ```rust
> if let Entry::Occupied(entry) = self.key_map.entry(key.into()) {
>     entry.insert(key);
> }
> ```

This isn't what we want semantically though. We actually want to insert a key/value pair with a new key (`key_to_give_out`), so `entry` isn't really applicable here.

",4170,2021-12-12T12:46:51Z,0
275,b-naber,@Darksonn Any other changes needed?,4170,2021-12-13T20:42:30Z,0
276,Darksonn,It's on my todo-list and I will have another look soon.,4170,2021-12-13T20:52:06Z,0
277,b-naber,"Thanks, sorry if the question might have come across as somewhat pushy, it wasn't meant that way.",4170,2021-12-13T21:18:35Z,0
278,Darksonn,"No worries. PRs do sometimes gets lost, and it's totally fair to ping me. :)",4170,2021-12-13T21:25:07Z,0
279,b-naber,"@Darksonn Can you take another look at this, please?",4170,2021-12-28T14:52:23Z,0
280,b-naber,@Darksonn fixed that. ,4170,2021-12-28T19:03:50Z,0
281,b-naber,@Darksonn Can you take one more look maybe? I'd like to finish this PR soon.,4170,2022-01-02T19:30:27Z,0
282,b-naber,@Darksonn Just pinging in case you forgot about this. When do you want to merge this?,4170,2022-01-09T11:14:35Z,0
283,Darksonn,"I'm not sure how we can actually test it. We can assert that the write and flush succeeds, but we can't look at what actually got written.",4348,2021-12-27T15:16:16Z,0
284,taiki-e,"Ah, it's indeed not easy to test it.

I think we can test this by adding the #4347's code to [tests-integration/src/bin](https://github.com/tokio-rs/tokio/tree/78e0f0b42a4f7a50f3986f576703e5a3cb473b79/tests-integration/src/bin) and doing something similar to the following shell script.

```sh
cargo run >tmp &
kill $!
grep <tmp ""What's your name?: ""
```

However, it is more complex and tricky than what I had imagined during the initial review, so I'm okay with merging this without adding a test.",4348,2021-12-27T16:32:19Z,0
285,Darksonn,It appears that the integration test keeps failing on Mac OS. Any ideas?,4348,2021-12-30T14:19:02Z,0
286,iitalics,"I'm with you, I'd rather understand what's going on before jumping to a fix like this.",4315,2021-12-13T21:29:24Z,0
287,Darksonn,"@iitalics You should be able to try out the version of Tokio available in this PR by adding the following to your `Cargo.toml`.
```
[patch.crates-io]
tokio = { git = ""https://github.com/tokio-rs/tokio.git"", branch = ""wait_with_output_oom"" }
```
If you're able to compare how it performs with and without this change without any other changes, that would be cool.",4315,2021-12-13T21:51:50Z,0
288,iitalics,I've tested this for about an hour with no crash observed.,4315,2021-12-13T23:07:14Z,0
289,Darksonn,"Okay, if adding/removing this patch makes the crash go away and come back, then it seems reasonable to merge this. Not that it makes any sense.",4315,2021-12-14T08:55:57Z,0
290,Darksonn,The CI failure is [mio#1539](https://github.com/tokio-rs/mio/issues/1539).,4310,2021-12-10T11:27:22Z,0
291,nylonicious,"The delay was not a problem, thanks for the review.",4310,2022-01-10T12:01:34Z,0
292,hi-rustin,I would like to try this.,3694,2021-04-20T05:27:29Z,0
293,Darksonn,Go ahead!,3694,2021-04-20T07:04:36Z,0
294,cssivision,"""copying data from the end of the buffer to the start"" is costly, IMO we should avoid doing this.",3694,2021-04-22T05:09:32Z,0
295,hi-rustin,"@cssivision ~~Can't you see I'm trying to fix this?~~

~~Shouldn't you ask before you do it? Don't waste my time right?~~

It doesn't matter. Whatever.",3694,2021-04-22T07:21:18Z,0
296,Darksonn,"Please be more careful when you take on PRs that someone have already claimed. If you are concerned that it is stalled, you should ask whether they are still working on it.",3694,2021-04-22T10:03:18Z,0
297,cssivision,"Yes, you are right.",3694,2021-04-22T11:04:24Z,0
298,hi-rustin,"> Yes, you are right.

@cssivision You don't have to close your PR, I'm just saying you should have asked me in advance instead of just submitting a PR, which would have caused everyone's time to be wasted. Please feel free to reopen your PR.(If you don't want to, that's fine) Thanks!",3694,2021-04-22T14:31:35Z,0
299,rneswold,"Is this a real problem that needs to be handled? I find it hard to believe that, with modern drives and network speeds, the 2k buffer isn't filled all the time. I started a conversation that resulted in #3722 because I found reading and writing 2k at a time severely limited throughput.

I've found that reducing the calls into the kernel can increase performance due to context switches being a fairly expensive operation. Checking to see if a socket is readable or writable probably results in a kernel call (via `ioctl`) and then being added to the underlying `poll` is a call, albeit for all handles the scheduler blocks on. Making it do this multiple times for a 2k buffer seems like the wrong direction.

Unless you're targeting tiny machines (I don't know if Rust compiles code for embedded micros) 2k is a tiny amount of RAM (barely bigger than an MTU and much smaller than a GigE jumbo packet!)

I'm asking that, if this is a feature that is to be added, please benchmark it to see how it performs in reading or writing to a gigabit Ethernet link.",3694,2021-04-23T15:23:10Z,0
300,cssivision,"we don't have to check if a socket is readable or writable. either IO resource has returned Pending, we should not poll that resource again during this poll.",3694,2021-04-24T09:12:34Z,0
301,rneswold,"> we should not poll that resource again during this poll

That's good to know. However, I believe my other concerns are still valid. `io::copy` is a valuable building block but, if it doesn't transfer data quickly, everyone will be writing their own performant version.",3694,2021-04-26T15:50:35Z,0
302,Darksonn,"Yeah, we should probably change the default buffer size too. The one in std uses 8k, so I guess that would be reasonable?",3694,2021-04-26T16:03:35Z,0
303,rneswold,"8k should definitely be the minimum since Jumbo frames may be more common as GigE becomes the ""slowest"" network interface. It'll cut the number of kernel calls to a quarter of they are now, at least.

Another thing to consider is TCP's NO_DELAY option. Some network traffic is chit-chatty (e.g. `ssh` and `telnet`.) By default, TCP tries to buffer up more data to fill the MTU. But for login sessions or protocols with short request/reply messages, you don't want to wait for a full buffer to send because more data may be held off until a reply is received. So some applications turn on the NO_DELAY option for the socket so that small packets are sent immediately.

Trying to fill the buffer defeats that option. So I'd avoid trying to fill the 8k buffer; just send what was received. This covers short exchanges that don't fill the buffer and large data transfers that completely fill it.

(in other words, short messages won't see a benefit of 8k but large data transfers will.)",3694,2021-04-26T18:19:16Z,0
304,Darksonn,"When it comes to NO_DELAY, that's not something `io::copy` controls. You can set it on the socket when you create it.

As for filling the buffer, the design I had in mind would only try to read more data into the buffer if the `AsyncWrite` is _not_ able to send all of the data immediately. So it seems to me that this would not affect the case where the IO resources are extremely fast. It's still worth testing it of course.",3694,2021-04-26T20:25:39Z,0
305,Darksonn,"That is, the idea is that if we are not able to write anything right now, we might as well read some more data from the socket instead if we have space for it. It would not involve postponing writing any data in the cases where we are able to write data for the sake of buffering many smaller reads together.",3694,2021-04-26T20:29:20Z,0
306,rneswold,"> When it comes to NO_DELAY, that's not something io::copy controls. You can set it on the socket when you create it.

Sorry, I wasn't being clear. If I set NO_DELAY on the socket, but you're waiting to fill the 2k buffer before sending it on, then you've defeated the purpose of NO_DELAY.

But your later response indicates you're only filling when you can't write which I'm more in favor for. Especially if you increase the buffer size.",3694,2021-04-26T20:35:50Z,0
307,Darksonn,"Right, I understand now what you meant with NO_DELAY. To confirm, I am not suggesting that we wait for the buffer to be filled before we try to send, rather I am suggesting that if the send returns `WouldBlock`, then we try to do another read if there is space in the buffer.",3694,2021-04-26T20:41:14Z,0
308,b-naber,"> the design I had in mind would only try to read more data into the buffer if the AsyncWrite is not able to send all of the data immediately

@Darksonn This sounds to me like the condition `i < self.cap`, where `i = ready!(writer.as_mut().poll_write(cx, &me.buf[me.pos..me.cap]))?`, but in the current implementation we already get back into the reading part if that condition holds. Or do you mean to try to read more data if `poll_write` returns `Pending`? Can you clarify what you mean by 'not able to send all of the data'?

",3694,2022-01-10T17:31:18Z,0
309,taiki-e,"It seems that https://github.com/tokio-rs/tokio/commit/cc8ad367a0e5d8536f8be58fe560bfdea1a976a5 also needs to be reverted to fix remaining regression (double panic from hyper's test suite) that is currently occurring on master.
",4392,2022-01-11T18:40:23Z,0
310,carllerche,"Hmmm, alright, I will do that in a separate PR.",4392,2022-01-11T18:42:05Z,0
311,taiki-e,"> double panic from hyper's test suite

This can be reproduced by running the following commands in the tokio's workspace root:

```sh
git clone https://github.com/hyperium/hyper.git
cd hyper
echo '[workspace]' >>Cargo.toml
echo '[patch.crates-io]' >>Cargo.toml
echo 'tokio = { path = ""../tokio"" }' >>Cargo.toml
echo 'tokio-util = { path = ""../tokio-util"" }' >>Cargo.toml
echo 'tokio-stream = { path = ""../tokio-stream"" }' >>Cargo.toml
echo 'tokio-test = { path = ""../tokio-test"" }' >>Cargo.toml
cargo test --features full --test integration
```

result:

```
     Running tests/integration.rs (target/debug/deps/integration-a51c6d893196dace)

running 14 tests
thread panicked while panicking. aborting.
```

https://github.com/tokio-rs/tokio/runs/4779296976?check_suite_focus=true",4392,2022-01-11T19:00:38Z,0
312,taiki-e,"Oh, timeout in hyper/benches/connect.rs doesn't seem to happen on linux?: https://github.com/tokio-rs/tokio/runs/4775733526?check_suite_focus=true

EDIT: It seems to be unrelated to #4270 as it can be reproduced in tokio v1.15.0 on macos.",4390,2022-01-11T13:40:40Z,0
313,taiki-e,"The double panic from hyper's test suite also seems to be unrelated to this problem.

I can confirm the double panic on #4391.",4390,2022-01-11T18:31:15Z,0
314,taiki-e,"Ok, reverting https://github.com/tokio-rs/tokio/commit/cc8ad367a0e5d8536f8be58fe560bfdea1a976a5 solves the remaining problems.",4390,2022-01-11T18:49:01Z,0
315,Noah-Kennedy,I can try debugging this tonight. I have access to MacOS and Windows machines for development.,4390,2022-01-11T18:56:49Z,0
316,carllerche,I'm going to close this PR since the changes being fixed were reverted and the CI improvements were merged separately. We need to figure out how we want to move forward w/ mio 0.8 still.,4390,2022-01-11T22:35:09Z,0
317,Thomasdezeeuw,"> I'm going to close this PR since the changes being fixed were reverted and the CI improvements were merged separately. We need to figure out how we want to move forward w/ mio 0.8 still.

This is a rather surprising reaction, especially considering the bug wasn't even in Mio. Really the only problem was the missing check for `EINPROGRESS` which this pr would fix.",4390,2022-01-12T17:03:23Z,0
318,sfackler,Aborting the `h` task doesn't abort the `hh` task.,4400,2022-01-13T16:25:48Z,0
319,Darksonn,"I'm fine with adding this, though note that `futures::executor::block_on` is a solution that works today.",4319,2021-12-14T19:15:47Z,0
320,hi-rustin,I am working on this.,4319,2021-12-18T15:26:50Z,0
321,besok,Apparently the issue should be closed,4319,2022-01-13T21:40:48Z,0
322,Darksonn,Thanks.,4319,2022-01-13T21:50:32Z,0
323,carllerche,"@Darksonn @hawkw this should be ready for review, it would be nice to have it confirmed in a ""real app"" too.",4383,2022-01-12T06:08:58Z,0
324,blt,"> @Darksonn @hawkw this should be ready for review, it would be nice to have it confirmed in a ""real app"" too.

I'm planning on wiring this up into [vector](https://github.com/vectordotdev/vector) and running it through our integrated benchmarks. Assuming all goes well I'll link numbers here in a handful of hours. ",4383,2022-01-12T18:13:32Z,0
325,blt,"No change in observed vector throughput, per [these results](https://github.com/vectordotdev/vector/pull/10823#issuecomment-1011424094). That said, we don't track CPU use in our experimental setup and the observation duration in a PR is constrained to 200 seconds to make turn-around time feasible. ",4383,2022-01-12T21:01:55Z,0
326,carllerche,"@blt thanks for checking. It would be interesting to know the CPU load as that would let us know if it falls within the scope of this change. If you are saturating all workers in your tests, then there would be no visible improvement as it only applies when only a few workers are actually kept busy.",4383,2022-01-12T22:16:08Z,0
327,blt,"Unfortunately we don't capture that kind of saturation information yet, though we do have an issue for it: https://github.com/vectordotdev/vector/issues/10456. That said, we're intending to fully saturate vector so it's reasonable to believe there's little idle time.",4383,2022-01-12T22:46:54Z,0
328,Darksonn,There's not much I can say based on what you have posted here.,4399,2022-01-13T14:23:06Z,0
329,Antiarchitect,@Darksonn Is there any additional info I can provide?,4399,2022-01-13T14:47:47Z,0
330,Darksonn,"Well, for example, if you can figure out which allocation is the culprit, that would be one way.

Another thing that's helpful is a graph with memory usage over time for a long duration of time.",4399,2022-01-13T15:26:32Z,0
331,Antiarchitect,"The whole graph (with app start and terminate): https://i.imgur.com/x37UqIj.png
Rising to plateau: https://i.imgur.com/FDGnaVN.png
And related leakage: https://i.imgur.com/dzciPQd.png
plateau: (seems clear) https://i.imgur.com/nTRI0zh.png

Seems like the situation is normal. It stabilizes after a few minutes, do not why heaptrack count this as leakage

",4399,2022-01-13T17:02:36Z,0
332,Antiarchitect,@Darksonn I will close the issue. If you can tell this is a real problem - please reopen. Long-running testing shows that these 1-5 MB are consumed only in the first several minutes of app works,4399,2022-01-14T04:32:10Z,0
333,Darksonn,"Well, the conversion is fallible, which the trait doesn't support.",4403,2022-01-14T10:46:32Z,0
334,NobodyXu,Is it because the conversion into `RawFd` requires the `O_NONBLOCKING` flags to be removed?,4403,2022-01-14T10:47:37Z,0
335,Darksonn,"Yes, but also because its registration with epoll needs to be removed.",4403,2022-01-14T10:59:32Z,0
336,NobodyXu,"OK, I got it.

Is it possible to have some alternative API that can return the fd along with its ownership?

I personally is OK with the fd being `O_NONBLOCKING`.",4403,2022-01-14T11:02:45Z,0
337,Darksonn,"Sure, we could have an `into_std` function.",4403,2022-01-14T11:28:09Z,0
338,NobodyXu,"> Sure, we could have an `into_std` function.

I don’t think we can support “into_std” since “ChildStdin” doesn’t support FromRawFd.",4403,2022-01-14T13:31:51Z,0
339,Darksonn,"Ok, I guess that's why we don't have it already. I'm sure we could find some other alternative.",4403,2022-01-14T14:06:11Z,0
340,NobodyXu,"Maybe something like `fn to_owned_fd(self) -> Result<io_lifetimes::OwnedFd, io::Error>` where [`io_lifetimes::OwnedFd`](https://docs.rs/io-lifetimes/0.4.4/io_lifetimes/struct.OwnedFd.html) is implementation of the unstable `std::os::unix::io::OwnedFd`?",4403,2022-01-15T05:12:12Z,0
341,Darksonn,"We would probably use the same return type as the `IntoRawFd` trait. We are not going to add a dependency on `io_lifetimes` for this, nor are we going to define a similar type in Tokio just for the stdio types.",4403,2022-01-15T11:15:54Z,0
342,NobodyXu,"I agree that introduce a new dependency is too much and using “RawFd” is fair enough.

I think having a new unix-only trait for this is the way to go, since “ChildStdin”, “ChildStdout” and “ChildStderr” will all have this method implemented.",4403,2022-01-15T12:24:49Z,0
343,alamb,Thank you @Darksonn  -- I think this change will reduce confusion going forward.,4105,2021-09-15T12:52:45Z,0
344,Darksonn,"When people talk about ""the bounded channel"", they generally refer to `tokio::sync::mpsc::bounded`, but in #4404 you asked about the broadcast channel. Which is it?

And if you mean the broadcast channel, then it is not necessarily obvious what it should return. Different receivers may be different amounts of items behind.",4405,2022-01-16T09:29:13Z,0
345,alex88,"Sorry I'm referring about the broadcast bounded channel, in that case I would imagine each receiver would see its own queue, because like you said, it would be different for each receiver. I imagine it's also more relevant to the receiver to know how much it's behind to know what to do.",4405,2022-01-16T16:27:43Z,0
346,Darksonn,"So you are asking for a method on the receiver, and not the sender? I suppose that could make sense.",4405,2022-01-16T17:08:20Z,0
347,alex88,"Yeah sorry I did mention the receiver but it wasn't clear, I'll add to the initial comment.",4405,2022-01-16T17:12:30Z,0
348,Darksonn,What does the call return?,4402,2022-01-14T08:13:41Z,0
349,hbprotoss,"> What does the call return?

returns 0

I've traced to `tokio-1.15.0/src/io/async_read.rs`:67

```
Pin::new(&mut **self).poll_read(cx, buf)
```

It seems the function `poll_read ` does not return, and jump to asm code.

outer call from `tokio-1.15.0/src/io/util/take.rs`:89 do returns, and next line `n` is 0

```
        ready!(me.inner.poll_read(cx, &mut b))?;
        let n = b.filled().len();
```",4402,2022-01-14T08:19:07Z,0
350,Darksonn,Sounds like your `self.reader` object has reached EOF then.,4402,2022-01-14T08:20:02Z,0
351,hbprotoss,"I'm sure it has retained bytes.

client sent `*2\r\n$7\r\nCOMMAND\r\n`, the code before consumes `*2\r\n$7\r\n`",4402,2022-01-14T08:23:50Z,0
352,hbprotoss,How can I find call stack from underlying C++ perhaps?,4402,2022-01-14T08:25:04Z,0
353,hbprotoss,"I replace the call to this `read_n` with `read_until`, reader reads the left `COMMAND\r\n` into buffer",4402,2022-01-14T08:51:28Z,0
354,hbprotoss,"success:

```
let mut b = [0 as u8; 7];
self.reader.read_exact(&mut b).await?;
```

fail:

```
let mut b = [0 as u8; 7];
self.reader.get_mut().read_exact(&mut b).await?;
```",4402,2022-01-14T08:54:53Z,0
355,hbprotoss,"`get_mut()` returns reference to inner obj, maybe it has been consumed by the `BufReader`?",4402,2022-01-14T09:12:37Z,0
356,Darksonn,"Oh, yeah, you definitely shouldn't be calling `get_mut` on the `BufReader`. Use a mutable reference to the `BufReader` instead.",4402,2022-01-14T10:00:39Z,0
357,hbprotoss,"after modified to `(&mut self.reader).take(n).read_to_end(buf).await`, all works well",4402,2022-01-17T08:21:35Z,0
358,hbprotoss,"btw, `get_mut` is too confusing",4402,2022-01-17T08:24:00Z,0
359,Darksonn,"The documentation _does_ say this:

> It is inadvisable to directly read from the underlying reader.

I'm going to close this as there's no bug in Tokio here.",4402,2022-01-17T10:07:15Z,0
360,Darksonn,"I applied the following change to Tokio:
```diff
diff --git a/tokio/src/sync/mpsc/list.rs b/tokio/src/sync/mpsc/list.rs
index e4eeb4541..ed97f8942 100644
--- a/tokio/src/sync/mpsc/list.rs
+++ b/tokio/src/sync/mpsc/list.rs
@@ -353,10 +353,15 @@ impl<T> Rx<T> {
             self.head = NonNull::dangling();
         }
 
+        let mut counter = 0;
+
         while let Some(block) = cur {
             cur = block.as_ref().load_next(Relaxed);
             drop(Box::from_raw(block.as_ptr()));
+            counter += 1;
         }
+
+        println!(""{}"", counter);
     }
 }
```
Then I ran the following test:
```Rust
#[tokio::main]
async fn main() {
    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<[u8; 4096]>();
    let mut counter = 0;

    tokio::spawn(async move {
        for _ in 0..100000 {
            if let Err(error) = tx.send([0; 4096]) {
                println!(""{}"", error);
            }
        }
    });

    while let Some(_) = rx.recv().await {
        counter += 1;
        println!(""{} of 100000"", counter);
    }
}
```
When it exited, it reported that the channel had 5 blocks left when the channel was dropped. That means that the total number of slots that the channel had allocated memory for is 32*5 = 160 messages.

I encourage you to try out jemalloc with a configuration that aggressively releases memory back to the OS like [this one](https://github.com/tokio-rs/tokio/issues/2479#issuecomment-623157381) and see what happens.",4321,2021-12-15T08:18:26Z,0
361,semirix,"Just for my own benefit, why would using jemalloc change the behaviour? Isn't the block code responsible for when/how blocks get dropped? If the minimum example still has blocks left and that's observable, why would those blocks use more/less memory under a different allocator?",4321,2021-12-20T00:28:26Z,0
362,sfackler,Memory allocators do not immediately return all freed memory to the operating system.,4321,2021-12-20T00:34:56Z,0
363,semirix,"> Memory allocators do not immediately return all freed memory to the operating system.

@sfackler If those blocks are still in use by the MPSC channel and haven't been dropped, shouldn't it be impossible for it to be deallocated? I thought the issue here is that the blocks aren't being dropped and thus aren't being deallocated by the system. It's possible I'm not understanding this fully.",4321,2021-12-20T00:46:51Z,0
364,Tarang,I don't know if gemalloc is a good way to fix this. Is there a way to drop anything at the end? It seems very counterintuitive and against purpose of using rust if there is no way to control it.,4321,2021-12-20T01:57:17Z,0
365,Darksonn,"> If those blocks are still in use by the MPSC channel and haven't been dropped, shouldn't it be impossible for it to be deallocated?

Yes, if they really are in use by the MPSC channel, then using jemalloc wont make a difference. The reason I mention jemalloc is that according to the test I did, the MPSC channel is _not_ holding on to most of the blocks anymore, and they have already been deallocated.",4321,2021-12-20T09:29:55Z,0
366,Darksonn,"Alternatively you might find this more convincing:
```Rust
use std::sync::atomic::{AtomicUsize, Ordering};
use std::alloc::{GlobalAlloc, System, Layout};
use std::time::Duration;

struct AllocCounter {
    count: AtomicUsize,
}

unsafe impl GlobalAlloc for AllocCounter {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        self.count.fetch_add(layout.size(), Ordering::Relaxed);
        System.alloc(layout)
    }

    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        self.count.fetch_sub(layout.size(), Ordering::Relaxed);
        System.dealloc(ptr, layout)
    }

    unsafe fn alloc_zeroed(&self, layout: Layout) -> *mut u8 {
        self.count.fetch_add(layout.size(), Ordering::Relaxed);
        System.alloc_zeroed(layout)
    }
}

#[global_allocator]
static A: AllocCounter = AllocCounter {
    count: AtomicUsize::new(0),
};

#[tokio::main]
async fn main() {
    std::thread::spawn(|| {
        loop {
            std::thread::sleep(Duration::from_secs(1));
            println!(""Current memory: {}"", A.count.load(Ordering::Relaxed));
        }
    });

    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<[u8; 4096]>();
    let mut counter = 0;

    tokio::spawn(async move {
        for _ in 0..100000 {
            if let Err(error) = tx.send([0; 4096]) {
                println!(""{}"", error);
            }
        }

        loop {}
    });

    while let Some(_) = rx.recv().await {
        counter += 1;
        println!(""{} of 100000"", counter);
    }
}
```
For me, this claims that the memory usage is 580 kB once the loop completes, far less than the 200 MB you were talking about.",4321,2021-12-20T09:49:40Z,0
367,semirix,"Things keep getting weirder.

![image](https://user-images.githubusercontent.com/7454145/147040141-b6db0c1e-9131-4ba1-96a3-f0f39891d70a.png)

The system monitor is reporting something different than the allocator. I'm not really sure what to make of this. I left it running for a few minutes and I saw no change in the system monitor.
",4321,2021-12-22T23:39:16Z,0
368,Darksonn,"It's not particularly surprising to me. Every time someone has reported a potential memory leak to Tokio that they found by looking at their system monitor (as opposed to e.g. valgrind), it was not actually a memory leak. Every single time it has been because the memory allocator was not releasing the deallocated memory back to the OS so it could reuse it for future allocations, speeding up those future allocations.",4321,2021-12-23T11:25:03Z,0
369,Darksonn,I'm going to close this as incorrectly interpreting the numbers displayed by the task manager.,4321,2022-01-17T14:27:03Z,0
370,Darksonn,"The `JoinHandle` type holds a reference count to the task, and the task's memory is not freed until the `JoinHandle` is dropped. It does not matter whether it is awaited or not, dropping it without awaiting it will also release the memory.",4406,2022-01-17T14:26:01Z,0
371,flumm,"Hi, yeah, sorry for the noise.

I researched a bit more, and it seems that the default allocator does not properly free the memory in those situations (I don't completely understand why, though). 
I can ""fix"" the behavior by either using jemalloc, or by tuning the malloc parameters according to mallopt(3).
So tokio does the right thing, but the example (and seemingly our code), seems to trigger some bad cases for the default rust allocator under Linux.

Thanks",4406,2022-01-17T15:32:44Z,0
372,sfackler,Allocators hold onto deallocated memory regions to improve allocation throughput.,4406,2022-01-17T15:36:03Z,0
373,Darksonn,"> I researched a bit more, and it seems that the default allocator does not properly free the memory in those situations (I don't completely understand why, though).

This is actually the cause behind most of the leak reports we get for Tokio. It happens for performance reasons — it is faster to hold on to the memory than to release it and then ask for it again later.

In your case, I actually guessed that you had commented out the entire for loop (as opposed to just the line with the `.await`), in which case the memory really wouldn't have been freed because the `JoinHandle`s still existed in the vector during the read from stdin.",4406,2022-01-17T15:43:22Z,0
374,flumm,"> Allocators hold onto deallocated memory regions to improve allocation throughput.

i knew that, but my understanding was/is that there will be memory returned to the host 'under certain conditions' (e.g. see mallopt(3) comment about M_TRIM_THRESHOLD) but in case of glibc it seems not to be that simple.. 

a program that basically has gigabytes allocated but not used seems rather pointless. in our case, customers ran into oom situations because of the combined used ram of various things, including our main daemon (that ""used"" >5GiB)

> > I researched a bit more, and it seems that the default allocator does not properly free the memory in those situations (I don't completely understand why, though).
> 
> This is actually the cause behind most of the leak reports we get for Tokio. It happens for performance reasons — it is faster to hold on to the memory than to release it and then ask for it again later.
> 
> In your case, I actually guessed that you had commented out the entire for loop (as opposed to just the line with the `.await`), in which case the memory really wouldn't have been freed because the `JoinHandle`s still existed in the vector during the read from stdin.

no i deliberately left the 'into_iter()' so that the joinhandles were consumed. i could have phrased it better though, yes.

",4406,2022-01-17T15:49:42Z,0
375,Darksonn,"This particular trick has previously been discussed in [this blog post][1], where we said that we would not implement this.

[1]: https://tokio.rs/blog/2020-04-preemption#a-note-on-blocking",4407,2022-01-17T14:43:22Z,0
376,hidva,"> This particular trick has previously been discussed in [this blog post](https://tokio.rs/blog/2020-04-preemption#a-note-on-blocking), where we said that we would not implement this.

Thanks for your reply. I have read this article before, and I still want to try the effect of sysmon. According to the data obtained from this attempt, I have a better understanding of the reason mentioned in the article. It seems that we really don't need to introduce sysmon. Sorry to bother you, I should discuss it in Discussions first.",4407,2022-01-17T15:42:47Z,0
377,carllerche,"I’m not against a sysmon style strategy. What is key is that it isn’t applied against everything. What we could investigate is an annotation (“might_block”), which opts into the sysmon. We also need to think about how to deal with blocking the rest of the task (e.g. select). ",4407,2022-01-17T17:16:10Z,0
378,taiki-e,"hmm, it's still slow sometimes: 

- 45m https://github.com/tokio-rs/tokio/runs/4674059799?check_suite_focus=true
- 48m https://github.com/tokio-rs/tokio/runs/4674237353?check_suite_focus=true

all runs: https://github.com/tokio-rs/tokio/actions/workflows/stress-test.yml?query=branch%3Ataiki-e%2Fvalgrind",4367,2021-12-31T14:32:14Z,0
379,Darksonn,It also seems like it's complaining about a leak? Is the test not working?,4367,2021-12-31T14:47:10Z,0
380,taiki-e,"Oh, we should use --error-exitcode.

https://valgrind.org/docs/manual/manual-core.html#manual-core.erropts

> --error-exitcode=<number> [default: 0]
>
> Specifies an alternative exit code to return if Valgrind reported any errors in the run. When set to the default value (zero), the return value from Valgrind will always be the return value of the process being simulated. When set to a nonzero value, that value is returned instead, if Valgrind detects any errors. This is useful for using Valgrind as part of an automated test suite, since it makes it easy to detect test cases for which Valgrind has reported errors, just by inspecting return codes.
",4367,2022-01-01T06:29:56Z,0
381,Darksonn,"I did try to add that in the past in #4066, but I missed the stress test apparently.",4367,2022-01-01T10:18:06Z,0
382,carllerche,What's up w/ this PR?,4367,2022-01-18T18:28:00Z,0
383,carllerche,"Ah, it is still marked as a draft.",4367,2022-01-18T18:28:10Z,0
384,PeterCorless,Related discussion on Hacker News today: https://news.ycombinator.com/item?id=30023615,4415,2022-01-21T16:19:44Z,0
385,carllerche,"We can confirm that users of Tokio could be affected by this issue if they use the `tokio::fs::remove_dir_all` function in an environment executing untrusted workloads. We recommend that you follow the guidance published with this advisory to update your Rust toolchain to 1.58.1. Note that programs compiled with previous versions of the toolchain will need to be recompiled to mitigate this issue.

After evaluating the Rust CVE, we have determined that we have no action to take at this time. If you believe this to be incorrect, please email security@tokio.rs as per our [security policy](https://github.com/tokio-rs/tokio/security/policy).",4415,2022-01-21T18:34:43Z,0
386,carllerche,@LucioFranco @Matthias247 any opportunities to try using the metrics?,4073,2021-09-21T18:24:35Z,0
387,Matthias247,"I was on vacation and after that mostly block on other things. But I might be able to try this out this week.

I will however say upfront that I'll expect mostly to report back on the general accessor APIs and how integration into an application will look like. I think that putting `poll_count`/`steal_count`/etc on a dashboard will not be super useful for most people, because the numbers in itself have no significant meaning. They don't necessarily indicate that something is right or wrong. The not-yet-implemented timing metrics are more interesting, because they would indicate issues with code in tasks blocking too long. I will nevertheless check and see how the other metrics would look like.",4073,2021-09-28T01:09:14Z,0
388,carllerche,"We should add a counter tracking the number of ""false-positive"" runtime wakeups. This would be incremented when a worker wakes up without having any work to do.",4073,2021-12-23T22:40:49Z,0
389,e-ivkov,"> The not-yet-implemented timing metrics are more interesting, because they would indicate issues with code in tasks blocking too long.

I second this. In our project built on tokio, we implemented a custom [macro](https://github.com/hyperledger/iroha/blob/d7c0b582920f036548768b501769e5785cf709db/futures/derive/src/lib.rs) to track poll times. It would be very useful to have it in tokio.",4073,2021-12-31T10:23:46Z,0
390,carllerche,"@Darksonn @hawkw @LucioFranco I'm marking this as ready to review. There is more work to do on docs, but we can do that in follow-up PRs. The feature is still marked as unstable and others are waiting for this PR to land to do work in parallel. ",4373,2022-01-20T04:09:37Z,0
391,carllerche,"I'm going to merge once CI passes. If there any other points you want to continue discussing or track before stabilizing, please add them [here](https://github.com/tokio-rs/tokio/issues/4073). I already tracked that docs (TODO) should be handled.",4373,2022-01-21T21:32:19Z,0
392,carllerche,"Looking at your original issue, I think we can support better performance by working directly with `Bytes`. In that case, we can avoid the copying and instead send the bytes handle to the remote thread.",1976,2019-12-21T21:38:41Z,0
393,blasrodri,"I’d like to work on this. Some guidance would be appreciated. 
Especially around 
>  I think we can support better performance by working directly with Bytes",1976,2020-08-03T18:40:26Z,0
394,grantperry,"@carllerche
By working with`Bytes` do you mean call `AsyncReadExt::read_buf` instead of `AsyncReadExt::read`? Will this fill the `Bytes` buffer if it is larger that `MAX_BUF`?",1976,2021-01-09T00:15:52Z,0
395,Darksonn,Not with the current implementation. It would probably require a special function for `BytesMut` specifically.,1976,2021-01-09T10:48:13Z,0
396,fetchadd,"@carllerche  Why is 16K, and not others, is there some special reason, or testing proved a better performance with 16K?",1976,2021-06-28T13:36:45Z,0
397,Darksonn,The reason to have a maximum buffer size is that the file API allocates an intermediate buffer separate from the user-provided buffer. I don't think the exact choice of size was benchmarked.,1976,2021-06-28T13:43:50Z,0
398,blasrodri,Why not replacing `Vec<u8>` in `Io::blocking::Buf`  for `BytesMut`?,1976,2021-07-04T21:59:36Z,0
399,Darksonn,"Well why would we? If both can be used, it's better to use a `Vec<u8>`.",1976,2021-07-04T22:17:51Z,0
400,blasrodri,"> Well why would we? If both can be used, it's better to use a `Vec<u8>`.

You're right.

Any hints on how to move forward w/ this?",1976,2021-07-04T23:35:33Z,0
401,Darksonn,"The file operations that are offloaded to the `spawn_blocking` threadpool will probably continue to have a maximum buffer size. One thing that would be nice is to finish #3821, and operations executed through that setup would not be limited in size. That would involve finding a way to test various kernel versions in CI.

E.g. maybe there is a way to have some machines on AWS with the desired kernel version participate in running CI? Not sure on the details.",1976,2021-07-05T07:54:38Z,0
402,mcronce,"It's an _extremely_ narrow test, but in my quest to optimize I/O performance on something reading/writing whole (1-50GiB) files sequentially, I tested a quick hack that simply changes `MAX_BUF` to 128MiB and, much to my surprise, it Just Worked(TM):  With a 128MiB buffer, I'm getting 128MiB per `read()` and `write()` syscall according to strace.

This is an obnoxiously large I/O size, but it does work on this very specific use case:  The files are on cephfs, in an erasure coded pool, with 128MiB object size.  Aligning I/O to read whole objects per request substantially improves performance (approx 80MiB/s per task with four tasks up to approx. 220MiB/s in my case)

This is on Fedora with kernel 5.6.13

**EDIT**:  Fixed numbers",1976,2022-01-22T06:31:13Z,0
403,Darksonn,I am open to changing the buffer size used by the `File` type.,1976,2022-01-22T08:35:42Z,0
404,Hodkinson,I can take a look at this,4414,2022-01-23T10:07:57Z,0
405,taiki-e,"FWIW, Cirrus CI also provides arm container: https://cirrus-ci.org/guide/linux",4417,2022-01-23T19:42:03Z,0
406,carllerche,"hmm, might try it. Circle CI isn't working well with checks.",4417,2022-01-24T00:50:11Z,0
407,Hodkinson,Fixes: #4414 ,4418,2022-01-23T11:21:50Z,0
408,carllerche,It seems like circle CI isn't reporting success :-/,4418,2022-01-23T19:32:30Z,0
409,carllerche,Are you able to abort on panic? You could also set a panic_handler that signals the root task (`block_on`) to exit.,2002,2019-12-20T22:16:42Z,0
410,mikhailOK,"panic hook to signal the root task works.
Is there a plan to add API to pass a panic_handler to Harness::poll as opposed to std panic hook?",2002,2019-12-20T22:34:30Z,0
411,Vlad-Shcherbina,"Dealing with it in the panic handler is not the best option because maybe I still want to explicitly catch panics in specific scopes, but unexpected panics elsewhere should terminate the whole thing. By default. It's an unpleasant surprise when they don't (see [fail-fast](https://en.wikipedia.org/wiki/Fail-fast)).",2002,2020-01-30T12:48:41Z,0
412,Darksonn,Closing in favor of #2699.,2002,2020-07-25T09:49:55Z,0
413,Vlad-Shcherbina,"It's not about tests. It's about panics being silently caught everywhere. In production too.

Anywhere else in Rust, if there is a panic in the code not explicitly wrapped in `catch_unwind`, the whole program terminates with a diagnostic message. This goes in line with Rust's emphasis on correctness. Panic usually indicated a bug in the code, and I don't want bugs to be silently ignored. I want bugs to be reported and fixed.

It is true that sometimes we need to catch panics to ensure robustness. For example, perhaps we don't want a panic in a request handler to terminate the whole web server program. But that's none of tokio's business! It's web framework's or even web application's business! It is possible to use tokio for something besides web applications, and in those use cases panics definitely shouldn't be silently ignored.

Consider reopening.",2002,2020-07-25T11:03:25Z,0
414,Darksonn,"Regarding the ""anywhere else in Rust"" part, I will note that we are mirroring the behavior of `std::thread`. See also #1830 and  #1879.",2002,2020-07-25T11:06:36Z,0
415,carllerche,"`tokio::spawn` models `thread::spawn`. As @Darksonn  mentioned, `thread::spawn` does not abort the process on panic. Spawned tasks are unwind-safe due to the `Send + 'static` bound.

In order to deviate from `thread::spawn`'s behavior, we would need a compelling argument.

I could buy into a `shutdown_on_panic` flag to runtime given a compelling argument. One would have to explain why `std`'s behavior is not sufficient (i.e. configure the process to abort on panic).",2002,2020-07-25T18:25:31Z,0
416,s97712,"> `tokio::spawn` models `thread::spawn`. As @Darksonn mentioned, `thread::spawn` does not abort the process on panic. Spawned tasks are unwind-safe due to the `Send + 'static` bound.
> 
> In order to deviate from `thread::spawn`'s behavior, we would need a compelling argument.
> 
> I could buy into a `shutdown_on_panic` flag to runtime given a compelling argument. One would have to explain why `std`'s behavior is not sufficient (i.e. configure the process to abort on panic).

What about `spawn_local`? Whether to consider to end the current thread when panic in the ""local task""?",2002,2020-08-06T19:05:25Z,0
417,Darksonn,I don't think `spawn_local` and `spawn` should have _different_ behaviour on this point.,2002,2020-08-06T20:24:12Z,0
418,hawkw,"> > 
> 
> What about `spawn_local`? Whether to consider to end the current thread when panic in the ""local task""?

I think inconsistent behavior between `spawn` and `spawn_local` is not ideal --- it would introduce more complexity and confusion.

> I could buy into a `shutdown_on_panic` flag to runtime given a compelling argument. One would have to explain why `std`'s behavior is not sufficient (i.e. configure the process to abort on panic).

IMO, the main argument for a `shutdown_on_panic` flag is for test code. If assertions are made in code that ends up being run in a spawned task, the `JoinHandle`s of all those spawned tasks must be awaited in the main test body to ensure panics from assertion failures are propagated. This can be unwieldy, and in some cases, it's easy to misplace a `JoinHandle` and forget to await it, resulting in a test that passes even if an assertion fails --- which is far from ideal. If there was a `shutdown_on_panic` flag, I would definitely use it in tests (and might want `tokio::test` to enable it).
",2002,2020-08-06T20:27:54Z,0
419,s97712,"If these tasks are running in the same thread and one of tasks is panic, why the other tasks still working, which makes me more confused.",2002,2020-08-07T06:31:02Z,0
420,Darksonn,"@s97712 Panics in spawned tasks are caught, just like they are for spawned threads in std. Tasks spawned with the ordinary `tokio::spawn` function also share their threads in some manner.",2002,2020-08-07T06:47:19Z,0
421,lamafab,Any progress on this?,2002,2021-12-01T14:01:13Z,0
422,Darksonn,"No, there's currently no way to do this.",2002,2021-12-01T21:48:31Z,0
423,Venryx,"Having a way to tell Tokio to ""not catch"" panics that occur in its threads seems like a useful feature for me.

My use-case: I have my Rust program deployed in Kubernetes. When a panic occurs, I want my program to crash/completely-close, so that Kubernetes can notice the crash and perform its regular handling (eg. restarting the pod, unless it keeps crashing immediately, in which case back off for a while).

I looked through the source-code of Tokio, and could not find a way to directly achieve what I wanted. That said, here are some workarounds I have found.

#### Workaround 1

Enable Rust's ""abort on panic"" setting.

You can do this by...
A) Adding the following to your root `Cargo.toml` file, as [seen here](https://stackoverflow.com/q/47663961):
```
[profile.XXX]
panic = ""abort""
```
B) Or, by adding `-C panic=abort` to the rustflags, as [seen here](https://stackoverflow.com/a/47664111).

You can control the granularity of the stack-traces logged to the console by setting the `RUST_BACKTRACE` environment variable:
```
RUST_BACKTRACE=0 # no backtraces
RUST_BACKTRACE=1 # partial backtraces
RUST_BACKTRACE=full # full backtraces
```

#### Workaround 2

Add a custom panic handler, which receives the error, prints a backtrace (optionally), and then manually aborts your program (optionally):
```
#![feature(backtrace)]

use std::backtrace::Backtrace;

#[tokio::main]
async fn main() {
    //panic::always_abort();
    panic::set_hook(Box::new(|info| {
        //let stacktrace = Backtrace::capture();
        let stacktrace = Backtrace::force_capture();
        println!(""Got panic. @info:{}\n@stackTrace:{}"", info, stacktrace);
        std::process::abort();
    }));

    [...]
}
```

I like this approach better because it gives me control of how much of the stacktrace to print (they can be quite long!), as well as whether the panic is of a type that is worth calling `abort()` for.

The one main drawback is that the backtrace-generation code (`Backtrace.capture()`) is currently only available on Rust nightly.

****

If you want to use the backtrace-generation on Rust stable, you can actually, but it requires a hack where you set this environment variable: `RUSTC_BOOTSTRAP=1` (as [described here](https://fasterthanli.me/articles/why-is-my-rust-build-so-slow#:~:text=I%27m%20going%20to%20export%20RUSTC_BOOTSTRAP%3D1%20and%20have%20stable%20pretend%20it%27s%20a%20nightly%20build))

You can set that as a global environment variable, or have it set specifically for your cargo-build command.

For Docker: Just add a `ENV RUSTC_BOOTSTRAP=1` line before your build commands. (or use `RUN RUSTC_BOOTSTRAP=1 <rest of command>` for each command)

For rust-analyzer (in VSCode): Add this to your project's `.vscode/settings.json` file:
```
    ""rust-analyzer.server.extraEnv"": {""RUSTC_BOOTSTRAP"": ""1""}
```",2002,2022-01-24T19:06:01Z,0
424,notgull,"What kind of work is left here to do? This would be very useful to me, as I would like to be able to wrap a [`breadx::Display`](https://docs.rs/breadx/latest/breadx/display/trait.Display.html) in an `AsyncHandle` on Windows in order to avoid having to define entirely different types to work with async.",3781,2022-01-24T23:50:57Z,0
425,Darksonn,"I believe the major issue is determining what the API surface should be. We don't want to stabilize something that only works for a few kinds of handles, or which has other challenges. I simply don't understand handles well enough to tell whether any given proposal for an API surface is appropriate.",3781,2022-01-25T06:45:40Z,0
426,Matthias247,"If the OS limit is readable we could set the max threadpool size to `min(OS_LIMIT, THREADPOOL_SIZE)`. That would queue the task. However that doesn't account for the fact that you likely can change the OS limit during runtime of a process.

Returning an error is rather ugly. I would rather keep the current way, and ask users to increase limits. I think users might already get other panics if opening objects above limits (e.g files).
",2309,2020-03-12T03:38:25Z,0
427,Darksonn,What is the status on this?,2309,2020-04-20T12:26:44Z,0
428,kayru,"I've hit this issue recently in a production environment (application running in a docker container, based on very vanilla `alpine:latest`). The OS thread limit seemingly was quite high. We also limit the number of blocking threads to a pretty low number (12 in this specific case). The unwrap panic [here](https://github.com/tokio-rs/tokio/blob/master/tokio/src/runtime/blocking/pool.rs#L248) is very unfortunate.

Executing the task on an already-existing blocking thread is a potential solution sometimes, but I can definitely see how it might lead to deadlocks in pathological cases. In my particular application, I am actually able to handle a failure gracefully and so a guaranteed non-panicing variant of blocking task spawn API would be welcome.

A file handle limit example mentioned in this thread isn't quite the same as this panic, since nothing unwraps the file open result.

What makes this issue particularly annoying is that it can happen quite sporadically and it's hard to catch it in the act to really diagnose things. One potential workaround that I have considered was to just force spawn all the blocking worker threads on startup and turn off the timeout. This may be a reasonable thing to do for 12 thread case, but not so much for default 512 max :)",2309,2021-06-22T12:55:26Z,0
429,rapiz1,I've received users' feedback hitting this issue. The thread limit is high and unlikely to be reached. However the panic still occurs,2309,2022-01-25T06:46:35Z,0
430,rapiz1,"After some digging, this boils down to `pthread_create` on unix. https://github.com/rust-lang/rust/blob/e7825f2b690c9a0d21b6f6d84c404bb53b151b38/library/std/src/sys/unix/thread.rs#L87

From `pthread_create`'s man page:
```
ERRORS
       EAGAIN Insufficient resources to create another thread.

       EAGAIN A  system-imposed  limit on the number of threads was encountered.  There are a number
              of limits that may trigger this error: the RLIMIT_NPROC soft resource limit  (set  via
              setrlimit(2)),  which  limits  the number of processes and threads for a real user ID,
              was reached; the kernel's system-wide limit on the number of  processes  and  threads,
              /proc/sys/kernel/threads-max,  was  reached  (see  proc(5));  or the maximum number of
              PIDs, /proc/sys/kernel/pid_max, was reached (see proc(5)).
```

So there's something wrong with the system resource and it's not tokio's fault.

Maybe one way forward is to create a spawn_blocking version that returns a Result and doesn't panic.",2309,2022-01-25T07:29:08Z,0
431,Darksonn,"Well, spawn_blocking uses a thread pool. If it is unable to spawn more threads, it could just keep using the ones it has.

Of course if it's unable to spawn even on spawn_blocking thread, then there's a problem. ",2309,2022-01-25T08:30:45Z,0
432,avnerbarr,"maybe this explains? what is the workaround?

```
/// It can also panic whenever a timer is created outside of a
/// Tokio runtime. That is why `rt.block_on(sleep(...))` will panic,
/// since the function is executed outside of the runtime.
/// Whereas `rt.block_on(async {sleep(...).await})` doesn't panic.
/// And this is because wrapping the function on an async makes it lazy,
/// and so gets executed inside the runtime successfully without
/// panicking.
#[track_caller]
pub fn timeout<T>(duration: Duration, future: T) -> Timeout<T>
where
    T: Future,
{
```

",4423,2022-01-25T15:48:29Z,0
433,Darksonn,"Does the standard library provide any blocking versions of this feature? If so, then we should probably mirror their API.",4422,2022-01-25T11:01:43Z,0
434,teozkr,"Not that I'm aware of, no. The closest thing would be to use `socket2` to create a `Socket`, use their `AsRawFd` and `libc::fchmod` to set the mode, bind that, and finally turn that into a `UnixListener`. But, eww.

And I don't even think that would actually work, since `socket2::Socket` doesn't seem to expose a way to bind to a path (rather than a `SocketAddr`).",4422,2022-01-25T11:11:18Z,0
435,Darksonn,"Ok, I am open to adding some sort of `UnixSocket` to Tokio. It would be helpful if you are able to figure out which methods it would be useful to have on such a type, besides the one you already mentioned, if any.",4422,2022-01-25T11:44:00Z,0
436,Darksonn,"As a temporary workaround, you can perform the appropriate syscalls yourself with `libc` and convert the file descriptor into a Tokio `UnixListener`.",4422,2022-01-25T11:44:33Z,0
437,teozkr,"> Ok, I am open to adding some sort of `UnixSocket` to Tokio. It would be helpful if you are able to figure out which methods it would be useful to have on such a type, besides the one you already mentioned, if any.

I wish, but I'm still trying to get the hang of UDSes myself, so I'm not sure I'd be too useful sadly.",4422,2022-01-25T16:59:30Z,0
438,Darksonn,"This is because file operations are offloaded to the `spawn_blocking` pool, and you need the flush to wait for it to complete. Normally the runtime will always wait for `spawn_blocking` tasks when shutting down, but it actually only waits if the `spawn_blocking` task has _started_ before the shutdown happens. So the places where your operation fails is if the runtime shutdown happens before the `spawn_blocking` task starts.

Note that the above implies that the data can only be lost if the write happens just before runtime shutdown. Otherwise it will complete normally even if the file is dropped before the write completes.

It seems like async-std's approach is the following:
```Rust
impl Drop for File {
    fn drop(&mut self) {
        // We need to flush the file on drop. Unfortunately, that is not possible to do in a
        // non-blocking fashion, but our only other option here is losing data remaining in the
        // write cache. Good task schedulers should be resilient to occasional blocking hiccups in
        // file destructors so we don't expect this to be a common problem in practice.
        let _ = futures_lite::future::block_on(self.flush());
    }
}
```
However I would prefer to avoid this solution.

If we want to solve this, I would find it better to have the runtime somehow always wait for the file operation before shutting down.",4296,2021-12-04T10:33:42Z,0
439,krtab,"Hi!

> This is because file operations are offloaded to the spawn_blocking pool, and you need the flush to wait for it to complete. 

I don't understand why this guarantee is not/cannot be upheld by awaiting `poll_write`. Before diving into this issues (and the associated [thread](https://users.rust-lang.org/t/async-write-all-sometimes-silently-fails/68195) on URLO), my mental model was:

1. In an async context, I'm calling (indirectly or not) `poll_write`. This return a Future which will be `Ready` **once the bytes have actually been passed on to the OS for writting**¹.
2. Upon the first polling of this Future, a ""task"" (i'm unsure about the proper terminology here) is started on the blocking thread_pool. This task will use the blocking OS primitives to write the bytes and **once this is done wake the future by calling its waker's method**.
3. When polled afterwards, the Future checks if some internal state has been changed to ""n bytes have been succefuly written"" and if so returns `Ready(Ok(n))`

But my understanding is that it is rather:

2. Upon polling, the Future returns `Ready(Ok(n))` if n bytes have successfully been written to some internal buffer shared with the blocking task.

Is my first mental impractical to carry out, or is it a deliberate choice? In any case, I think @igorsol had a similar confusion so there may be some way we can think of improving the documentation.

------

¹ What I mean by ""passed on to the OS for writting"" is akin to `<std::fs::File as std::io::Write>::write` has returned.",4296,2021-12-04T11:59:45Z,0
440,Darksonn,"Unfortunately, having `poll_write` wait until the write actually completes is incompatible with the contract for `AsyncWrite`. If you return `Poll::Pending`, then the user may supply a different buffer next time they call `poll_write`. This becomes visible in the `cancellation safety` section for `AsyncWriteExt::write`, which says the following:

> This method is cancellation safe in the sense that if it is used as the event in a `tokio::select!` statement and some other branch completes first, then it is guaranteed that no data was written to this `AsyncWrite`.

If the `write` call returns `Poll::Pending`, then some other branch in a `select!` can complete first but the data would still be written in violation with the above.

The behavior is conceptually similar to writing to a buffered writer.",4296,2021-12-04T18:05:06Z,0
441,igorsol,"Just some additional thoughts:
1. So write_all() reports that future is completed when actual write is not completed yet. If write_all() is a write operation then its future just lies about operation completion. If `write_all()` is not write operation then what it is? Maybe it should be documented as some scheduling operation?
2. `write_all()` returns `io::Result<()>` but if write operation is not completed then it cannot return result of actual writes. Does it mean that if there is some real IO error it will be lost?
3. I don't understand how file operations in `spawn_blocking` pool are executed after File's destructor. Is file handle duplicated or stored behind some reference-counting smart pointer? Sorry I didn't look into tokio sources. The fact that file handle lives longer than `File` instance is also very confusing.",4296,2021-12-06T11:13:02Z,0
442,Darksonn,"The current semantics of `write` (which `write_all` inherit for its last call to `write`) is that if a write has already been started, then it will wait until that write completes, then starts a new write and immediately return. Io errors are returned on the next call to write or flush. And yes, the file descriptor is reference counted between the `File` object and the background task.

The documentation already contains the following snippet:

> A file will not be closed immediately when it goes out of scope if there are any IO operations that have not yet completed. To ensure that a file is closed immediately when it is dropped, you should call `flush` before dropping it.

Of course, documentation can always be improved.",4296,2021-12-06T12:49:42Z,0
443,malaire,"> ... starts a new write and immediately return. Io errors are returned on the next call to write or flush ...

This at the very least should be documented as it's completely unexpected behavior at least for me.

So errors returned by `write` are NEVER about that write which user attempted to do but about PREVIOUS write. Totally unexpected.",4296,2021-12-06T16:17:37Z,0
444,BraulioVM,"From @Darksonn's comment above
> If we want to solve this, I would find it better to have the runtime somehow always wait for the file operation before shutting down.

Is this something you think should be solved? Would it make sense that I look into writing a PR along the lines those lines? Or is it likely that the change wouldn't be accepted? ",4296,2021-12-11T15:26:17Z,0
445,Darksonn,"If we can find a good solution, then I am ok with making the change, but it is probably best to discuss the details of the implementation idea with me before you spend a lot of time on implementing it.",4296,2021-12-11T16:12:09Z,0
446,BraulioVM,"Cool, I'll have a look and if I find a way it could be solved then I'll draft a proposal to see what you think",4296,2021-12-11T16:31:16Z,0
447,iitalics,"I ran into this before, I think the main point of confusion is how `std::fs::File` immediately flushes on drop so `{ let f = File::create(); f.write_all(...)?; }` just works; then you port this line by line to tokio and it doesn't anymore. *yet* the `write_all` method is doing the same behavior in both API's, technically speaking. It's the invisible flush that is different.

I don't really think the API should be changed but perhaps it should be documented loudly that this is something you may need to handle.",4296,2021-12-12T02:51:56Z,0
448,malaire,"> I ran into this before, I think the main point of confusion is how `std::fs::File` immediately flushes on drop so `{ let f = File::create(); f.write_all(...)?; }` just works; then you port this line by line to tokio and it doesn't anymore. _yet_ the `write_all` method is doing the same behavior in both API's, technically speaking. It's the invisible flush that is different.

Behaviour is definitely NOT same. Standard `write_all` is [documented](https://doc.rust-lang.org/std/io/trait.Write.html#method.write_all) to ` not return until the entire buffer has been successfully written or such an error occurs` while last `write` of tokio `write_all` doesn't even bother checking for errors, it just `starts a new write and immediately return`.
",4296,2021-12-12T04:04:54Z,0
449,Darksonn,"I agree that the behavior is not the same, but regarding the standard `write_all`, if you use a `BufWriter<File>`, then you get a very similar behavior with the std `write_all` and it also will not always catch errors until you flush, so this is not unique to Tokio's IO traits.",4296,2021-12-12T09:45:46Z,0
450,BraulioVM,This issue is fixed now! ,4296,2022-01-25T19:10:33Z,0
451,1tgr,"I have this workaround:

```rust
fn set_codec<T: AsyncRead + AsyncWrite, C1, C2: Encoder + Decoder>(framed: Framed<T, C1>, codec: C2) -> Framed<T, C2> {
    let parts1 = framed.into_parts();
    let mut parts2 = Framed::new(parts1.io, codec).into_parts();
    parts2.read_buf = parts1.read_buf;
    parts2.write_buf = parts1.write_buf;
    Framed::from_parts(parts2)
}
```

But it seems like `Framed<T, U>` needs a `set_codec` method that returns `Framed<T, U2>`, or likewise on `FramedParts<T, U>`.",717,2018-10-21T19:46:39Z,0
452,carllerche,"An interesting case. It feels pretty specialized, but also that it should be better supported.

We could possibly add `fn from_framed(framed, codec)` that takes a framed instance and a new codec.

Thoughts?",717,2018-11-19T17:43:34Z,0
453,1tgr,"Yep, `from_framed(framed, codec)` would do the job.

Here is where I ended up using my workaround:
https://github.com/1tgr/rust-websocket-lite/blob/d3594c08124fcabd03ebb87a086e811e2b925ac1/src/client.rs#L187-L194",717,2018-11-19T17:45:51Z,0
454,carllerche,One issue @seanmonstar raises is that you might want to take ownership of data form the first codec and move it into the second codec. My proposed API would not solve that.,717,2018-11-19T17:47:36Z,0
455,carllerche,We could add `map_codec(|old_codec| build_new_codec(old_codec))` instead.,717,2018-11-19T18:14:11Z,0
456,carllerche,"I think the solution for this will be to switch to using `AsyncBufRead` instead of a custom solution. That said, the trait needs to be tweaked to support our use case.",717,2019-12-21T17:51:23Z,0
457,bschwind,"> I think the solution for this will be to switch to using `AsyncBufRead` instead of a custom solution.

@carllerche can you expand a bit on how the `AsyncBufRead` trait will solve this issue?",717,2020-02-10T03:55:16Z,0
458,carllerche,"Refs: https://github.com/tokio-rs/tokio/issues/2428

A BufReader will contain the buffer as well as the stream. This type can be removed from the codec and passed to a new one.

I am going to close this issue as it is relatively low priority and is blocked on #2428. However, once #2428 happens (soonish), anyone should feel free to attempt this work.",717,2020-07-24T15:59:46Z,0
459,saiintbrisson,"As #2428 and #2896 have been put aside, are there any other blockers for this implementation? If not, I'd be willing to work on it.",717,2022-01-26T15:36:17Z,0
460,Darksonn,I'm not aware of any blockers.,717,2022-01-26T15:54:30Z,0
461,Darksonn,"You will need to run rustfmt on this.
```
rustfmt --check --edition 2018 $(git ls-files '*.rs')
```",4427,2022-01-27T08:41:47Z,0
462,sfackler,It seems like this is a lot of complexity that could be avoided if your database client was Sync. That might be a more reasonable route?,3370,2021-01-04T01:15:59Z,0
463,AzureMarker,"I would like to avoid that. The specific DB in my use case is SQLite, but there are other `Send + !Sync` DB connections (see the connection types used by `diesel` for example). tokio-postgres gets around this by moving the connection to its own future and passing around handlers to it in the form of [`Client`](https://docs.rs/tokio-postgres/0.7.0/tokio_postgres/struct.Client.html), but that is not a pattern available everywhere. There are also other situations besides DB connections which require `!Send` futures.

This is a hard to work around limitation of the multithreaded executor, so I think it's important to provide a general solution instead of only fixing my specific case.",3370,2021-01-04T01:51:22Z,0
464,MikailBag,"Global pool approach is something new for Tokio and Tokio-util.

I'd consider instead:
```rust
fn new_local_pool(/*some config, e.g. thread count*/) -> Handle;

struct Handle: Clone;

impl Handle {
    fn spawn(&self, fut) -> JoinHandle;
}

```
This way user can configure local pool (and use multiple pools if they wish).
Additionally you now have simple shutdown logic: when all `Handle`s are dropped, all worker threads stop.",3370,2021-01-04T09:59:53Z,0
465,AzureMarker,Thanks for the insight and suggestion @MikailBag! I'll rework the code to implement that.,3370,2021-01-04T16:00:15Z,0
466,AzureMarker,@MikailBag Is there anything blocking review of this PR?,3370,2021-03-03T20:55:33Z,0
467,MikailBag,"IMHO your PR looks good.

Please note that I'm not Tokio maintainer though.

",3370,2021-03-03T20:59:17Z,0
468,Darksonn,"I haven't had time to look closely at this PR yet, sorry.",3370,2021-03-04T11:23:14Z,0
469,Noah-Kennedy,I'll try and make time to go through this later.,3370,2021-09-30T16:38:12Z,0
470,AzureMarker,Thanks for the review @Darksonn! I only just saw it now for some reason... I'll go through and apply the suggestions soon.,3370,2021-10-27T05:08:50Z,0
471,zortax,"Hey, any updates on this?",3370,2021-11-18T19:15:29Z,0
472,Darksonn,"Ah, I didn't actually see that this had any updates.",3370,2021-11-19T08:56:22Z,0
473,Noah-Kennedy,"I'll review this tomorrow. Currently, I'm traveling, so I can't easily review code.",3370,2021-11-20T21:02:48Z,0
474,AzureMarker,"@Darksonn thanks for the review again. I updated the PR with the refactors. The two open issues are:
1. The non-blocking spawn_pinned function could use a better name, but even more, needs some more design work. See this comment: https://github.com/tokio-rs/tokio/pull/3370#discussion_r738929725
2. Decide if we should use `LocalPoolHandle::new` or `LocalPool::new` (returns `LocalPoolHandle` either way).",3370,2021-11-21T03:10:27Z,0
475,Darksonn,"> Decide if we should use `LocalPoolHandle::new` or `LocalPool::new` (returns `LocalPoolHandle` either way).

The `LocalPool` type is private, so it should be `LocalPoolHandle::new`.

> The non-blocking spawn_pinned function could use a better name, but even more, needs some more design work.

I think it probably makes sense to have the one that returns a `JoinHandle` async. I'd rather do this than block on having the thread spawn it.",3370,2021-11-24T09:18:47Z,0
476,AzureMarker,"Updated to make `spawn_pinned` async, with `spawn_pinned_blocking` being the blocking version (waits for the join handle to be returned via std channel).

I noted in a comment (https://github.com/tokio-rs/tokio/pull/3370#discussion_r757105109) that this does leave a gap, where we don't have a function that is nonblocking but also is synchronous (doesn't wait for the join handle). Do we still want this? I'm fine either way.",3370,2021-11-25T19:54:12Z,0
477,Darksonn,"Well, `spawn_pinned_blocking` is certainly not a good name because of its similarity with `tokio::task::spawn_blocking`.",3370,2021-11-25T21:13:14Z,0
478,AzureMarker,"I made `spawn_pinned` synchronously send the task and return the future, good idea. This also made `spawn_pinned_blocking` basically redundant, so I removed it (and now we don't have to decide its name).",3370,2021-11-25T21:27:49Z,0
479,AzureMarker,@Darksonn I patched up the issues related to task cancellation and task count decrementing. I think it's much more robust now.,3370,2021-12-05T01:13:32Z,0
480,AzureMarker,@Darksonn I think this PR is pretty close to being merged. Could you take another look?,3370,2022-01-25T01:32:32Z,0
481,Darksonn,Sorry about the many delays in reviewing this. ,3370,2022-01-27T14:25:48Z,0
482,AzureMarker,Thanks for reviewing!,3370,2022-01-27T16:00:52Z,0
483,MikailBag,"It seems that a basic integration is pretty straightforward: https://play.rust-lang.org/?version=stable&mode=debug&edition=2021&gist=c63e153f8a0eae7af6f84e7a7f76fb73
",4297,2021-12-04T13:19:54Z,0
484,Darksonn,"Tokio does not depend on the futures crate, and we aren't going to add any non-1.x crates to the public API of Tokio.",4297,2021-12-04T18:07:26Z,0
485,realcr,"@MikailBag This is a really neat workaround that I am definitely going to use. Thanks!

> Tokio does not depend on the futures crate, and we aren't going to add any non-1.x crates to the public API of Tokio.

I can understand this reasoning. Maybe such a trait should be somewhere in `core::future::*`? But then of course it should be a well thought trait. Maybe this is a future consideration, when things become more stable in the async ecosystem.
",4297,2021-12-05T06:41:10Z,0
486,Darksonn,"If such a trait gets added to the standard library, we would implement it once our minimum-supported-rust-version is recent enough to allow us to.

I'll close this issue for now.",4297,2021-12-05T09:48:50Z,0
487,Sytten,I feel this could be added to a tokio-compact library as one was created in the past for tokio 0.2 and futures 0.1.,4297,2022-01-27T17:07:27Z,0
488,BraulioVM,"No need to run CI on this one because I haven't changed any code yet, but I don't think I can disable it myself",4316,2021-12-12T20:40:30Z,0
489,Darksonn,"I'm not so sure about making this change for all `spawn_blocking` tasks. Certainly any tasks spawned after shutdown should not run. It's also unclear when it comes to tasks still in the queue if every thread up to the limit is currently taken.

However on the other hand, tasks spawned just before shutdown when the pool isn't full would be ok to always do it for. That is already behavior that can happen under the current implementation, so it would certainly not be breaking.",4316,2021-12-12T21:29:40Z,0
490,BraulioVM,">  Certainly any tasks spawned after shutdown should not run

I agree. I wouldn't change this bit of code, which handles that
https://github.com/tokio-rs/tokio/blob/4b6bb1d9a790b0b81e56d4d852944a2a08b1f70d/tokio/src/runtime/blocking/pool.rs#L174-L186

>  It's also unclear when it comes to tasks still in the queue if every thread up to the limit is currently taken.

So this would mean something like: once `shutdown` is called, every thread that is currently idle will get a chance to execute exactly one task before completely terminating. Is that it more less?

Note that even in the current implementation, it could be the case that once `shutdown` is called, a thread in the blocking pool could still take on and execute an arbitrary number of blocking tasks before terminating. Say for example you have just one thread in the blocking pool, and you `spawn_blocking` a task that takes 30s to complete, then, before that blocking task has finished, you schedule 500 more blocking tasks, and finally you call `shutdown`. The thread in the blocking pool would probably be executing

https://github.com/tokio-rs/tokio/blob/4b6bb1d9a790b0b81e56d4d852944a2a08b1f70d/tokio/src/runtime/blocking/pool.rs#L262-L267

and won't notice the `shutdown` after it has finished going through all the tasks in the queue. (I haven't actually tested this, it's just my understanding of the code).

In that sense, it's already possible that we'll complete all the blocking tasks in the queue before shutdown, even if all threads were taken at the time of shutdown. ",4316,2021-12-12T21:59:48Z,0
491,Darksonn,Interesting. ,4316,2021-12-13T10:09:29Z,0
492,BraulioVM,">  (I haven't actually tested this, it's just my understanding of the code).

I have confirmed this is the case. The following code:

```rust
#[test]
fn blocking_tasks_block_shutdown() {
    let rt = tokio::runtime::Builder::new_multi_thread()
        .worker_threads(1)
        .max_blocking_threads(1)
        .build()
        .unwrap();
    rt.block_on(async {
        for i in 0..10 {
            println!(
                ""TT: {:?} Scheduling task {}"",
                std::time::Instant::now(),
                i);
            let j = i;
            rt.spawn_blocking(move || {
                println!(
                    ""TT: {:?} Initiating task {}"",
                    std::time::Instant::now(),
                    j
                );
                std::thread::sleep(
                    std::time::Duration::from_secs(3)
                );
            });
        }

    });
    println!(
        ""TT: {:?} done scheduling tasks"",
        std::time::Instant::now(),
    );
    drop(rt);
}
```
produces the following output:
```
$ cargo test --all-features blocking_tasks_block -- --nocapture &| grep TT
TT: Instant { tv_sec: 500504, tv_nsec: 186272702 } Scheduling task 0
TT: Instant { tv_sec: 500504, tv_nsec: 186324929 } Scheduling task 1
TT: Instant { tv_sec: 500504, tv_nsec: 186330571 } Scheduling task 2
TT: Instant { tv_sec: 500504, tv_nsec: 186333739 } Scheduling task 3
TT: Instant { tv_sec: 500504, tv_nsec: 186335307 } Scheduling task 4
TT: Instant { tv_sec: 500504, tv_nsec: 186336593 } Scheduling task 5
TT: Instant { tv_sec: 500504, tv_nsec: 186339816 } Scheduling task 6
TT: Instant { tv_sec: 500504, tv_nsec: 186341120 } Scheduling task 7
TT: Instant { tv_sec: 500504, tv_nsec: 186345392 } Scheduling task 8
TT: Instant { tv_sec: 500504, tv_nsec: 186346793 } Scheduling task 9
TT: Instant { tv_sec: 500504, tv_nsec: 186351871 } done scheduling tasks
TT: Instant { tv_sec: 500504, tv_nsec: 186394403 } Initiating task 0
TT: Instant { tv_sec: 500504, tv_nsec: 186411780 } Initiating task 1
TT: Instant { tv_sec: 500507, tv_nsec: 186532582 } Initiating task 2
TT: Instant { tv_sec: 500507, tv_nsec: 186533490 } Initiating task 3
TT: Instant { tv_sec: 500510, tv_nsec: 186726888 } Initiating task 4
TT: Instant { tv_sec: 500510, tv_nsec: 186795241 } Initiating task 5
TT: Instant { tv_sec: 500513, tv_nsec: 187019986 } Initiating task 6
TT: Instant { tv_sec: 500513, tv_nsec: 187021385 } Initiating task 7
TT: Instant { tv_sec: 500516, tv_nsec: 187261531 } Initiating task 8
TT: Instant { tv_sec: 500516, tv_nsec: 187261759 } Initiating task 9
```

Note that all tasks get executed even 10 seconds after calling `drop` on the runtime. I'm actually a bit surprised that tasks get executed in groups of two, but anyway. ",4316,2021-12-13T21:05:38Z,0
493,BraulioVM,"As for ways forward, I can think of three alternatives:

1. We accept and embrace the fact that shutting down a runtime will block on completion of all blocking tasks that were scheduled before the shutdown. In this sense, the fact that, in some very specific circumstances, blocking tasks that were scheduled before shutdown do not get executed should be considered a bug. Users that do not want this behaviour can (and should) call `shutdown_timeout` or `shutdown_background`. I don't think that embracing this aspect of shutting down would negatively affect any existing users, because in many cases their applications were probably already blocking on shutdown, as evidenced by my example above.
2. We treat the blocking nature of `shutdown` as a bug and attempt to fix it. I don't think this perspective is backed by any of the tokio documentation. I believe this could break existing users, but I don't really have any data to back this claim (other than the issue referenced above, where users were surprised that some tasks wouldn't get executed).
3. We go for something in the middle, where we try something like https://github.com/tokio-rs/tokio/pull/4316#issuecomment-991973940 . I believe this could be a bit confusing and hard to explain to users (""depending on the timing of operations between threads, the runtime may either block until all blocking tasks have been executed, or until each idle thread in the blocking pool has executed one of the pending tasks in the pool. Some tasks may get discarded after one task has been assigned to each idle thread). I'm not sure if I'm mischaracterising what @Darksonn  meant in that comment though.

In my opinion as a tokio noob, option 1 makes more sense. I want to hear what the experts think though.

Any thoughts? ",4316,2021-12-13T21:21:35Z,0
494,carllerche,"Thoughts:

- Runtime shutdown is intended to **not** be graceful, favoring cancellation when possible, deferring graceful shutdown to the user.
- `write_all` losing data is a bug",4316,2021-12-13T22:31:21Z,0
495,BraulioVM,"Keeping the blocking nature of `shutdown` aside for a moment, couldn't we fix the original issue by doing a flush at the end of `write_all`? As @Darksonn pointed out, we cannot make `write` wait for the data to be written (https://github.com/tokio-rs/tokio/issues/4296#issuecomment-986068548), but I don't see why we couldn't change `write_all` to do a flush at the end. In fact the documentation for [`write_all`](https://docs.rs/tokio/1.14.0/tokio/io/trait.AsyncWriteExt.html#method.write_all) says:

> This method will not return until the entire buffer has been successfully written or such an error occurs

which we know is not true - the method may return before the last batch of data has actually been written to the file. Also, errors originating from the last write will not be returned to the user. The documentation also says that `write_all` is not cancellation-safe, which was the reason we couldn't change `write`, so maybe we can afford the change here?

If you agree, I could make a change in https://github.com/tokio-rs/tokio/blob/master/tokio/src/io/util/write_all.rs#L40 and then we could deal with runtime shutdown and blocking tasks in a separate PR",4316,2021-12-14T12:21:56Z,0
496,Darksonn,"For one, this would affect all IO resources - they are not customizable. For another, I still think it's a bug for `write` as well.",4316,2021-12-14T12:37:17Z,0
497,BraulioVM,"> For one, this would affect all IO resources - they are not customizable.

But is that a problem? I see that flush is a no-op on UNIX domain sockets and TCP streams. Not sure about other IO resources though. 

> For another, I still think it's a bug for write as well.

As in, `write` should wait for the content to actually be written? You pointed out that changing that could be a breaking change. Are you saying the breaking change is worth it because it's preferable to have `write` wait for the writing result? 

Or are you saying that `write` may not wait for the data to be written but that we should make sure the associated blocking task gets executed? ",4316,2021-12-14T13:10:29Z,0
498,Darksonn,"> But is that a problem? I see that flush is a no-op on UNIX domain sockets and TCP streams. Not sure about other IO resources though.

The `write_all` method should not include a call to flush. E.g. if someone wraps their tcp stream in a `BufWriter`, then they should be able to call `write_all` with many small segments and flush afterwards.

> > For another, I still think it's a bug for write as well.
> 
> As in, write should wait for the content to actually be written? You pointed out that changing that could be a breaking change. Are you saying the breaking change is worth it because it's preferable to have write wait for the writing result?
> 
> Or are you saying that write may not wait for the data to be written but that we should make sure the associated blocking task gets executed?

I mean that the runtime should wait for the `spawn_blocking` task associated with the `write` call when shutting down. It's not possible to wait for the task in the `write` call itself.",4316,2021-12-14T13:56:50Z,0
499,BraulioVM,"> The write_all method should not include a call to flush. E.g. if someone wraps their tcp stream in a BufWriter, then they should be able to call write_all with many small segments and flush afterwards.

Got it, that makes sense.",4316,2021-12-14T18:25:01Z,0
500,BraulioVM,"> I mean that the runtime should wait for the spawn_blocking task associated with the write call when shutting down. It's not possible to wait for the task in the write call itself.

Ok makes sense. What if:

1. We extend the `UnownedTask` struct to also contain (or to expose from an inner field, like the `RawTask`? No idea where this would live yet) a `is_mandatory: bool` field. 
2. The value of `is_mandatory` for tasks spawned with `spawn_blocking` will be `false`.
3. We will add a new method `spawn_mandatory_blocking` that will make sure to set `is_mandatory: true` in the created tasks.
4. `File` will spawn its file-writing tasks using this new `spawn_mandatory_blocking` function.
5. On shutdown, threads in the blocking pool will work through the queue of blocking tasks checking the `is_mandatory` field. If the field is `false`, the task will be cancelled. If the field is `true`, the task will be executed. (We may deal with [the race in the shutdown logic](https://github.com/tokio-rs/tokio/pull/4316#issuecomment-992908736) in a separate PR).

Does this sound like a promising approach? I have also thought about extending the runtime to know about `File`s specifically, and the operations associated to them, but it seems to me leveraging the `BlockingPool` for all the file IO is a good idea? It feels a bit weird though to have a general-sounding concept (""mandatory blocking task"") to achieve a very concrete goal (`write` on `File`s do not get lost) and nothing else.

There would still be some open questions:
- Better names for all of these concepts (`is_mandatory`, `spawn_mandatory_blocking`)
- What to do about `shutdown_background` and `shutdown_timeout`? Is it fine to lose the writes there? 
- Do we expose these new APIs (`spawn_mandatory_blocking`, basically) to the users? Or do we keep them internal to the library? It makes sense to me that we would want to keep them private. As @carllerche says, users should handle graceful shutdown on their own and not rely on the runtime shutdown to do it for them.",4316,2021-12-15T20:41:09Z,0
501,Darksonn,"This is more or less what I was thinking the solution would be myself, so it makes sense.

> Better names for all of these concepts (`is_mandatory`, `spawn_mandatory_blocking`)

The names are ok for now.

> What to do about `shutdown_background` and `shutdown_timeout`? Is it fine to lose the writes there?

The operations will continue in the background if you use them. If someone exits the process before that, then we lose the writes, but there's not anything we can do about that.

> Do we expose these new APIs (`spawn_mandatory_blocking`, basically) to the users?

No.",4316,2021-12-15T20:49:06Z,0
502,BraulioVM,Cool! I'll look into it ,4316,2021-12-15T21:02:38Z,0
503,BraulioVM,"This is still a work in progress.

I've clumsily written what I think is a fix. I have no idea whether I put the `is_mandatory` field in the right type because I'm still figuring out how the task-adjacent types relate to each other. 

I am thinking about how to test the fix. I managed to test it manually using the code in the original issue. Without the fix, that test fails at most in a few hundred iterations. With the fix, I had to ctrl-c the script after 60K thousand iterations, as it wasn't failing anymore. However, this is not a very good test for the automated test suite.

I was thinking about using `loom` to prove the fix is actually a fix, but I have seen it's only used for testing the concurrency components, and not to test higher-level components like the runtime, so maybe `loom` is not a good candidate for the test that I want to write? The test would just be something like (pseudocode):

```rust
loom::model(|| {
   runtime.block_on(async {
       let mut file = File::create(""foo.txt"").await?;

      file.write_all(b""some bytes"").await?;
      Ok(())
    });
   drop(runtime);
   // check file contains ""some bytes""
});
```

but maybe it's not possible to do this...",4316,2021-12-17T19:11:41Z,0
504,Darksonn,"You can probably write literally the loom test you posted here. It would go in [this directory][1]. Though we might want the test to just use `spawn_blocking_mandatory` directly so we don't need to actually involve the file system.

[1]: https://github.com/tokio-rs/tokio/tree/master/tokio/src/runtime/tests",4316,2021-12-17T20:21:04Z,0
505,BraulioVM,"I tried writing a loom test but couldn't manage to write one that would fail when using `spawn_blocking` in the way that I would expect. Maybe it would have, but if so, it would have taken >30minutes. As a sanity check, I've written a test that doesn't use loom but uses good-ol' looping, trying the same thing many times and hoping for the best.The test is called `mandatory_blocking_tasks_get_executed`, and you can find it in my last commit. The goal of the test is:

1. To fail quickly when using `spawn_blocking` instead of `spawn_mandatory_blocking`. This shows that the test is effective at covering what it's intended to cover.
2. To succeed when using `spawn_mandatory_blocking`, to show the fix is actually a fix.

The approach is not great for some reasons:

1. It's very coupled to the implementation of the blocking pool. For example, the test makes sure to spawn a first blocking task before spawning the one that is supposed to mutate the atomic boolean. The test has to tickle the blocking pool in a very specific way to get a failure when using `spawn_blocking`. 
2. It's non-deterministic. If there is a test failure, that's deterministic evidence of an error in our implementation (assuming the test is implemented correctly). However, the absence of a test failure is not evidence that the implementation is correct. Classic concurrency. I would love to have got the loom test working.

On the other hand, the test fails relatively quickly when using `spawn_blocking` instead of `spawn_mandatory_blocking` (see [here](https://gist.github.com/BraulioVM/9bdae9b99b11772cfc1ba01d8a6e9d09), the numbers represent the iterations it took the test code to fail when using `spawn_blocking` instead of `spawn_mandatory_blocking`). That's good in my opinion.

What do you think of the testing approach? Is it acceptable? Or should I keep trying to get a loom test that showcases the issue? 

Once we agree on the testing approach I will proceed to clean-up the implementation (of both the tests and the fix).",4316,2021-12-27T22:32:23Z,0
506,Darksonn,"This loom test fails, and if your PR is correct, then it should succeed if changed to use `spawn_blocking_mandatory`. (I didn't try it with your PR.)
```Rust
use crate::runtime::tests::loom_oneshot;

#[test]
fn spawn_blocking_should_always_run() {
    loom::model(|| {
        let rt = Builder::new_current_thread().build().unwrap();

        let (tx, rx) = loom_oneshot::channel();
        rt.spawn_blocking(|| {});
        rt.spawn_blocking(move || {
            let _ = tx.send(());
        });

        drop(rt);

        // This call will deadlock if `spawn_blocking` doesn't run.
        let () = rx.recv();
    });
}
```
Note that loom will catch the deadlock and panic, so the above test doesn't actually hang. You can put the test in one of the files in `src/runtime/tests` or make a new file in that directory.",4316,2021-12-28T09:15:06Z,0
507,BraulioVM,"Thank you very much for helping out @Darksonn! That test fails as expected with `spawn_blocking` and succeeds with `spawn_mandatory_blocking` (at least using `LOOM_MAX_PREEMPTIONS=5`). I will remove the previous test I wrote, include the one you suggested, and clean up the implementation of the fix",4316,2021-12-28T16:05:10Z,0
508,BraulioVM,"Well there are a few builds failing that I could put more time into fixing, but I wanted to ask first whether the approach seems reasonable. I'm most interested in whether adding the `is_mandatory: bool` field to `UnownedTask` was the right thing to do. I personally have no idea. On the one hand, it currently is just a reference-counted handle to a task, and maybe it's very important to keep it small. On the other hand, it's the only type that applies specifically to blocking tasks, which the change is concerned with. 

Is it fine to put the `is_mandatory` field in the `UnownedTask`? Do you have suggestions about other places where to store this information?",4316,2021-12-29T19:37:11Z,0
509,Darksonn,I do think that the approach makes sense.,4316,2021-12-29T20:52:13Z,0
510,BraulioVM,"I find it odd that I'm getting https://github.com/tokio-rs/tokio/runs/4666196266?check_suite_focus=true#step:5:30 even though I have https://github.com/tokio-rs/tokio/pull/4316/commits/c549b72b6f30c4a94d5854a8a5f756489c42b30d#diff-c632261b41085469310e4840a71f9582257c348bf23807251dde960fc985b21cR115

It seems that loom tests do not respect `cfg_attr(test` but that contradicts the fact that the tests module gets added to the build...

I will look into it later",4316,2021-12-30T11:34:29Z,0
511,BraulioVM,"Ok I've figured it out...

When running `cargo test` on `<repo>/tokio`, you would expect that `rustc` only gets executed against `tokio` using the `--test` flag, which would make the `cfg_attr(test, ...)` conditional attribute apply. That's not true though. When running `cargo test` on `tokio`, the tokio crate gets built *twice*, once without `--test` and once with (this can be verified using strace). This is because, f.i, tokio specifies a dev-dependency on `tokio-test, which specifies back a dependency on `tokio`.

So when running a regular test build, tokio will first get built without `--test`. We will not get dead code errors there because for that build, `tokio::fs::File` uses the new `spawn_mandatory_blocking`. For the next build, the one with `--test`, we will not get dead code errors because, even though `tokio::fs::File` uses a mock `spawn_mandatory_blocking`, the `cfg_attr(test` is working as expected.

Things are different for loom builds. We will first get a build of tokio without `--test` but with the `--cfg loom` flag. The fact that `tokio::fs::File` uses `spawn_mandatory_blocking` won't save us here, because https://github.com/tokio-rs/tokio/blob/master/tokio/src/fs/mod.rs#L1 . The fact that `s

The solution I can think of is extending the `cfg_attr(test` to `cfg_attr(any(test, loom)`, which is a bit unfortunate because `spawn_mandatory_blocking` *is* used in the loom tests, only not in the regular loom build of the crate, which is triggered by the cyclical dependency between tokio and tokio-test.",4316,2021-12-30T13:18:37Z,0
512,Darksonn,The attributes looks good :+1: ,4316,2021-12-30T14:33:12Z,0
513,BraulioVM,Is there anything you'd want me to do before this can get reviewed? ,4316,2022-01-20T20:17:37Z,0
514,Darksonn,"Ah, no, I had just missed the notification that you updated the PR.",4316,2022-01-21T18:10:48Z,0
515,Darksonn,"Besides [my comment on the error](https://github.com/tokio-rs/tokio/pull/4316#discussion_r789892292), I am fine with this.",4316,2022-01-21T20:13:39Z,0
516,BraulioVM,"> Besides [my comment on the error](https://github.com/tokio-rs/tokio/pull/4316#discussion_r789892292), I am fine with this.

I believe this is done then? 

",4316,2022-01-25T18:34:37Z,0
517,Darksonn,Thanks!,4316,2022-01-25T18:46:22Z,0
518,carllerche,"I missed this, but I just wanted to say thanks to all and this solution is simpler than anything I had thought of yet. Well done :+1: ",4316,2022-01-27T21:07:39Z,0
519,cecton,I'm trying out with the current API using type alias but it seems it doesn't work with my use case because the TCP address/Unix socket path is provided at runtime so I would need to dynamic dispatch somewhere. `Box<dyn Listener>` doesn't work because the type aliases are required.,4385,2022-01-12T13:37:01Z,0
520,Darksonn,You can implement the trait for the `Either` type also in `tokio-util` to get what you want.,4385,2022-01-12T14:04:08Z,0
521,cecton,Oh I think I see! Thanks!,4385,2022-01-12T14:39:52Z,0
522,cecton,@Darksonn ok I think I'm ready for review ,4385,2022-01-13T11:47:49Z,0
523,cecton,(Fixed clippy stuff),4385,2022-01-13T16:26:17Z,0
524,cecton,I think this is ready for review again.,4385,2022-01-14T11:17:21Z,0
525,Darksonn,Your branch is 59 commits behind tokio-rs:master and is therefore missing a fix for the CI failure in the two FreeBSD CI runs. Please merge master into your branch.,4385,2022-01-14T14:47:56Z,0
526,cecton,"> Your branch is 59 commits behind tokio-rs:master and is therefore missing a fix for the CI failure in the two FreeBSD CI runs. Please merge master into your branch.

oops :sweat_smile: I didn't notice somehow, sorry! Fixed in 812dfa5a440201ff15fafee93ce8ceabd44ebde9

And ready for review.",4385,2022-01-14T17:50:32Z,0
527,cecton,Thank you for your time ,4385,2022-01-27T16:49:08Z,0
528,Darksonn,Thank you for the PR!,4385,2022-01-27T21:44:06Z,0
529,Darksonn,Adding this to `tokio-util` is fine with me.,4048,2021-08-21T06:02:41Z,0
530,hi-rustin,I am working on this.,4048,2021-09-25T04:30:35Z,0
531,hi-rustin,"@Darksonn  When you have time, could you help describe what you think the right API looks like?",4048,2021-09-28T02:51:25Z,0
532,Darksonn,"Well there are probably several options. You could have an enum with the listener types hard-coded. You could also define a listener trait, which would have a `poll_accept` method, and have the enum's accept method work via that. If you use the trait, you could add the feature to the existing `Either` enum instead of adding a new one.",4048,2021-09-28T06:22:47Z,0
533,Hodkinson,Hi @hi-rustin how's it going with this one? I could have a look at it this week if you like.,4048,2022-01-10T07:22:40Z,0
534,Darksonn,"Please note that this PR was recently posted: #4385
",4048,2022-01-10T07:52:33Z,0
535,Darksonn,"Fixed in #4385.
",4048,2022-01-27T21:45:07Z,0
536,carllerche,@Darksonn done,4431,2022-01-27T22:10:19Z,0
537,carllerche,"I'm releasing, the other PR is still running CI (loom).",4431,2022-01-27T23:16:22Z,0
538,kitsuneninetails,I don't know the rocket usage under the covers that might isolate the specific usage of this take function.  We do not use take directly in our code.,4435,2022-01-28T01:57:15Z,0
539,LEXUGE,I can reproduce here.,4435,2022-01-28T07:41:56Z,0
540,Darksonn,"Sorry about that. It will be fixed by #4437, which I will release as soon as possible.",4435,2022-01-28T08:43:39Z,0
541,Darksonn,We are currently preparing a release in #4438.,4435,2022-01-28T09:08:29Z,0
542,Darksonn,"And indeed I think we should swallow the panic here. The reason is that panics in tasks are delivered to the user via the `JoinHandle`, so if the user has thrown the `JoinHandle` away, then they have told us they don't want the panic.",4430,2022-01-27T21:41:08Z,0
543,BraulioVM,Done? ,4430,2022-01-28T16:16:04Z,0
544,Darksonn,"Yeah, I didn't merge immediately because CI was still running.",4430,2022-01-28T16:20:49Z,0
545,Grubba27,"Hello there,  this is a good first issue ? seems like i only have to change 

```rust
#![cfg(feature = ""full"")]
```

for:
```rust
#![cfg(feature = ""time"")]
```

 right ? if there is something beside this ?",4341,2022-01-28T12:22:33Z,0
546,Darksonn,"Yeah, that seems like it would fix it.",4341,2022-01-28T12:24:36Z,0
547,Grubba27,"In stream_timeout if i use time or default it gives me errors on tests
other options are ok?:
```rust
net = [""tokio/net""]
io-util = [""tokio/io-util""]
fs = [""tokio/fs""]
sync = [""tokio/sync"", ""tokio-util""]
signal = [""tokio/signal""]
```",4341,2022-01-28T13:57:13Z,0
548,Darksonn,"Pick a feature or combination of features that works. You can list several features like this:
```Rust
#![cfg(all(feature = ""time"", feature = ""sync"", feature = ""io-util""))]
```",4341,2022-01-28T14:19:36Z,0
549,Grubba27,"With multiple features all tests in local ran smoothly, adjusting PR and linking here",4341,2022-01-28T14:29:58Z,0
550,Grubba27,"Thx btw, did not knew that could be done, multiple features at once in this manner",4341,2022-01-28T14:31:03Z,0
551,Grubba27,PR is in this [link](https://github.com/tokio-rs/tokio/pull/4441/files),4341,2022-01-28T14:31:43Z,0
552,Darksonn,It appears that the test does not compile. You will probably have to fix it.,4441,2022-01-28T14:41:08Z,0
553,Grubba27,I think was only a wrong import that was we did not see because we were not running these tests,4441,2022-01-28T16:25:49Z,0
554,Darksonn,I _did_ test locally that this fails before the bugfix.,4443,2022-01-28T17:28:37Z,0
555,biluohc,Please continue to review @Darksonn ,4436,2022-01-28T09:43:19Z,0
556,Darksonn,It seems reasonable to me.,4436,2022-01-28T09:48:40Z,0
557,biluohc,"ok, thanks you very much",4436,2022-01-28T09:51:27Z,0
558,LucioFranco,"This looks like an oversight in the docs, `Receiver` is not supposed to implement `Clone`. So would be good to get that removed from the docs. The only way to get more `Receiver`'s is to call `Sender::subscribe`.",2032,2019-12-27T17:32:55Z,0
559,vadimcn,"> Receiver is not supposed to implement Clone

Is there a technical reason for this?  

I'm trying to implement a pub/sub-like architecture with a single publisher and multiple subscribers.  Once publisher shuts down, the subscribers must shut down as well.   
It's be nice if I could just drop the publisher, and subscribers could detect this by seeing end of the event stream.  However, currently I need to keep extra copies of the Sender around just to be able to create new subscribers, as well as an explicit way of signalling subscribers that the ""real"" publisher has shut down.  If I were able to clone Receiver's, this could be done much simpler.",2032,2020-04-16T06:54:49Z,0
560,carllerche,Implementing `Clone` would not be trivial. `Clone` would imply the new `Receiver` continues with the same cursor as the original receiver. This would require some non-trivial synchronization and a linear walk of all messages not yet seen by the receiver in order to increment a counter.,2032,2020-04-16T19:47:55Z,0
561,albel727,"Well, one also doesn't need `Clone` for the said use-case. One needs a `Receiver::subscribe_to_same(&self) -> Self` method, or more flexible yet, a `Receiver::get_sender(&self) -> Sender` method. Both trivially implementable in the current code.",2032,2021-04-07T20:53:09Z,0
562,Venryx,"Just wanted to mention for other googlers: If you're looking for a channel implementation that has `Clone` support on both its Sender and Receiver objects, the ""flume"" crate worked well for me: https://github.com/zesterer/flume

Not only does it have full `Clone` support, it also lets you send and receive either synchronously or asynchronously (ie. using Futures), unlike most of the other `Clone`-compatible channel implementations I have seen.",2032,2022-01-29T07:17:17Z,0
563,Darksonn,"The mock type in tokio-test is written for the `AsyncRead`/`AsyncWrite` traits, not the `Stream` trait. ",4106,2021-09-13T14:08:12Z,0
564,Munksgaard,"Ah, good point. Perhaps this is not the right place for such a feature request, then?",4106,2021-09-13T14:45:22Z,0
565,Darksonn,"I don't mind adding some sort of `Stream` mock, but it would be a new type.",4106,2021-09-13T14:54:12Z,0
566,Munksgaard,Sounds perfect.,4106,2021-09-17T09:04:36Z,0
567,Munksgaard,"Though I guess it would need to be written for the `Stream` _and_ `Sink` traits, right?",4106,2021-09-17T09:05:09Z,0
568,Darksonn,You could do both. I'm also ok with something that starts with just `Stream`.,4106,2021-09-21T21:49:41Z,0
569,AnIrishDuck,"I'm interested in taking this on. So that I understand: the desired behavior for the mock `Stream` is the ability to dynamically insert `Item`s during the test? Otherwise you could just use something like [`Stream::iter`](https://docs.rs/tokio-stream/0.1.7/tokio_stream/fn.iter.html)?

As for mocking a `Sink` , it seems like we'd need to track all values inserted with `start_send` (via a `Vec`?), and some way to set the next `Poll` state for associated `poll_` methods?

It also seems like we might want sequence-building functionality like the current `io::Builder` has?

A rough example test snippet could be helpful here.",4106,2021-09-21T22:32:31Z,0
570,Darksonn,"Right, we need it to do more than just `Stream::iter` if we want it to be useful, so it probably needs both the `Stream` and `Sink` impl. 

A minimal builder would probably look like this, mirroring the existing builder.
```
impl Builder<T, U, E> {
    fn next(&mut self, result: T);
    fn send_ok(&mut self, item: U);
    fn send_err(&mut self, error: E);
    fn wait(&mut self, duration: Duration);
}
```
Of course, if you have better ideas, that's fine too.",4106,2021-09-22T05:42:22Z,0
571,Darksonn,"The best example I can think of where this could be used is for e.g. testing code that communicates over a web socket. Such code would probably split it into two halves with [`StreamExt::split`][1] and have one task constantly read from it while another is writing. So we would need it to not panic if a read happens during a write, instead just return `Poll::Pending` until the write we want happens.

[1]: https://docs.rs/futures/0.3/futures/stream/trait.StreamExt.html#method.split",4106,2021-09-22T05:47:41Z,0
572,AnIrishDuck,"Digging into this some more, it looks like both `Stream` and `Sink` have some runtime requirements that should also be enforced:

- non-fused `Stream` objects have [undefined behavior](https://docs.rs/futures-core/0.3.17/futures_core/stream/trait.Stream.html#tymethod.poll_next) when `next` is called after the stream closes, so we should probably panic if this happens in a test.
- the various `poll_` methods in `Sink` have [requirements](https://docs.rs/futures/0.3.17/futures/sink/trait.Sink.html#tymethod.poll_ready) about call order that should also be enforced. e.g. `poll_ready` _must_ be called before each call to `start_send`. It's strongly implied that not doing so is undefined behavior, so my gut reaction is we should attempt to detect violations of this rule too and panic if we find them.

As you mentioned in the previous comment, there's some awkwardness around the fact that `Stream` and `Sink` are theoretically decoupled. I'm toying with the idea of having separate action queues for each side, but still need to think through dependency use cases there (e.g. request/response protocols). As `tokio_serde` was the impetus for this change, I'm trying to imagine what tests might look like for its tiny [example](https://github.com/carllerche/tokio-serde/tree/master/examples).",4106,2021-09-22T23:23:23Z,0
573,Darksonn,"I think it would probably best to tie the `Stream` and `Sink` together instead of decoupling them.

---

Just to be clear, none of the things you mentioned can cause _undefined_ behavior. That has a very specific meaning in Rust-land, which e.g. the `poll_next` documentation also points out:

> However, as the `poll_next` method is not marked unsafe, Rust’s usual rules apply: calls must never cause undefined behavior (memory corruption, incorrect use of unsafe functions, or the like), regardless of the stream’s state.

The word for this kind of thing is unspecified behavior.",4106,2021-09-23T06:38:05Z,0
574,AnIrishDuck,"Ah, that makes sense. Thanks for the clarification. I have the `Stream` half ready. The `Sink` side is significantly more complicated, but I think I should be able to complete that in the next few days.",4106,2021-09-27T21:59:14Z,0
575,AnIrishDuck,"I'm puzzling out some additional behavior for waking, which is a bit more complex with a two-sided communication channel.

In particular, the documentation for [`.poll()`](https://doc.rust-lang.org/stable/std/future/trait.Future.html) on the `Future` trait specifically states:

> on multiple calls to poll, only the Waker from the Context passed to the most recent call should be scheduled to receive a wakeup.

There is no such guidance for the [poll methods](https://docs.rs/futures/0.3.17/futures/sink/trait.Sink.html) on the `Sink` trait. I'm currently proceeding under the assumption that all calls to `poll` should replace a single waker stored in the sink state, similar to how `poll` works in `Future`. If that needs to be adjusted (e.g. different wakers per poll method), or if there is a better way to figure this out, let me know.",4106,2021-09-29T22:03:28Z,0
576,Darksonn,"In general, multiple calls to any poll method should only wake the most recent waker. This includes sinks.",4106,2021-09-30T06:44:41Z,0
577,AnIrishDuck,"Ok, one final question: is `.wait()` necessary? Couldn't it be defined directly in the test by spawning a task that does a `sleep` and then sequences the next operation via the provided `Handle`?

Is the function useful enough that it should just be included anyway?

And finally, if it is necessary, I'm not seeing any tests for similar functionality in `io`. Is it ok to punt on that for now, or should we use something like [mock_instant](https://lib.rs/crates/mock_instant) to create one?",4106,2021-10-08T21:45:36Z,0
578,Darksonn,"I believe `.wait()` is useful because without it you don't really get any `Poll::Pending` return values in other situations.

Regarding tests that use time, you should not do that using `mock_instant`. The main Tokio crate already has utilities for mocking the `tokio::time::Instant` type.",4106,2021-10-10T07:55:34Z,0
579,Grubba27,Is this still open  ? and is this a good begginer issue ?,4106,2022-01-29T14:37:41Z,0
580,Darksonn,It is indeed still open. You are welcome to give it a try!,4106,2022-01-29T14:45:39Z,0
581,Grubba27,"Awesome! from where could I start ? 
I was reading this thread and i think i got it like 50% so i'm leaving here what i got it as my to-do list. 


- [ ] I would start creating a test function in tokio-stream/tests/stream_<something>.rs or like tokio/tests/io_write.rs

- [ ] Create a builder like there was mentioned in this thread for the stream or use tokio stream like this 
 ```rust
 tokio_stream::{self, StreamExt};
 ```
_tokio stream_
```rust
impl Builder<T, U, E> {
    fn next(&mut self, result: T);
    fn send_ok(&mut self, item: U);
    fn send_err(&mut self, error: E);
    fn wait(&mut self, duration: Duration);
}

```

_mentioned builder_

- [ ] Then add a vec of values for the execution of the stream
- [ ] assert if the values are there for the execution time like is done in stream_timeout.rs ?",4106,2022-01-29T15:05:06Z,0
582,Darksonn,I believe that the implementation would be very similar to the mock in `tokio_test/io.rs`. I would start by reading that for inspiration.,4106,2022-01-29T15:35:08Z,0
583,Grubba27,"
Okay, I think I'm getting somewere. How can i create a stream from a Mock type since its not iterable? 
```rust
use futures_util::stream; // why not the use tokio_stream::{self as stream, StreamExt}; ?

/* ....  */


#[tokio::test]
async fn stream_read(){

    let mut mock: Stream = stream:: something_here ::from(Builder::new().read(b""hello "").read(b""world!"").build());
    let res: Poll<Option<...>::item> = mock.poll_next(); // here i'm trying to figure out how to get the read values
    assert_eq!(res, b""hello "");
```

",4106,2022-01-29T16:43:14Z,0
584,Darksonn,The `Mock` type itself should be a `Stream`. No conversion should be necessary.,4106,2022-01-30T09:30:15Z,0
585,carllerche,Hmmm I’m not sure why CI is hanging…,4448,2022-01-30T01:09:00Z,0
586,carllerche,Ok it was a merge fail. This PR should be good to go,4448,2022-01-30T02:57:06Z,0
587,carllerche,The CircleCI build will fail due to missing configuration. It shouldn't block merging.,4434,2022-01-27T23:56:09Z,0
588,Darksonn,This needs to include #4437.,4434,2022-01-28T08:45:17Z,0
589,carllerche,"@Darksonn thanks, I'll pull it in once I wrangle CI.",4434,2022-01-28T18:21:47Z,0
590,carllerche,"@Darksonn Ok, I backported the fix as well.",4434,2022-01-29T23:01:26Z,0
591,carllerche,Reordering commit before merge.,4434,2022-01-30T18:13:30Z,0
592,danburkert,"I've added a PR with the proposed new methods here: https://github.com/tokio-rs/tokio/pull/4447. I went with the names `[try_]acquire_permits[_owned]`, but definitely open to changing that. I did not add deprecation tags to the original methods, but can if that's the consensus.",4446,2022-01-29T22:27:43Z,0
593,Darksonn,"It's unfortunate that there's a mismatch in the types, but I'm not convinced its worth fixing it. Note that on 32-bit platforms, the maximum number of permits isn't 2^32, but rather 2^29. Similarly, on 64-bit, the maximum is 2^61.",4446,2022-01-30T09:37:46Z,0
594,danburkert,"@Darksonn with the API as it is, it's not possible to use `Semaphore` to implement memory leases with support for leases > 4GiB.  Memory leases of this style are a common technique data analysis systems, warehousing, query execution, etc.  For these usecases a 4GiB limit is orders of magnitude too low.  The 2^61 limit is not an issue since no practical system is approaching that much memory, and x86-64 is typically limited to 52 bytes of addressable space anyway.

WRT the cost of fixing it, the semaphore internals are already in terms of `usize` so it's not a major change.",4446,2022-01-30T20:23:55Z,0
595,danburkert,"It's situationally possible to extend the usefulness of the current `Semaphore` type by mapping a lease to a page of memory instead of a byte, yielding a leases of up to 2 ^ 44 = 16 TiB, which is fine for most systems now, but perhaps only for a few more years.",4446,2022-01-30T20:30:56Z,0
596,Darksonn,Thanks.,4452,2022-01-31T09:06:47Z,0
597,carllerche,This should be good to go,4451,2022-01-30T21:52:14Z,0
598,taiki-e,"stacked borrows violation with -Zmiri-tag-raw-pointers:

```sh
MIRIFLAGS=""-Zmiri-disable-isolation -Zmiri-tag-raw-pointers"" cargo miri test -p tokio --features full --lib -- runtime::tests::queue::overflow
```

<details>
<summary>output</summary>

```
error: Undefined Behavior: trying to reborrow for SharedReadWrite at alloc4551509+0x30, but parent tag <13554726> does not have an appropriate item in the borrow stack
   --> /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ptr/non_null.rs:327:18
    |
327 |         unsafe { &*self.as_ptr() }
    |                  ^^^^^^^^^^^^^^^ trying to reborrow for SharedReadWrite at alloc4551509+0x30, but parent tag <13554726> does not have an appropriate item in the borrow stack
    |
    = help: this indicates a potential bug in the program: it performed an invalid operation, but the rules it violated are still experimental
    = help: see https://github.com/rust-lang/unsafe-code-guidelines/blob/master/wip/stacked-borrows.md for further information
            
    = note: inside `std::ptr::NonNull::<runtime::task::core::Cell<std::future::from_generator::GenFuture<[static generator@tokio/src/runtime/tests/queue.rs:29:46: 29:48]>, runtime::blocking::schedule::NoopSchedule>>::as_ref` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ptr/non_null.rs:327:18
note: inside `runtime::task::harness::Harness::<std::future::from_generator::GenFuture<[static generator@tokio/src/runtime/tests/queue.rs:29:46: 29:48]>, runtime::blocking::schedule::NoopSchedule>::trailer` at tokio/src/runtime/task/harness.rs:34:19
   --> tokio/src/runtime/task/harness.rs:34:19
    |
34  |         unsafe { &self.cell.as_ref().trailer }
    |                   ^^^^^^^^^^^^^^^^^^
note: inside `runtime::task::harness::Harness::<std::future::from_generator::GenFuture<[static generator@tokio/src/runtime/tests/queue.rs:29:46: 29:48]>, runtime::blocking::schedule::NoopSchedule>::dealloc` at tokio/src/runtime/task/harness.rs:148:9
   --> tokio/src/runtime/task/harness.rs:148:9
    |
148 |         self.trailer().waker.with_mut(drop);
    |         ^^^^^^^^^^^^^^
note: inside `runtime::task::raw::dealloc::<std::future::from_generator::GenFuture<[static generator@tokio/src/runtime/tests/queue.rs:29:46: 29:48]>, runtime::blocking::schedule::NoopSchedule>` at tokio/src/runtime/task/raw.rs:118:5
   --> tokio/src/runtime/task/raw.rs:118:5
    |
118 |     harness.dealloc();
    |     ^^^^^^^^^^^^^^^^^
note: inside `runtime::task::raw::RawTask::dealloc` at tokio/src/runtime/task/raw.rs:76:13
   --> tokio/src/runtime/task/raw.rs:76:13
    |
76  |             (vtable.dealloc)(self.ptr);
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^
note: inside `<runtime::task::Task<runtime::blocking::schedule::NoopSchedule> as std::ops::Drop>::drop` at tokio/src/runtime/task/mod.rs:394:13
   --> tokio/src/runtime/task/mod.rs:394:13
    |
394 |             self.raw.dealloc();
    |             ^^^^^^^^^^^^^^^^^^
    = note: inside `std::ptr::drop_in_place::<runtime::task::Task<runtime::blocking::schedule::NoopSchedule>> - shim(Some(runtime::task::Task<runtime::blocking::schedule::NoopSchedule>))` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ptr/mod.rs:188:1
    = note: inside `std::ptr::drop_in_place::<runtime::task::Notified<runtime::blocking::schedule::NoopSchedule>> - shim(Some(runtime::task::Notified<runtime::blocking::schedule::NoopSchedule>))` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ptr/mod.rs:188:1
    = note: inside `std::ptr::drop_in_place::<std::option::Option<runtime::task::Notified<runtime::blocking::schedule::NoopSchedule>>> - shim(Some(std::option::Option<runtime::task::Notified<runtime::blocking::schedule::NoopSchedule>>))` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ptr/mod.rs:188:1
note: inside `runtime::tests::queue::overflow` at tokio/src/runtime/tests/queue.rs:35:32
   --> tokio/src/runtime/tests/queue.rs:35:32
    |
35  |     while inject.pop().is_some() {
    |                                ^
note: inside closure at tokio/src/runtime/tests/queue.rs:24:1
   --> tokio/src/runtime/tests/queue.rs:24:1
    |
23  |   #[test]
    |   ------- in this procedural macro expansion
24  | / fn overflow() {
25  | |     let (_, mut local) = queue::local();
26  | |     let inject = Inject::new();
27  | |
...   |
43  | |     assert_eq!(n, 257);
44  | | }
    | |_^
    = note: inside `<[closure@tokio/src/runtime/tests/queue.rs:24:1: 44:2] as std::ops::FnOnce<()>>::call_once - shim` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227:5
    = note: inside `<fn() as std::ops::FnOnce<()>>::call_once - shim(fn())` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227:5
    = note: inside `test::__rust_begin_short_backtrace::<fn()>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:574:5
    = note: inside closure at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:565:30
    = note: inside `<[closure@test::run_test::{closure#1}] as std::ops::FnOnce<()>>::call_once - shim(vtable)` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227:5
    = note: inside `<std::boxed::Box<dyn std::ops::FnOnce() + std::marker::Send> as std::ops::FnOnce<()>>::call_once` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854:9
    = note: inside `<std::panic::AssertUnwindSafe<std::boxed::Box<dyn std::ops::FnOnce() + std::marker::Send>> as std::ops::FnOnce<()>>::call_once` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271:9
    = note: inside `std::panicking::r#try::do_call::<std::panic::AssertUnwindSafe<std::boxed::Box<dyn std::ops::FnOnce() + std::marker::Send>>, ()>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:406:40
    = note: inside `std::panicking::r#try::<(), std::panic::AssertUnwindSafe<std::boxed::Box<dyn std::ops::FnOnce() + std::marker::Send>>>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:370:19
    = note: inside `std::panic::catch_unwind::<std::panic::AssertUnwindSafe<std::boxed::Box<dyn std::ops::FnOnce() + std::marker::Send>>, ()>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:133:14
    = note: inside `test::run_test_in_process` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:597:18
    = note: inside closure at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:491:39
    = note: inside `test::run_test::run_test_inner` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:529:13
    = note: inside `test::run_test` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:561:28
    = note: inside `test::run_tests::<[closure@test::run_tests_console::{closure#2}]>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:304:17
    = note: inside `test::run_tests_console` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/console.rs:290:5
    = note: inside `test::test_main` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:115:15
    = note: inside `test::test_main_static` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:134:5
    = note: inside `main`
    = note: inside `<fn() as std::ops::FnOnce<()>>::call_once - shim(fn())` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227:5
    = note: inside `std::sys_common::backtrace::__rust_begin_short_backtrace::<fn(), ()>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:123:18
    = note: inside closure at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:145:18
    = note: inside `std::ops::function::impls::<impl std::ops::FnOnce<()> for &dyn std::ops::Fn() -> i32 + std::marker::Sync + std::panic::RefUnwindSafe>::call_once` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:259:13
    = note: inside `std::panicking::r#try::do_call::<&dyn std::ops::Fn() -> i32 + std::marker::Sync + std::panic::RefUnwindSafe, i32>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:406:40
    = note: inside `std::panicking::r#try::<i32, &dyn std::ops::Fn() -> i32 + std::marker::Sync + std::panic::RefUnwindSafe>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:370:19
    = note: inside `std::panic::catch_unwind::<&dyn std::ops::Fn() -> i32 + std::marker::Sync + std::panic::RefUnwindSafe, i32>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:133:14
    = note: inside closure at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128:48
    = note: inside `std::panicking::r#try::do_call::<[closure@std::rt::lang_start_internal::{closure#2}], isize>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:406:40
    = note: inside `std::panicking::r#try::<isize, [closure@std::rt::lang_start_internal::{closure#2}]>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:370:19
    = note: inside `std::panic::catch_unwind::<[closure@std::rt::lang_start_internal::{closure#2}], isize>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:133:14
    = note: inside `std::rt::lang_start_internal` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128:20
    = note: inside `std::rt::lang_start::<()>` at /home/runner/.rustup/toolchains/nightly-2022-01-12-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:144:17
    = note: this error originates in the attribute macro `test` (in Nightly builds, run with -Z macro-backtrace for more info)
```

</details>

If run without -Zmiri-tag-raw-pointers, the test succeeds.

",4397,2022-01-12T16:00:44Z,0
599,taiki-e,"~~Hopefully, I know how to fix that SB violation.~~

Nah, it seems so complicated...
",4397,2022-01-12T18:15:37Z,0
600,Darksonn,That typically means that it depends on unreleased (at the time) tokio-macros changes. Try 1.6.0,4455,2022-01-31T17:51:28Z,0
601,carllerche,@Darksonn Done.,4455,2022-01-31T17:59:32Z,0
602,carllerche,It seems like Cirrus ARM does not support IPv6. Thoughts @taiki-e ?,4450,2022-01-30T21:41:20Z,0
603,taiki-e,"IIRC, Arm containers are executed in AWS's EKS. And AWS added support for IPv6 [a few weeks ago](https://github.com/aws/containers-roadmap/issues/835#issuecomment-1006804864). So it will probably be available in Cirrus CI in the near future.",4450,2022-01-31T03:45:04Z,0
604,carllerche,"@taiki-e Thanks, I reached out to Cirrus (see linked issue above).",4450,2022-01-31T21:06:44Z,0
605,Darksonn,The first version of Cargo that supports the `rust-version` field is 1.56.0.,4457,2022-01-31T18:45:16Z,0
606,olix0r,@Darksonn just noting because it's set here https://github.com/tokio-rs/tokio/blob/77468ae3b0296eaa12a1c14b3abb44d0b3fb5ce6/tokio/Cargo.toml#L11 (and maybe elsewhere),4457,2022-01-31T18:46:39Z,0
607,olix0r,I found some additional places where 1.46 was specified. Fixed in https://github.com/tokio-rs/tokio/pull/4458 (which is a PR against this branch),4457,2022-01-31T20:45:32Z,0
608,BraulioVM,"I've implemented the functionality (epollerr awaking waiters) for the `AsyncRead` slot as well (also for the `AsyncWrite` one actually, but that one is tricky because `ready.is_error()` implies `ready.is_write_closed()`, which already waiters with a write interest). This is not part of the original issue but it makes sense to me to be consistent? We can do it in separate PRs if you prefer though.

My current plan is to find a satisfactory way to test these changes. It doesn't seem immediately obvious how to do it, because the only way I know to trigger a `EPOLLERR` is through a setup that generally requires root permissions (the one in the issue). Once I write some tests, I will think through the TODOs I've left in the code, and clean-up the code I've already written (which isn't great yet).

Please let me know if you have any thoughts",4444,2022-02-01T00:10:27Z,0
609,BraulioVM,"I think I've found a way to test this automatically! By searching `EPOLLERR` on https://github.com/torvalds/linux (who would have guessed this would actually work) I've found https://github.com/torvalds/linux/blob/46f4945e2b39dda4b832909434785483e028419d/fs/eventfd.c#L175-L176 Indeed, the man page for eventfd (2) mentions:

> If an overflow of the counter value was detected, then
                 select(2) indicates the file descriptor as being both
                 readable and writable, and poll(2) returns a POLLERR
                 event.

So this looks very promising! I will try this over the following days",4444,2022-02-01T00:29:24Z,0
610,BraulioVM,"> So this looks very promising!

That was wrong. It's true that if the counter's value is 0xfffffffff then polling on the eventfd will return EPOLLERR, but it's very hard to write that value to the eventfd in the first place. According to the man page

> As noted above, write(2) can never overflow the
                 counter.  However an overflow can occur if 2^64 eventfd
                 ""signal posts"" were performed by the KAIO subsystem
                 (theoretically possible, but practically unlikely)

I will search for other ways to trigger EPOLLER",4444,2022-02-01T18:35:13Z,0
611,carllerche,"Additionally, when a `TaskSet` is dropped, it should cancel all remaining spawned tasks.",3903,2021-06-30T16:16:16Z,0
612,hawkw," Couple thoughts:
 - it would be nice for `tokio-util` to provide a stream adapter for this, which we could do easily with the `poll_next_finished` method :+1: 
 - I notice that `next_finished` and `poll_next_finished` return `Option<T>`s, presumably ignoring any `JoinError`s. Most of the time, this is _probably_ sufficient and is definitely a more ergonomic interface than having to deal with fallibility...but I can also imagine not wanting to have the `TaskSet` swallow panics. Maybe there should be a separate `try_next_finished` or something that returns an `Option<Result<T, JoinError>>`, so we can propagate task panics?

Overall this would definitely be nice to have!",3903,2021-06-30T16:59:35Z,0
613,carllerche,"Right, we probably should include the join error. good call.",3903,2021-06-30T17:10:03Z,0
614,Darksonn,"Right, I forgot about the `JoinError`. And `tokio-stream` will definitely provide a `Stream` adapter in [`tokio_stream::wrappers`][1].

Another thought is to include a `FromIterator` impl you so you can `collect` in to the `TaskSet` type.

[1]: https://docs.rs/tokio-stream/0.1/tokio_stream/wrappers/index.html",3903,2021-06-30T18:11:37Z,0
615,Darksonn,"Regarding methods that don't return a `Result<T, JoinError>`, we could have it resume the panic. However, do we even want to have any methods that do this?",3903,2021-06-30T20:12:47Z,0
616,hawkw,"Currently, we never resume panics...but I think it might be reasonable to at least have one version of the API that does that?",3903,2021-06-30T22:40:58Z,0
617,Matthias247,"So is this a clone of `FuturesUnordered`, or explicitly for `JoinHandle`s?

It seemes like something in between, because `spawn` sounds it would spawn a real independent task. Then this seems like a version of #1897 without any of the guarantees that tasks actually stop if you stop interacting with the `TaskSet`.

If this is just a `FuturesUnordered` clone, then it has stronger guarantees. People can store `JoinHandle`s manually in it, and might have some idea that those tasks might continue to run in the background. But then again people can directly use `FuturesUnordered` - I have yet to see an application which imports tokio and not `futures`.
",3903,2021-07-01T18:48:13Z,0
618,Darksonn,"The `spawn` method spawns a real task, yes. It can be thought of as a `FuturesUnordered` for `JoinHandle`s.",3903,2021-07-01T19:04:07Z,0
619,Noah-Kennedy,"I'm actually most interested in the cancellation capability here. It's not uncommon in certain types of code to need to mass cancel a set of tasks under some sort of ""scope"", and this seems like an excellent solution to that problem.",3903,2021-09-25T20:20:32Z,0
620,Noah-Kennedy,I might release a prototype in a separate crate.,3903,2021-09-27T15:47:05Z,0
621,Noah-Kennedy,"I'm working on a prototype for this right now. One nice feature not mentioned that I added is an async `shutdown` method which aborts all tasks and then waits for them all to finish, consuming the `TaskSet` in the process.",3903,2021-10-29T17:39:45Z,0
622,Noah-Kennedy,"Opened a PR: https://github.com/tokio-rs/tokio/pull/4238

This _should_ be pretty close to something we can all agree on.",3903,2021-11-16T00:17:39Z,0
623,Noah-Kennedy,"So, an issue with the approach I used is that there isn't a good way from tokio-util to do something like FuturesUnordered without re-implementing the internal data structure that FuturesUnordered is built around, unless you want `poll_next_finished` to be O(n), as the naive approach of using a vec requires you to scan through your list of tasks, polling as you go.",3903,2021-11-16T20:17:19Z,0
624,carllerche,@Darksonn How would we add an API in the future for spawning tasks that *don't* abort when the set is dropped?,4335,2021-12-21T19:44:20Z,0
625,Noah-Kennedy,"Considering that task set is going to be used a lot for resource cleanup and management, we probably want something along the lines of a `fn shutdown(self) -> impl Future<()>` that aborts all of the tasks and waits for them to finish, so that users can await the termination of the tasks on the set.",4335,2021-12-21T20:02:48Z,0
626,Darksonn,How should we handle panics in the sub-tasks when the user has called `shutdown`?,4335,2022-01-10T10:34:37Z,0
627,carllerche,"""clean shutdown"" is a good call.

What I would probably do is a call to `shutdown()` aborts remaining tasks, but there *may* be some tasks that have since completed (either successfully or w/ a panic). The user of `TaskSet` would then continue to drain results until `None`. Once `None` is reached, then no more tasks will complete and the `TaskSet` can be shutdown.

This is a similar strategy as the `mpsc` channel, which maps pretty well to this case.",4335,2022-01-10T16:33:45Z,0
628,Noah-Kennedy,I'm in agreement with Carl here. It would be best not to propagate the panics.,4335,2022-01-11T02:48:42Z,0
629,Darksonn,"I have reworked the `IdleNotifiedSet` to guarantee that the value stored in an entry is dropped when the entry is removed from the set. If a value isn't destroyed when the entry is dropped, then that's a ref-cycle because the entry holds a `JoinHandle` to the task, but the task holds a waker to the entry.

I'm also adding a test for the coop budget as I realized that `join_one` didn't correctly handle it before I reworked `IdleNotifiedSet`.",4335,2022-01-24T12:01:36Z,0
630,carllerche,cc @seanmonstar @hawkw ,4335,2022-01-24T18:11:54Z,0
631,Noah-Kennedy,"This seems about ready. I left a few questions and comments, but I don't think they block a merge here.",4335,2022-01-26T17:51:31Z,0
632,Noah-Kennedy,"@saleemadat @Darksonn This is a draft PR. I am adding `#[track_caller]` to the whole codebase, then adding tests. This is not finished yet.",4445,2022-02-01T23:17:48Z,0
633,ellenhp,Spoke a bit with some Rust ESP-32 people and they indicated this was the right way to go about this change. Their only warning was that most of tokio still won't build which I knew already. Ready for review. Thank you!,4460,2022-02-01T15:24:11Z,0
634,Darksonn,I don't really understand why CI is stuck. Are you able to push an empty commit?,4460,2022-02-02T15:24:39Z,0
